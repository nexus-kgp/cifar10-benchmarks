{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader= torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "classes=('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 2.203\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 1.837\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 1.637\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 1.574\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 1.518\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 1.473\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 1.397\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 1.366\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 1.378\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 1.348\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 1.314\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 1.306\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 1.218\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 1.242\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 1.199\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 1.195\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 1.195\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 1.198\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 1.106\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 1.135\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 1.139\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 1.100\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 1.103\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 1.102\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 1.030\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 1.055\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 1.048\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 1.063\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 1.044\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 1.040\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 0.978\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 0.984\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 0.994\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 0.983\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 0.979\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 0.995\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 0.926\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 0.938\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 0.954\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 0.957\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 0.943\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 0.961\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 0.866\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 0.876\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 0.895\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 0.918\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 0.911\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 0.938\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 0.816\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 0.869\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 0.884\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 0.875\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 0.885\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 0.874\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 0.773\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 0.834\n",
      "[Epoch :: 10, Mini Batch ::  6000] loss: 0.829\n",
      "[Epoch :: 10, Mini Batch ::  8000] loss: 0.848\n",
      "[Epoch :: 10, Mini Batch :: 10000] loss: 0.878\n",
      "[Epoch :: 10, Mini Batch :: 12000] loss: 0.876\n",
      "[Epoch :: 11, Mini Batch ::  2000] loss: 0.744\n",
      "[Epoch :: 11, Mini Batch ::  4000] loss: 0.813\n",
      "[Epoch :: 11, Mini Batch ::  6000] loss: 0.800\n",
      "[Epoch :: 11, Mini Batch ::  8000] loss: 0.847\n",
      "[Epoch :: 11, Mini Batch :: 10000] loss: 0.839\n",
      "[Epoch :: 11, Mini Batch :: 12000] loss: 0.829\n",
      "[Epoch :: 12, Mini Batch ::  2000] loss: 0.725\n",
      "[Epoch :: 12, Mini Batch ::  4000] loss: 0.759\n",
      "[Epoch :: 12, Mini Batch ::  6000] loss: 0.786\n",
      "[Epoch :: 12, Mini Batch ::  8000] loss: 0.810\n",
      "[Epoch :: 12, Mini Batch :: 10000] loss: 0.810\n",
      "[Epoch :: 12, Mini Batch :: 12000] loss: 0.826\n",
      "[Epoch :: 13, Mini Batch ::  2000] loss: 0.705\n",
      "[Epoch :: 13, Mini Batch ::  4000] loss: 0.738\n",
      "[Epoch :: 13, Mini Batch ::  6000] loss: 0.758\n",
      "[Epoch :: 13, Mini Batch ::  8000] loss: 0.782\n",
      "[Epoch :: 13, Mini Batch :: 10000] loss: 0.805\n",
      "[Epoch :: 13, Mini Batch :: 12000] loss: 0.801\n",
      "[Epoch :: 14, Mini Batch ::  2000] loss: 0.676\n",
      "[Epoch :: 14, Mini Batch ::  4000] loss: 0.726\n",
      "[Epoch :: 14, Mini Batch ::  6000] loss: 0.752\n",
      "[Epoch :: 14, Mini Batch ::  8000] loss: 0.771\n",
      "[Epoch :: 14, Mini Batch :: 10000] loss: 0.772\n",
      "[Epoch :: 14, Mini Batch :: 12000] loss: 0.766\n",
      "[Epoch :: 15, Mini Batch ::  2000] loss: 0.662\n",
      "[Epoch :: 15, Mini Batch ::  4000] loss: 0.706\n",
      "[Epoch :: 15, Mini Batch ::  6000] loss: 0.733\n",
      "[Epoch :: 15, Mini Batch ::  8000] loss: 0.741\n",
      "[Epoch :: 15, Mini Batch :: 10000] loss: 0.757\n",
      "[Epoch :: 15, Mini Batch :: 12000] loss: 0.769\n",
      "[Epoch :: 16, Mini Batch ::  2000] loss: 0.642\n",
      "[Epoch :: 16, Mini Batch ::  4000] loss: 0.682\n",
      "[Epoch :: 16, Mini Batch ::  6000] loss: 0.734\n",
      "[Epoch :: 16, Mini Batch ::  8000] loss: 0.739\n",
      "[Epoch :: 16, Mini Batch :: 10000] loss: 0.759\n",
      "[Epoch :: 16, Mini Batch :: 12000] loss: 0.776\n",
      "[Epoch :: 17, Mini Batch ::  2000] loss: 0.632\n",
      "[Epoch :: 17, Mini Batch ::  4000] loss: 0.661\n",
      "[Epoch :: 17, Mini Batch ::  6000] loss: 0.709\n",
      "[Epoch :: 17, Mini Batch ::  8000] loss: 0.726\n",
      "[Epoch :: 17, Mini Batch :: 10000] loss: 0.739\n",
      "[Epoch :: 17, Mini Batch :: 12000] loss: 0.735\n",
      "[Epoch :: 18, Mini Batch ::  2000] loss: 0.608\n",
      "[Epoch :: 18, Mini Batch ::  4000] loss: 0.643\n",
      "[Epoch :: 18, Mini Batch ::  6000] loss: 0.692\n",
      "[Epoch :: 18, Mini Batch ::  8000] loss: 0.701\n",
      "[Epoch :: 18, Mini Batch :: 10000] loss: 0.726\n",
      "[Epoch :: 18, Mini Batch :: 12000] loss: 0.755\n",
      "[Epoch :: 19, Mini Batch ::  2000] loss: 0.604\n",
      "[Epoch :: 19, Mini Batch ::  4000] loss: 0.661\n",
      "[Epoch :: 19, Mini Batch ::  6000] loss: 0.664\n",
      "[Epoch :: 19, Mini Batch ::  8000] loss: 0.705\n",
      "[Epoch :: 19, Mini Batch :: 10000] loss: 0.706\n",
      "[Epoch :: 19, Mini Batch :: 12000] loss: 0.728\n",
      "[Epoch :: 20, Mini Batch ::  2000] loss: 0.593\n",
      "[Epoch :: 20, Mini Batch ::  4000] loss: 0.629\n",
      "[Epoch :: 20, Mini Batch ::  6000] loss: 0.701\n",
      "[Epoch :: 20, Mini Batch ::  8000] loss: 0.693\n",
      "[Epoch :: 20, Mini Batch :: 10000] loss: 0.719\n",
      "[Epoch :: 20, Mini Batch :: 12000] loss: 0.729\n",
      "[Epoch :: 21, Mini Batch ::  2000] loss: 0.577\n",
      "[Epoch :: 21, Mini Batch ::  4000] loss: 0.647\n",
      "[Epoch :: 21, Mini Batch ::  6000] loss: 0.683\n",
      "[Epoch :: 21, Mini Batch ::  8000] loss: 0.662\n",
      "[Epoch :: 21, Mini Batch :: 10000] loss: 0.706\n",
      "[Epoch :: 21, Mini Batch :: 12000] loss: 0.707\n",
      "[Epoch :: 22, Mini Batch ::  2000] loss: 0.589\n",
      "[Epoch :: 22, Mini Batch ::  4000] loss: 0.616\n",
      "[Epoch :: 22, Mini Batch ::  6000] loss: 0.661\n",
      "[Epoch :: 22, Mini Batch ::  8000] loss: 0.666\n",
      "[Epoch :: 22, Mini Batch :: 10000] loss: 0.674\n",
      "[Epoch :: 22, Mini Batch :: 12000] loss: 0.733\n",
      "[Epoch :: 23, Mini Batch ::  2000] loss: 0.583\n",
      "[Epoch :: 23, Mini Batch ::  4000] loss: 0.624\n",
      "[Epoch :: 23, Mini Batch ::  6000] loss: 0.641\n",
      "[Epoch :: 23, Mini Batch ::  8000] loss: 0.688\n",
      "[Epoch :: 23, Mini Batch :: 10000] loss: 0.689\n",
      "[Epoch :: 23, Mini Batch :: 12000] loss: 0.688\n",
      "[Epoch :: 24, Mini Batch ::  2000] loss: 0.581\n",
      "[Epoch :: 24, Mini Batch ::  4000] loss: 0.602\n",
      "[Epoch :: 24, Mini Batch ::  6000] loss: 0.649\n",
      "[Epoch :: 24, Mini Batch ::  8000] loss: 0.677\n",
      "[Epoch :: 24, Mini Batch :: 10000] loss: 0.701\n",
      "[Epoch :: 24, Mini Batch :: 12000] loss: 0.681\n",
      "[Epoch :: 25, Mini Batch ::  2000] loss: 0.556\n",
      "[Epoch :: 25, Mini Batch ::  4000] loss: 0.603\n",
      "[Epoch :: 25, Mini Batch ::  6000] loss: 0.657\n",
      "[Epoch :: 25, Mini Batch ::  8000] loss: 0.641\n",
      "[Epoch :: 25, Mini Batch :: 10000] loss: 0.678\n",
      "[Epoch :: 25, Mini Batch :: 12000] loss: 0.683\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# For Adagrad\n",
    "net = LeNet().cuda()\n",
    "lossvsiter_adam=[]\n",
    "\n",
    "# To see if the model is on CUDA or not !\n",
    "if (next(net.parameters()).is_cuda) :\n",
    "    print(\"The model is on CUDA\")\n",
    "else :\n",
    "    print(\"The model is on CPU\")\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Declare a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Declare an optimizer\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.001,momentum=0.9)\n",
    "\n",
    "\n",
    "# No of iterations !\n",
    "iterations = 25\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "    \n",
    "    # Reset the loss for the current epoch !\n",
    "    tloss = 0.0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # wrap them in Variable & if possible make them cuda tensors\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients for the current epoch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Calculate gradients of whatever variable set to req_gardients = True\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take one step of the gradient descent for this epoch ! \n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        tloss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            lossvsiter_adam.append(running_loss / 2000)\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    loss_list.append(tloss)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pickle\n",
    "with open(\"lossvsiter_adam.pkl\",\"wb\") as f:\n",
    "    pickle.dump(lossvsiter_adam,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 58 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    images=images.cuda()\n",
    "    labels=labels.cuda()\n",
    "    try:\n",
    "        outputs = net(Variable(images))\n",
    "    except RuntimeError as re:\n",
    "        print(outputs.is_cuda)\n",
    "        print(str(re))\n",
    "        sys.exit()\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Accuracy of the network on the 50000 trained images: 65 %\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader, 0):\n",
    "    images, labels = data\n",
    "    images=images.cuda()\n",
    "    labels=labels.cuda()\n",
    "    try:\n",
    "        outputs = net(Variable(images))\n",
    "    except RuntimeError as re:\n",
    "        print(outputs.is_cuda)\n",
    "        print(str(re))\n",
    "        sys.exit()\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    if (i%1000) == 0:\n",
    "        print(i)\n",
    "\n",
    "print('Accuracy of the network on the 50000 trained images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 61 %\n",
      "Accuracy of   car : 81 %\n",
      "Accuracy of  bird : 41 %\n",
      "Accuracy of   cat : 42 %\n",
      "Accuracy of  deer : 53 %\n",
      "Accuracy of   dog : 37 %\n",
      "Accuracy of  frog : 81 %\n",
      "Accuracy of horse : 60 %\n",
      "Accuracy of  ship : 61 %\n",
      "Accuracy of truck : 79 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    images=images.cuda()\n",
    "    labels=labels.cuda()\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls = []\n",
    "for i in range(len(loss_list)):\n",
    "    ls.append(loss_list[i+1]-loss_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdc6c3fc358>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkTb+Pz3YBgnXemEMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdcb806ef98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
