{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader= torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "classes=('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 1.992\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 1.751\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 1.650\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 1.571\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 1.524\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 1.465\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 1.417\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 1.390\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 1.382\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 1.342\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 1.335\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 1.313\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 1.278\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 1.251\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 1.258\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 1.239\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 1.236\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 1.222\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 1.179\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 1.168\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 1.161\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 1.184\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 1.166\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 1.165\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 1.110\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 1.126\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 1.112\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 1.105\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 1.111\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 1.098\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 1.064\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 1.061\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 1.044\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 1.052\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 1.074\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 1.057\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 1.033\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 1.026\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 1.003\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 1.008\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 1.017\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 0.998\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 0.954\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 0.974\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 0.971\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 0.972\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 0.967\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 0.988\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 0.934\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 0.919\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 0.934\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 0.955\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 0.930\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 0.936\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 0.898\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 0.878\n",
      "[Epoch :: 10, Mini Batch ::  6000] loss: 0.878\n",
      "[Epoch :: 10, Mini Batch ::  8000] loss: 0.916\n",
      "[Epoch :: 10, Mini Batch :: 10000] loss: 0.908\n",
      "[Epoch :: 10, Mini Batch :: 12000] loss: 0.913\n",
      "[Epoch :: 11, Mini Batch ::  2000] loss: 0.851\n",
      "[Epoch :: 11, Mini Batch ::  4000] loss: 0.859\n",
      "[Epoch :: 11, Mini Batch ::  6000] loss: 0.862\n",
      "[Epoch :: 11, Mini Batch ::  8000] loss: 0.875\n",
      "[Epoch :: 11, Mini Batch :: 10000] loss: 0.865\n",
      "[Epoch :: 11, Mini Batch :: 12000] loss: 0.891\n",
      "[Epoch :: 12, Mini Batch ::  2000] loss: 0.820\n",
      "[Epoch :: 12, Mini Batch ::  4000] loss: 0.809\n",
      "[Epoch :: 12, Mini Batch ::  6000] loss: 0.841\n",
      "[Epoch :: 12, Mini Batch ::  8000] loss: 0.851\n",
      "[Epoch :: 12, Mini Batch :: 10000] loss: 0.857\n",
      "[Epoch :: 12, Mini Batch :: 12000] loss: 0.845\n",
      "[Epoch :: 13, Mini Batch ::  2000] loss: 0.770\n",
      "[Epoch :: 13, Mini Batch ::  4000] loss: 0.796\n",
      "[Epoch :: 13, Mini Batch ::  6000] loss: 0.838\n",
      "[Epoch :: 13, Mini Batch ::  8000] loss: 0.802\n",
      "[Epoch :: 13, Mini Batch :: 10000] loss: 0.826\n",
      "[Epoch :: 13, Mini Batch :: 12000] loss: 0.817\n",
      "[Epoch :: 14, Mini Batch ::  2000] loss: 0.777\n",
      "[Epoch :: 14, Mini Batch ::  4000] loss: 0.781\n",
      "[Epoch :: 14, Mini Batch ::  6000] loss: 0.776\n",
      "[Epoch :: 14, Mini Batch ::  8000] loss: 0.785\n",
      "[Epoch :: 14, Mini Batch :: 10000] loss: 0.793\n",
      "[Epoch :: 14, Mini Batch :: 12000] loss: 0.784\n",
      "[Epoch :: 15, Mini Batch ::  2000] loss: 0.737\n",
      "[Epoch :: 15, Mini Batch ::  4000] loss: 0.746\n",
      "[Epoch :: 15, Mini Batch ::  6000] loss: 0.744\n",
      "[Epoch :: 15, Mini Batch ::  8000] loss: 0.788\n",
      "[Epoch :: 15, Mini Batch :: 10000] loss: 0.767\n",
      "[Epoch :: 15, Mini Batch :: 12000] loss: 0.758\n",
      "[Epoch :: 16, Mini Batch ::  2000] loss: 0.739\n",
      "[Epoch :: 16, Mini Batch ::  4000] loss: 0.719\n",
      "[Epoch :: 16, Mini Batch ::  6000] loss: 0.730\n",
      "[Epoch :: 16, Mini Batch ::  8000] loss: 0.727\n",
      "[Epoch :: 16, Mini Batch :: 10000] loss: 0.740\n",
      "[Epoch :: 16, Mini Batch :: 12000] loss: 0.752\n",
      "[Epoch :: 17, Mini Batch ::  2000] loss: 0.690\n",
      "[Epoch :: 17, Mini Batch ::  4000] loss: 0.696\n",
      "[Epoch :: 17, Mini Batch ::  6000] loss: 0.688\n",
      "[Epoch :: 17, Mini Batch ::  8000] loss: 0.726\n",
      "[Epoch :: 17, Mini Batch :: 10000] loss: 0.721\n",
      "[Epoch :: 17, Mini Batch :: 12000] loss: 0.751\n",
      "[Epoch :: 18, Mini Batch ::  2000] loss: 0.683\n",
      "[Epoch :: 18, Mini Batch ::  4000] loss: 0.676\n",
      "[Epoch :: 18, Mini Batch ::  6000] loss: 0.680\n",
      "[Epoch :: 18, Mini Batch ::  8000] loss: 0.709\n",
      "[Epoch :: 18, Mini Batch :: 10000] loss: 0.694\n",
      "[Epoch :: 18, Mini Batch :: 12000] loss: 0.687\n",
      "[Epoch :: 19, Mini Batch ::  2000] loss: 0.641\n",
      "[Epoch :: 19, Mini Batch ::  4000] loss: 0.670\n",
      "[Epoch :: 19, Mini Batch ::  6000] loss: 0.657\n",
      "[Epoch :: 19, Mini Batch ::  8000] loss: 0.682\n",
      "[Epoch :: 19, Mini Batch :: 10000] loss: 0.690\n",
      "[Epoch :: 19, Mini Batch :: 12000] loss: 0.680\n",
      "[Epoch :: 20, Mini Batch ::  2000] loss: 0.609\n",
      "[Epoch :: 20, Mini Batch ::  4000] loss: 0.625\n",
      "[Epoch :: 20, Mini Batch ::  6000] loss: 0.664\n",
      "[Epoch :: 20, Mini Batch ::  8000] loss: 0.653\n",
      "[Epoch :: 20, Mini Batch :: 10000] loss: 0.671\n",
      "[Epoch :: 20, Mini Batch :: 12000] loss: 0.673\n",
      "[Epoch :: 21, Mini Batch ::  2000] loss: 0.599\n",
      "[Epoch :: 21, Mini Batch ::  4000] loss: 0.607\n",
      "[Epoch :: 21, Mini Batch ::  6000] loss: 0.630\n",
      "[Epoch :: 21, Mini Batch ::  8000] loss: 0.644\n",
      "[Epoch :: 21, Mini Batch :: 10000] loss: 0.637\n",
      "[Epoch :: 21, Mini Batch :: 12000] loss: 0.657\n",
      "[Epoch :: 22, Mini Batch ::  2000] loss: 0.579\n",
      "[Epoch :: 22, Mini Batch ::  4000] loss: 0.608\n",
      "[Epoch :: 22, Mini Batch ::  6000] loss: 0.611\n",
      "[Epoch :: 22, Mini Batch ::  8000] loss: 0.631\n",
      "[Epoch :: 22, Mini Batch :: 10000] loss: 0.641\n",
      "[Epoch :: 22, Mini Batch :: 12000] loss: 0.610\n",
      "[Epoch :: 23, Mini Batch ::  2000] loss: 0.568\n",
      "[Epoch :: 23, Mini Batch ::  4000] loss: 0.587\n",
      "[Epoch :: 23, Mini Batch ::  6000] loss: 0.596\n",
      "[Epoch :: 23, Mini Batch ::  8000] loss: 0.594\n",
      "[Epoch :: 23, Mini Batch :: 10000] loss: 0.606\n",
      "[Epoch :: 23, Mini Batch :: 12000] loss: 0.600\n",
      "[Epoch :: 24, Mini Batch ::  2000] loss: 0.541\n",
      "[Epoch :: 24, Mini Batch ::  4000] loss: 0.554\n",
      "[Epoch :: 24, Mini Batch ::  6000] loss: 0.576\n",
      "[Epoch :: 24, Mini Batch ::  8000] loss: 0.585\n",
      "[Epoch :: 24, Mini Batch :: 10000] loss: 0.596\n",
      "[Epoch :: 24, Mini Batch :: 12000] loss: 0.592\n",
      "[Epoch :: 25, Mini Batch ::  2000] loss: 0.527\n",
      "[Epoch :: 25, Mini Batch ::  4000] loss: 0.541\n",
      "[Epoch :: 25, Mini Batch ::  6000] loss: 0.569\n",
      "[Epoch :: 25, Mini Batch ::  8000] loss: 0.571\n",
      "[Epoch :: 25, Mini Batch :: 10000] loss: 0.575\n",
      "[Epoch :: 25, Mini Batch :: 12000] loss: 0.551\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# For Adamax\n",
    "net = LeNet().cuda()\n",
    "lossvsiter_adam=[]\n",
    "\n",
    "# To see if the model is on CUDA or not !\n",
    "if (next(net.parameters()).is_cuda) :\n",
    "    print(\"The model is on CUDA\")\n",
    "else :\n",
    "    print(\"The model is on CPU\")\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Declare a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Declare an optimizer\n",
    "optimizer = optim.Adamax(net.parameters(),lr=0.001)\n",
    "\n",
    "\n",
    "# No of iterations !\n",
    "iterations = 25\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "    \n",
    "    # Reset the loss for the current epoch !\n",
    "    running_loss = 0.0\n",
    "    tloss = 0.0\n",
    "    # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # wrap them in Variable & if possible make them cuda tensors\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients for the current epoch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Calculate gradients of whatever variable set to req_gardients = True\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take one step of the gradient descent for this epoch ! \n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        tloss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 100 mini-batches\n",
    "            print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1999))\n",
    "            running_loss = 0.0\n",
    "#         if i % 100 == 99:    # print every 100 mini-batches\n",
    "#             print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 100))\n",
    "#             running_loss = 0.0\n",
    "            \n",
    "    loss_list.append(tloss)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pickle\n",
    "with open(\"lossvsiter_adam.pkl\",\"wb\") as f:\n",
    "    pickle.dump(lossvsiter_adam,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 63 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    images=images.cuda()\n",
    "    labels=labels.cuda()\n",
    "    try:\n",
    "        outputs = net(Variable(images))\n",
    "    except RuntimeError as re:\n",
    "        print(outputs.is_cuda)\n",
    "        print(str(re))\n",
    "        sys.exit()\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Accuracy of the network on the 50000 trained images: 65 %\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader, 0):\n",
    "    images, labels = data\n",
    "    images=images.cuda()\n",
    "    labels=labels.cuda()\n",
    "    try:\n",
    "        outputs = net(Variable(images))\n",
    "    except RuntimeError as re:\n",
    "        print(outputs.is_cuda)\n",
    "        print(str(re))\n",
    "        sys.exit()\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    if (i%1000) == 0:\n",
    "        print(i)\n",
    "\n",
    "print('Accuracy of the network on the 50000 trained images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 61 %\n",
      "Accuracy of   car : 81 %\n",
      "Accuracy of  bird : 41 %\n",
      "Accuracy of   cat : 42 %\n",
      "Accuracy of  deer : 53 %\n",
      "Accuracy of   dog : 37 %\n",
      "Accuracy of  frog : 81 %\n",
      "Accuracy of horse : 60 %\n",
      "Accuracy of  ship : 61 %\n",
      "Accuracy of truck : 79 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    images=images.cuda()\n",
    "    labels=labels.cuda()\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6f012c8ba8>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng4iI1KBwEBGRGhQOIiJSg8JBRERq+P8WoU7zLk0tuwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6f016dada0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
