{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader= torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "classes=('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "# functions to show an image\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 2.304\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 2.303\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 2.300\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 2.299\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 2.296\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 2.292\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 2.283\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 2.276\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 2.267\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 2.252\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 2.235\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 2.214\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 2.185\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 2.163\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 2.139\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 2.117\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 2.095\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 2.082\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 2.044\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 2.044\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 2.028\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 2.014\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 2.000\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 1.985\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 1.969\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 1.960\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 1.960\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 1.946\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 1.955\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 1.951\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 1.932\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 1.922\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 1.925\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 1.926\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 1.913\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 1.912\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 1.894\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 1.901\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 1.893\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 1.908\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 1.897\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 1.886\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 1.877\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 1.889\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 1.876\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 1.885\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 1.878\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 1.867\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 1.880\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 1.865\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 1.853\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 1.856\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 1.877\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 1.851\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 1.850\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 1.855\n",
      "[Epoch :: 10, Mini Batch ::  6000] loss: 1.856\n",
      "[Epoch :: 10, Mini Batch ::  8000] loss: 1.849\n",
      "[Epoch :: 10, Mini Batch :: 10000] loss: 1.831\n",
      "[Epoch :: 10, Mini Batch :: 12000] loss: 1.842\n",
      "[Epoch :: 11, Mini Batch ::  2000] loss: 1.841\n",
      "[Epoch :: 11, Mini Batch ::  4000] loss: 1.842\n",
      "[Epoch :: 11, Mini Batch ::  6000] loss: 1.830\n",
      "[Epoch :: 11, Mini Batch ::  8000] loss: 1.835\n",
      "[Epoch :: 11, Mini Batch :: 10000] loss: 1.815\n",
      "[Epoch :: 11, Mini Batch :: 12000] loss: 1.828\n",
      "[Epoch :: 12, Mini Batch ::  2000] loss: 1.814\n",
      "[Epoch :: 12, Mini Batch ::  4000] loss: 1.810\n",
      "[Epoch :: 12, Mini Batch ::  6000] loss: 1.822\n",
      "[Epoch :: 12, Mini Batch ::  8000] loss: 1.827\n",
      "[Epoch :: 12, Mini Batch :: 10000] loss: 1.822\n",
      "[Epoch :: 12, Mini Batch :: 12000] loss: 1.815\n",
      "[Epoch :: 13, Mini Batch ::  2000] loss: 1.808\n",
      "[Epoch :: 13, Mini Batch ::  4000] loss: 1.810\n",
      "[Epoch :: 13, Mini Batch ::  6000] loss: 1.794\n",
      "[Epoch :: 13, Mini Batch ::  8000] loss: 1.804\n",
      "[Epoch :: 13, Mini Batch :: 10000] loss: 1.799\n",
      "[Epoch :: 13, Mini Batch :: 12000] loss: 1.812\n",
      "[Epoch :: 14, Mini Batch ::  2000] loss: 1.791\n",
      "[Epoch :: 14, Mini Batch ::  4000] loss: 1.796\n",
      "[Epoch :: 14, Mini Batch ::  6000] loss: 1.792\n",
      "[Epoch :: 14, Mini Batch ::  8000] loss: 1.798\n",
      "[Epoch :: 14, Mini Batch :: 10000] loss: 1.787\n",
      "[Epoch :: 14, Mini Batch :: 12000] loss: 1.784\n",
      "[Epoch :: 15, Mini Batch ::  2000] loss: 1.781\n",
      "[Epoch :: 15, Mini Batch ::  4000] loss: 1.790\n",
      "[Epoch :: 15, Mini Batch ::  6000] loss: 1.775\n",
      "[Epoch :: 15, Mini Batch ::  8000] loss: 1.766\n",
      "[Epoch :: 15, Mini Batch :: 10000] loss: 1.775\n",
      "[Epoch :: 15, Mini Batch :: 12000] loss: 1.778\n",
      "[Epoch :: 16, Mini Batch ::  2000] loss: 1.772\n",
      "[Epoch :: 16, Mini Batch ::  4000] loss: 1.765\n",
      "[Epoch :: 16, Mini Batch ::  6000] loss: 1.762\n",
      "[Epoch :: 16, Mini Batch ::  8000] loss: 1.759\n",
      "[Epoch :: 16, Mini Batch :: 10000] loss: 1.769\n",
      "[Epoch :: 16, Mini Batch :: 12000] loss: 1.767\n",
      "[Epoch :: 17, Mini Batch ::  2000] loss: 1.761\n",
      "[Epoch :: 17, Mini Batch ::  4000] loss: 1.766\n",
      "[Epoch :: 17, Mini Batch ::  6000] loss: 1.753\n",
      "[Epoch :: 17, Mini Batch ::  8000] loss: 1.752\n",
      "[Epoch :: 17, Mini Batch :: 10000] loss: 1.752\n",
      "[Epoch :: 17, Mini Batch :: 12000] loss: 1.744\n",
      "[Epoch :: 18, Mini Batch ::  2000] loss: 1.735\n",
      "[Epoch :: 18, Mini Batch ::  4000] loss: 1.740\n",
      "[Epoch :: 18, Mini Batch ::  6000] loss: 1.738\n",
      "[Epoch :: 18, Mini Batch ::  8000] loss: 1.738\n",
      "[Epoch :: 18, Mini Batch :: 10000] loss: 1.746\n",
      "[Epoch :: 18, Mini Batch :: 12000] loss: 1.742\n",
      "[Epoch :: 19, Mini Batch ::  2000] loss: 1.747\n",
      "[Epoch :: 19, Mini Batch ::  4000] loss: 1.715\n",
      "[Epoch :: 19, Mini Batch ::  6000] loss: 1.733\n",
      "[Epoch :: 19, Mini Batch ::  8000] loss: 1.718\n",
      "[Epoch :: 19, Mini Batch :: 10000] loss: 1.717\n",
      "[Epoch :: 19, Mini Batch :: 12000] loss: 1.735\n",
      "[Epoch :: 20, Mini Batch ::  2000] loss: 1.726\n",
      "[Epoch :: 20, Mini Batch ::  4000] loss: 1.709\n",
      "[Epoch :: 20, Mini Batch ::  6000] loss: 1.720\n",
      "[Epoch :: 20, Mini Batch ::  8000] loss: 1.724\n",
      "[Epoch :: 20, Mini Batch :: 10000] loss: 1.704\n",
      "[Epoch :: 20, Mini Batch :: 12000] loss: 1.725\n",
      "[Epoch :: 21, Mini Batch ::  2000] loss: 1.710\n",
      "[Epoch :: 21, Mini Batch ::  4000] loss: 1.710\n",
      "[Epoch :: 21, Mini Batch ::  6000] loss: 1.693\n",
      "[Epoch :: 21, Mini Batch ::  8000] loss: 1.712\n",
      "[Epoch :: 21, Mini Batch :: 10000] loss: 1.705\n",
      "[Epoch :: 21, Mini Batch :: 12000] loss: 1.709\n",
      "[Epoch :: 22, Mini Batch ::  2000] loss: 1.713\n",
      "[Epoch :: 22, Mini Batch ::  4000] loss: 1.698\n",
      "[Epoch :: 22, Mini Batch ::  6000] loss: 1.684\n",
      "[Epoch :: 22, Mini Batch ::  8000] loss: 1.677\n",
      "[Epoch :: 22, Mini Batch :: 10000] loss: 1.709\n",
      "[Epoch :: 22, Mini Batch :: 12000] loss: 1.697\n",
      "[Epoch :: 23, Mini Batch ::  2000] loss: 1.668\n",
      "[Epoch :: 23, Mini Batch ::  4000] loss: 1.688\n",
      "[Epoch :: 23, Mini Batch ::  6000] loss: 1.689\n",
      "[Epoch :: 23, Mini Batch ::  8000] loss: 1.687\n",
      "[Epoch :: 23, Mini Batch :: 10000] loss: 1.692\n",
      "[Epoch :: 23, Mini Batch :: 12000] loss: 1.698\n",
      "[Epoch :: 24, Mini Batch ::  2000] loss: 1.675\n",
      "[Epoch :: 24, Mini Batch ::  4000] loss: 1.680\n",
      "[Epoch :: 24, Mini Batch ::  6000] loss: 1.668\n",
      "[Epoch :: 24, Mini Batch ::  8000] loss: 1.680\n",
      "[Epoch :: 24, Mini Batch :: 10000] loss: 1.677\n",
      "[Epoch :: 24, Mini Batch :: 12000] loss: 1.682\n",
      "[Epoch :: 25, Mini Batch ::  2000] loss: 1.676\n",
      "[Epoch :: 25, Mini Batch ::  4000] loss: 1.677\n",
      "[Epoch :: 25, Mini Batch ::  6000] loss: 1.662\n",
      "[Epoch :: 25, Mini Batch ::  8000] loss: 1.671\n",
      "[Epoch :: 25, Mini Batch :: 10000] loss: 1.674\n",
      "[Epoch :: 25, Mini Batch :: 12000] loss: 1.655\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model on CUDA\n",
    "net = LeNet().cuda()\n",
    "\n",
    "lossvsiter_adadel=[]\n",
    "\n",
    "# To see if the model is on CUDA or not !\n",
    "if (next(net.parameters()).is_cuda) :\n",
    "    print(\"The model is on CUDA\")\n",
    "else :\n",
    "    print(\"The model is on CPU\")\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Declare a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Declare an optimizer\n",
    "optimizer = optim.Adadelta(net.parameters(),lr=0.001)\n",
    "\n",
    "\n",
    "# No of iterations !\n",
    "iterations = 25\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "    \n",
    "    # Reset the loss for the current epoch !\n",
    "    running_loss = 0.0\n",
    "    tloss = 0.0\n",
    "    # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # wrap them in Variable & if possible make them cuda tensors\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients for the current epoch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Calculate gradients of whatever variable set to req_gardients = True\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take one step of the gradient descent for this epoch ! \n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        tloss += loss.data[0]\n",
    "        \n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            lossvsiter_adadel.append(running_loss / 2000)\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    loss_list.append(tloss)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"lossvsiter_adadel.pkl\",\"wb\") as f:\n",
    "    pickle.dump(lossvsiter_adadel,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 39 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    images=images.cuda()\n",
    "    labels=labels.cuda()\n",
    "    try:\n",
    "        outputs = net(Variable(images))\n",
    "    except RuntimeError as re:\n",
    "        print(outputs.is_cuda)\n",
    "        print(str(re))\n",
    "        sys.exit()\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Accuracy of the network on the 50000 trained images: 34 %\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader, 0):\n",
    "    images, labels = data\n",
    "    images=images.cuda()\n",
    "    labels=labels.cuda()\n",
    "    try:\n",
    "        outputs = net(Variable(images))\n",
    "    except RuntimeError as re:\n",
    "        print(outputs.is_cuda)\n",
    "        print(str(re))\n",
    "        sys.exit()\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    if (i%1000) == 0:\n",
    "        print(i)\n",
    "\n",
    "print('Accuracy of the network on the 50000 trained images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 50 %\n",
      "Accuracy of   car : 44 %\n",
      "Accuracy of  bird :  7 %\n",
      "Accuracy of   cat : 18 %\n",
      "Accuracy of  deer :  6 %\n",
      "Accuracy of   dog : 39 %\n",
      "Accuracy of  frog : 60 %\n",
      "Accuracy of horse : 46 %\n",
      "Accuracy of  ship : 32 %\n",
      "Accuracy of truck : 45 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    images=images.cuda()\n",
    "    labels=labels.cuda()\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2c34054f60>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBYWDiIi0oHAQEZEW/j99T7dJBTsnUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2c945cfb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
