{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture is conv->pool->conv->pool->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader= torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "classes=('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 11 Layers : 8 conv layers and 3 fully connected layers !\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3,padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 12, 3,padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(12)\n",
    "        self.conv3 = nn.Conv2d(12,20, 3,padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(20)\n",
    "        self.conv4 = nn.Conv2d(20,24, 3,padding=1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(24)\n",
    "        self.conv5 = nn.Conv2d(24,32, 3,padding=1)\n",
    "        self.conv5_bn = nn.BatchNorm2d(32)\n",
    "        self.conv6 = nn.Conv2d(32,48, 3,padding=1)\n",
    "        self.conv6_bn = nn.BatchNorm2d(48)\n",
    "        self.conv7 = nn.Conv2d(48,64, 3,padding=1)\n",
    "        self.conv7_bn = nn.BatchNorm2d(64)\n",
    "        self.conv8 = nn.Conv2d(64,72, 3,padding=1)\n",
    "        self.conv8_bn = nn.BatchNorm2d(72)\n",
    "        self.conv9 = nn.Conv2d(72,80, 3,padding=1)\n",
    "        self.conv9_bn = nn.BatchNorm2d(80)\n",
    "        self.fc1 = nn.Linear(80*4*4, 120)\n",
    "        self.fc1_bn = nn.BatchNorm2d(120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc2_bn = nn.BatchNorm2d(84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.conv1(x))\n",
    "        x = F.sigmoid(self.conv2_bn((self.conv2(x))))\n",
    "        x = self.pool(F.sigmoid(self.conv3_bn((self.conv3(x)))))\n",
    "        x = F.sigmoid(self.conv4_bn((self.conv4(x))))\n",
    "        x = F.sigmoid(self.conv5_bn((self.conv5(x))))\n",
    "        x = self.pool(F.sigmoid(self.conv6_bn((self.conv6(x)))))\n",
    "        x = F.sigmoid(self.conv7_bn((self.conv7(x))))\n",
    "        x = F.sigmoid(self.conv8_bn((self.conv8(x))))\n",
    "        x = self.pool(F.sigmoid(self.conv9_bn((self.conv9(x)))))\n",
    "\n",
    "        x = x.view(-1, 80*4*4)\n",
    "        x = F.sigmoid(self.fc1_bn(self.fc1(x)))\n",
    "        x = F.sigmoid(self.fc2_bn(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create an instance of the model on CUDA\n",
    "def new_net(lrate,wd):\n",
    "\n",
    "    net = Net().cuda()\n",
    "\n",
    "    # net=Net()\n",
    "\n",
    "    lossvsiter=[]\n",
    "\n",
    "    # To see if the model is on CUDA or not !\n",
    "    if (next(net.parameters()).is_cuda) :\n",
    "        print(\"The model is on CUDA\")\n",
    "    else :\n",
    "        print(\"The model is on CPU\")\n",
    "\n",
    "    # Import the optimizers \n",
    "    import torch.optim as optim\n",
    "\n",
    "    # Declare a loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Declare an optimizer\n",
    "    optimizer = optim.Adam(net.parameters(),lr=lrate,weight_decay=wd)\n",
    "\n",
    "    #No of iterations !\n",
    "    iterations = 25\n",
    "\n",
    "\n",
    "    for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "        # Reset the loss for the current epoch !\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable & if possible make them cuda tensors\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "            # zero the parameter gradients for the current epoch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "\n",
    "            # forward\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate gradients of whatever variable set to req_gardients = True\n",
    "            loss.backward()\n",
    "\n",
    "            # Take one step of the gradient descent for this epoch ! \n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                lossvsiter.append(running_loss / 2000)\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "    return lossvsiter,net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 2.104\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 1.921\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 1.820\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 1.720\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 1.681\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 1.633\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 1.551\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 1.515\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 1.469\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 1.476\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 1.458\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 1.430\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 1.379\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 1.374\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 1.374\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 1.380\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 1.354\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 1.350\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 1.305\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 1.298\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 1.301\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 1.307\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 1.308\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 1.273\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 1.242\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 1.223\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 1.250\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 1.240\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 1.208\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 1.222\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 1.180\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 1.209\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 1.189\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 1.205\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 1.170\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 1.165\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 1.160\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 1.136\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 1.125\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 1.155\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 1.131\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 1.143\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 1.107\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 1.108\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 1.096\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 1.102\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 1.103\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 1.089\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 1.067\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 1.080\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 1.083\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 1.078\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 1.099\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 1.036\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 1.035\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 1.057\n",
      "[Epoch :: 10, Mini Batch ::  6000] loss: 1.052\n",
      "[Epoch :: 10, Mini Batch ::  8000] loss: 1.020\n",
      "[Epoch :: 10, Mini Batch :: 10000] loss: 1.059\n",
      "[Epoch :: 10, Mini Batch :: 12000] loss: 1.038\n",
      "[Epoch :: 11, Mini Batch ::  2000] loss: 1.023\n",
      "[Epoch :: 11, Mini Batch ::  4000] loss: 1.015\n",
      "[Epoch :: 11, Mini Batch ::  6000] loss: 0.995\n",
      "[Epoch :: 11, Mini Batch ::  8000] loss: 1.038\n",
      "[Epoch :: 11, Mini Batch :: 10000] loss: 1.014\n",
      "[Epoch :: 11, Mini Batch :: 12000] loss: 0.998\n",
      "[Epoch :: 12, Mini Batch ::  2000] loss: 0.995\n",
      "[Epoch :: 12, Mini Batch ::  4000] loss: 0.999\n",
      "[Epoch :: 12, Mini Batch ::  6000] loss: 1.004\n",
      "[Epoch :: 12, Mini Batch ::  8000] loss: 0.972\n",
      "[Epoch :: 12, Mini Batch :: 10000] loss: 0.997\n",
      "[Epoch :: 12, Mini Batch :: 12000] loss: 0.950\n",
      "[Epoch :: 13, Mini Batch ::  2000] loss: 0.954\n",
      "[Epoch :: 13, Mini Batch ::  4000] loss: 0.979\n",
      "[Epoch :: 13, Mini Batch ::  6000] loss: 0.976\n",
      "[Epoch :: 13, Mini Batch ::  8000] loss: 0.976\n",
      "[Epoch :: 13, Mini Batch :: 10000] loss: 0.984\n",
      "[Epoch :: 13, Mini Batch :: 12000] loss: 0.940\n",
      "[Epoch :: 14, Mini Batch ::  2000] loss: 0.939\n",
      "[Epoch :: 14, Mini Batch ::  4000] loss: 0.936\n",
      "[Epoch :: 14, Mini Batch ::  6000] loss: 0.925\n",
      "[Epoch :: 14, Mini Batch ::  8000] loss: 0.944\n",
      "[Epoch :: 14, Mini Batch :: 10000] loss: 0.963\n",
      "[Epoch :: 14, Mini Batch :: 12000] loss: 0.943\n",
      "[Epoch :: 15, Mini Batch ::  2000] loss: 0.916\n",
      "[Epoch :: 15, Mini Batch ::  4000] loss: 0.935\n",
      "[Epoch :: 15, Mini Batch ::  6000] loss: 0.890\n",
      "[Epoch :: 15, Mini Batch ::  8000] loss: 0.921\n",
      "[Epoch :: 15, Mini Batch :: 10000] loss: 0.916\n",
      "[Epoch :: 15, Mini Batch :: 12000] loss: 0.900\n",
      "[Epoch :: 16, Mini Batch ::  2000] loss: 0.879\n",
      "[Epoch :: 16, Mini Batch ::  4000] loss: 0.888\n",
      "[Epoch :: 16, Mini Batch ::  6000] loss: 0.921\n",
      "[Epoch :: 16, Mini Batch ::  8000] loss: 0.894\n",
      "[Epoch :: 16, Mini Batch :: 10000] loss: 0.897\n",
      "[Epoch :: 16, Mini Batch :: 12000] loss: 0.909\n",
      "[Epoch :: 17, Mini Batch ::  2000] loss: 0.881\n",
      "[Epoch :: 17, Mini Batch ::  4000] loss: 0.873\n",
      "[Epoch :: 17, Mini Batch ::  6000] loss: 0.876\n",
      "[Epoch :: 17, Mini Batch ::  8000] loss: 0.852\n",
      "[Epoch :: 17, Mini Batch :: 10000] loss: 0.873\n",
      "[Epoch :: 17, Mini Batch :: 12000] loss: 0.888\n",
      "[Epoch :: 18, Mini Batch ::  2000] loss: 0.863\n",
      "[Epoch :: 18, Mini Batch ::  4000] loss: 0.862\n",
      "[Epoch :: 18, Mini Batch ::  6000] loss: 0.862\n",
      "[Epoch :: 18, Mini Batch ::  8000] loss: 0.889\n",
      "[Epoch :: 18, Mini Batch :: 10000] loss: 0.867\n",
      "[Epoch :: 18, Mini Batch :: 12000] loss: 0.845\n",
      "[Epoch :: 19, Mini Batch ::  2000] loss: 0.827\n",
      "[Epoch :: 19, Mini Batch ::  4000] loss: 0.850\n",
      "[Epoch :: 19, Mini Batch ::  6000] loss: 0.854\n",
      "[Epoch :: 19, Mini Batch ::  8000] loss: 0.855\n",
      "[Epoch :: 19, Mini Batch :: 10000] loss: 0.851\n",
      "[Epoch :: 19, Mini Batch :: 12000] loss: 0.841\n",
      "[Epoch :: 20, Mini Batch ::  2000] loss: 0.823\n",
      "[Epoch :: 20, Mini Batch ::  4000] loss: 0.813\n",
      "[Epoch :: 20, Mini Batch ::  6000] loss: 0.819\n",
      "[Epoch :: 20, Mini Batch ::  8000] loss: 0.804\n",
      "[Epoch :: 20, Mini Batch :: 10000] loss: 0.835\n",
      "[Epoch :: 20, Mini Batch :: 12000] loss: 0.832\n",
      "[Epoch :: 21, Mini Batch ::  2000] loss: 0.801\n",
      "[Epoch :: 21, Mini Batch ::  4000] loss: 0.786\n",
      "[Epoch :: 21, Mini Batch ::  6000] loss: 0.800\n",
      "[Epoch :: 21, Mini Batch ::  8000] loss: 0.816\n",
      "[Epoch :: 21, Mini Batch :: 10000] loss: 0.776\n",
      "[Epoch :: 21, Mini Batch :: 12000] loss: 0.823\n",
      "[Epoch :: 22, Mini Batch ::  2000] loss: 0.770\n",
      "[Epoch :: 22, Mini Batch ::  4000] loss: 0.788\n",
      "[Epoch :: 22, Mini Batch ::  6000] loss: 0.809\n",
      "[Epoch :: 22, Mini Batch ::  8000] loss: 0.788\n",
      "[Epoch :: 22, Mini Batch :: 10000] loss: 0.799\n",
      "[Epoch :: 22, Mini Batch :: 12000] loss: 0.822\n",
      "[Epoch :: 23, Mini Batch ::  2000] loss: 0.750\n",
      "[Epoch :: 23, Mini Batch ::  4000] loss: 0.786\n",
      "[Epoch :: 23, Mini Batch ::  6000] loss: 0.774\n",
      "[Epoch :: 23, Mini Batch ::  8000] loss: 0.788\n",
      "[Epoch :: 23, Mini Batch :: 10000] loss: 0.777\n",
      "[Epoch :: 23, Mini Batch :: 12000] loss: 0.769\n",
      "[Epoch :: 24, Mini Batch ::  2000] loss: 0.752\n",
      "[Epoch :: 24, Mini Batch ::  4000] loss: 0.757\n",
      "[Epoch :: 24, Mini Batch ::  6000] loss: 0.751\n",
      "[Epoch :: 24, Mini Batch ::  8000] loss: 0.758\n",
      "[Epoch :: 24, Mini Batch :: 10000] loss: 0.772\n",
      "[Epoch :: 24, Mini Batch :: 12000] loss: 0.763\n",
      "[Epoch :: 25, Mini Batch ::  2000] loss: 0.754\n",
      "[Epoch :: 25, Mini Batch ::  4000] loss: 0.731\n",
      "[Epoch :: 25, Mini Batch ::  6000] loss: 0.737\n",
      "[Epoch :: 25, Mini Batch ::  8000] loss: 0.754\n",
      "[Epoch :: 25, Mini Batch :: 10000] loss: 0.768\n",
      "[Epoch :: 25, Mini Batch :: 12000] loss: 0.740\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "lossvsiter_crazy_architecture_with_batch_norm,model_crazy_architecture_with_batch_norm=new_net(0.0005,1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./results/lossvsiter_crazy_architecture_with_batch_norm_with_sigmoid.pkl\",\"wb\") as f:\n",
    "    pickle.dump(lossvsiter_crazy_architecture_with_batch_norm,f)\n",
    "    \n",
    "with open(\"./results/model_crazy_architecture_with_batch_norm_with_sigmoid.pkl\",\"wb\") as f:\n",
    "    pickle.dump(model_crazy_architecture_with_batch_norm,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_accuracy(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images=images.cuda()\n",
    "        labels=labels.cuda()\n",
    "        try:\n",
    "            outputs = model(Variable(images))\n",
    "        except RuntimeError as re:\n",
    "            print(outputs.is_cuda)\n",
    "            print(str(re))\n",
    "            sys.exit()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        accuracy_percentage= 100 * correct / total\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d' % (accuracy_percentage))\n",
    "    print(\"The network predicted correct for %s\"%(correct))\n",
    "    return accuracy_percentage,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_accuracy(model):\n",
    "    net = model\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        images, labels = data\n",
    "        images=images.cuda()\n",
    "        labels=labels.cuda()\n",
    "        try:\n",
    "            outputs = net(Variable(images))\n",
    "        except RuntimeError as re:\n",
    "            print(outputs.is_cuda)\n",
    "            print(str(re))\n",
    "            sys.exit()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        if (i%1000) == 0:\n",
    "            print(i)\n",
    "\n",
    "    print('Accuracy of the network on the 50000 trained images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 71\n",
      "The network predicted correct for 7100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(71.0, 7100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Accuracy of the network on the 50000 trained images: 76 %\n"
     ]
    }
   ],
   "source": [
    "train_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 69 %\n",
      "Accuracy of   car : 83 %\n",
      "Accuracy of  bird : 58 %\n",
      "Accuracy of   cat : 51 %\n",
      "Accuracy of  deer : 70 %\n",
      "Accuracy of   dog : 65 %\n",
      "Accuracy of  frog : 72 %\n",
      "Accuracy of horse : 73 %\n",
      "Accuracy of  ship : 80 %\n",
      "Accuracy of truck : 85 %\n"
     ]
    }
   ],
   "source": [
    "net=model_crazy_architecture_with_batch_norm\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    images=images.cuda()\n",
    "    labels=labels.cuda()\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUuP/AeLY5VwDQqVeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7d8bb8828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(lossvsiter_crazy_architecture_with_batch_norm)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('every 2000th mini-batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets save the model !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_crazy_architecture_with_batch_norm.state_dict(),\"./models/model_crazy_architecture_with_batch_norm_with_sigmoid_25_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It seems the loss will further decrease with more iterations !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 26, Mini Batch ::  2000] loss: 0.742\n",
      "[Epoch :: 26, Mini Batch ::  4000] loss: 0.749\n",
      "[Epoch :: 26, Mini Batch ::  6000] loss: 0.723\n",
      "[Epoch :: 26, Mini Batch ::  8000] loss: 0.746\n",
      "[Epoch :: 26, Mini Batch :: 10000] loss: 0.741\n",
      "[Epoch :: 26, Mini Batch :: 12000] loss: 0.752\n",
      "[Epoch :: 27, Mini Batch ::  2000] loss: 0.701\n",
      "[Epoch :: 27, Mini Batch ::  4000] loss: 0.733\n",
      "[Epoch :: 27, Mini Batch ::  6000] loss: 0.748\n",
      "[Epoch :: 27, Mini Batch ::  8000] loss: 0.707\n",
      "[Epoch :: 27, Mini Batch :: 10000] loss: 0.696\n",
      "[Epoch :: 27, Mini Batch :: 12000] loss: 0.753\n",
      "[Epoch :: 28, Mini Batch ::  2000] loss: 0.706\n",
      "[Epoch :: 28, Mini Batch ::  4000] loss: 0.730\n",
      "[Epoch :: 28, Mini Batch ::  6000] loss: 0.697\n",
      "[Epoch :: 28, Mini Batch ::  8000] loss: 0.712\n",
      "[Epoch :: 28, Mini Batch :: 10000] loss: 0.718\n",
      "[Epoch :: 28, Mini Batch :: 12000] loss: 0.714\n",
      "[Epoch :: 29, Mini Batch ::  2000] loss: 0.707\n",
      "[Epoch :: 29, Mini Batch ::  4000] loss: 0.683\n",
      "[Epoch :: 29, Mini Batch ::  6000] loss: 0.694\n",
      "[Epoch :: 29, Mini Batch ::  8000] loss: 0.684\n",
      "[Epoch :: 29, Mini Batch :: 10000] loss: 0.689\n",
      "[Epoch :: 29, Mini Batch :: 12000] loss: 0.711\n",
      "[Epoch :: 30, Mini Batch ::  2000] loss: 0.661\n",
      "[Epoch :: 30, Mini Batch ::  4000] loss: 0.681\n",
      "[Epoch :: 30, Mini Batch ::  6000] loss: 0.690\n",
      "[Epoch :: 30, Mini Batch ::  8000] loss: 0.676\n",
      "[Epoch :: 30, Mini Batch :: 10000] loss: 0.685\n",
      "[Epoch :: 30, Mini Batch :: 12000] loss: 0.697\n",
      "[Epoch :: 31, Mini Batch ::  2000] loss: 0.653\n",
      "[Epoch :: 31, Mini Batch ::  4000] loss: 0.653\n",
      "[Epoch :: 31, Mini Batch ::  6000] loss: 0.701\n",
      "[Epoch :: 31, Mini Batch ::  8000] loss: 0.683\n",
      "[Epoch :: 31, Mini Batch :: 10000] loss: 0.677\n",
      "[Epoch :: 31, Mini Batch :: 12000] loss: 0.681\n",
      "[Epoch :: 32, Mini Batch ::  2000] loss: 0.642\n",
      "[Epoch :: 32, Mini Batch ::  4000] loss: 0.654\n",
      "[Epoch :: 32, Mini Batch ::  6000] loss: 0.703\n",
      "[Epoch :: 32, Mini Batch ::  8000] loss: 0.652\n",
      "[Epoch :: 32, Mini Batch :: 10000] loss: 0.644\n",
      "[Epoch :: 32, Mini Batch :: 12000] loss: 0.661\n",
      "[Epoch :: 33, Mini Batch ::  2000] loss: 0.659\n",
      "[Epoch :: 33, Mini Batch ::  4000] loss: 0.639\n",
      "[Epoch :: 33, Mini Batch ::  6000] loss: 0.646\n",
      "[Epoch :: 33, Mini Batch ::  8000] loss: 0.648\n",
      "[Epoch :: 33, Mini Batch :: 10000] loss: 0.665\n",
      "[Epoch :: 33, Mini Batch :: 12000] loss: 0.654\n",
      "[Epoch :: 34, Mini Batch ::  2000] loss: 0.632\n",
      "[Epoch :: 34, Mini Batch ::  4000] loss: 0.626\n",
      "[Epoch :: 34, Mini Batch ::  6000] loss: 0.634\n",
      "[Epoch :: 34, Mini Batch ::  8000] loss: 0.647\n",
      "[Epoch :: 34, Mini Batch :: 10000] loss: 0.652\n",
      "[Epoch :: 34, Mini Batch :: 12000] loss: 0.655\n",
      "[Epoch :: 35, Mini Batch ::  2000] loss: 0.616\n",
      "[Epoch :: 35, Mini Batch ::  4000] loss: 0.622\n",
      "[Epoch :: 35, Mini Batch ::  6000] loss: 0.659\n",
      "[Epoch :: 35, Mini Batch ::  8000] loss: 0.619\n",
      "[Epoch :: 35, Mini Batch :: 10000] loss: 0.641\n",
      "[Epoch :: 35, Mini Batch :: 12000] loss: 0.634\n",
      "[Epoch :: 36, Mini Batch ::  2000] loss: 0.619\n",
      "[Epoch :: 36, Mini Batch ::  4000] loss: 0.601\n",
      "[Epoch :: 36, Mini Batch ::  6000] loss: 0.614\n",
      "[Epoch :: 36, Mini Batch ::  8000] loss: 0.619\n",
      "[Epoch :: 36, Mini Batch :: 10000] loss: 0.629\n",
      "[Epoch :: 36, Mini Batch :: 12000] loss: 0.635\n",
      "[Epoch :: 37, Mini Batch ::  2000] loss: 0.592\n",
      "[Epoch :: 37, Mini Batch ::  4000] loss: 0.610\n",
      "[Epoch :: 37, Mini Batch ::  6000] loss: 0.595\n",
      "[Epoch :: 37, Mini Batch ::  8000] loss: 0.602\n",
      "[Epoch :: 37, Mini Batch :: 10000] loss: 0.626\n",
      "[Epoch :: 37, Mini Batch :: 12000] loss: 0.632\n",
      "[Epoch :: 38, Mini Batch ::  2000] loss: 0.565\n",
      "[Epoch :: 38, Mini Batch ::  4000] loss: 0.595\n",
      "[Epoch :: 38, Mini Batch ::  6000] loss: 0.592\n",
      "[Epoch :: 38, Mini Batch ::  8000] loss: 0.597\n",
      "[Epoch :: 38, Mini Batch :: 10000] loss: 0.597\n",
      "[Epoch :: 38, Mini Batch :: 12000] loss: 0.620\n",
      "[Epoch :: 39, Mini Batch ::  2000] loss: 0.574\n",
      "[Epoch :: 39, Mini Batch ::  4000] loss: 0.586\n",
      "[Epoch :: 39, Mini Batch ::  6000] loss: 0.602\n",
      "[Epoch :: 39, Mini Batch ::  8000] loss: 0.605\n",
      "[Epoch :: 39, Mini Batch :: 10000] loss: 0.590\n",
      "[Epoch :: 39, Mini Batch :: 12000] loss: 0.577\n",
      "[Epoch :: 40, Mini Batch ::  2000] loss: 0.578\n",
      "[Epoch :: 40, Mini Batch ::  4000] loss: 0.578\n",
      "[Epoch :: 40, Mini Batch ::  6000] loss: 0.575\n",
      "[Epoch :: 40, Mini Batch ::  8000] loss: 0.602\n",
      "[Epoch :: 40, Mini Batch :: 10000] loss: 0.578\n",
      "[Epoch :: 40, Mini Batch :: 12000] loss: 0.589\n",
      "[Epoch :: 41, Mini Batch ::  2000] loss: 0.553\n",
      "[Epoch :: 41, Mini Batch ::  4000] loss: 0.567\n",
      "[Epoch :: 41, Mini Batch ::  6000] loss: 0.542\n",
      "[Epoch :: 41, Mini Batch ::  8000] loss: 0.581\n",
      "[Epoch :: 41, Mini Batch :: 10000] loss: 0.557\n",
      "[Epoch :: 41, Mini Batch :: 12000] loss: 0.574\n",
      "[Epoch :: 42, Mini Batch ::  2000] loss: 0.520\n",
      "[Epoch :: 42, Mini Batch ::  4000] loss: 0.549\n",
      "[Epoch :: 42, Mini Batch ::  6000] loss: 0.552\n",
      "[Epoch :: 42, Mini Batch ::  8000] loss: 0.560\n",
      "[Epoch :: 42, Mini Batch :: 10000] loss: 0.565\n",
      "[Epoch :: 42, Mini Batch :: 12000] loss: 0.581\n",
      "[Epoch :: 43, Mini Batch ::  2000] loss: 0.514\n",
      "[Epoch :: 43, Mini Batch ::  4000] loss: 0.547\n",
      "[Epoch :: 43, Mini Batch ::  6000] loss: 0.540\n",
      "[Epoch :: 43, Mini Batch ::  8000] loss: 0.563\n",
      "[Epoch :: 43, Mini Batch :: 10000] loss: 0.554\n",
      "[Epoch :: 43, Mini Batch :: 12000] loss: 0.561\n",
      "[Epoch :: 44, Mini Batch ::  2000] loss: 0.510\n",
      "[Epoch :: 44, Mini Batch ::  4000] loss: 0.536\n",
      "[Epoch :: 44, Mini Batch ::  6000] loss: 0.516\n",
      "[Epoch :: 44, Mini Batch ::  8000] loss: 0.552\n",
      "[Epoch :: 44, Mini Batch :: 10000] loss: 0.531\n",
      "[Epoch :: 44, Mini Batch :: 12000] loss: 0.553\n",
      "[Epoch :: 45, Mini Batch ::  2000] loss: 0.518\n",
      "[Epoch :: 45, Mini Batch ::  4000] loss: 0.525\n",
      "[Epoch :: 45, Mini Batch ::  6000] loss: 0.526\n",
      "[Epoch :: 45, Mini Batch ::  8000] loss: 0.537\n",
      "[Epoch :: 45, Mini Batch :: 10000] loss: 0.532\n",
      "[Epoch :: 45, Mini Batch :: 12000] loss: 0.531\n",
      "[Epoch :: 46, Mini Batch ::  2000] loss: 0.524\n",
      "[Epoch :: 46, Mini Batch ::  4000] loss: 0.500\n",
      "[Epoch :: 46, Mini Batch ::  6000] loss: 0.510\n",
      "[Epoch :: 46, Mini Batch ::  8000] loss: 0.532\n",
      "[Epoch :: 46, Mini Batch :: 10000] loss: 0.527\n",
      "[Epoch :: 46, Mini Batch :: 12000] loss: 0.515\n",
      "[Epoch :: 47, Mini Batch ::  2000] loss: 0.503\n",
      "[Epoch :: 47, Mini Batch ::  4000] loss: 0.489\n",
      "[Epoch :: 47, Mini Batch ::  6000] loss: 0.511\n",
      "[Epoch :: 47, Mini Batch ::  8000] loss: 0.531\n",
      "[Epoch :: 47, Mini Batch :: 10000] loss: 0.496\n",
      "[Epoch :: 47, Mini Batch :: 12000] loss: 0.499\n",
      "[Epoch :: 48, Mini Batch ::  2000] loss: 0.492\n",
      "[Epoch :: 48, Mini Batch ::  4000] loss: 0.518\n",
      "[Epoch :: 48, Mini Batch ::  6000] loss: 0.500\n",
      "[Epoch :: 48, Mini Batch ::  8000] loss: 0.525\n",
      "[Epoch :: 48, Mini Batch :: 10000] loss: 0.539\n",
      "[Epoch :: 48, Mini Batch :: 12000] loss: 0.516\n",
      "[Epoch :: 49, Mini Batch ::  2000] loss: 0.501\n",
      "[Epoch :: 49, Mini Batch ::  4000] loss: 0.492\n",
      "[Epoch :: 49, Mini Batch ::  6000] loss: 0.500\n",
      "[Epoch :: 49, Mini Batch ::  8000] loss: 0.524\n",
      "[Epoch :: 49, Mini Batch :: 10000] loss: 0.517\n",
      "[Epoch :: 49, Mini Batch :: 12000] loss: 0.504\n",
      "[Epoch :: 50, Mini Batch ::  2000] loss: 0.474\n",
      "[Epoch :: 50, Mini Batch ::  4000] loss: 0.476\n",
      "[Epoch :: 50, Mini Batch ::  6000] loss: 0.482\n",
      "[Epoch :: 50, Mini Batch ::  8000] loss: 0.496\n",
      "[Epoch :: 50, Mini Batch :: 10000] loss: 0.482\n",
      "[Epoch :: 50, Mini Batch :: 12000] loss: 0.508\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net=model_crazy_architecture_with_batch_norm\n",
    "lossvsiter=lossvsiter_crazy_architecture_with_batch_norm\n",
    "# To see if the model is on CUDA or not !\n",
    "if (next(net.parameters()).is_cuda) :\n",
    "    print(\"The model is on CUDA\")\n",
    "else :\n",
    "    print(\"The model is on CPU\")\n",
    "\n",
    "# Import the optimizers \n",
    "import torch.optim as optim\n",
    "\n",
    "# Declare a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Declare an optimizer\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.0005,weight_decay=1e-7)\n",
    "\n",
    "#No of iterations !\n",
    "iterations = 25\n",
    "\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "    # Reset the loss for the current epoch !\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable & if possible make them cuda tensors\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients for the current epoch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Calculate gradients of whatever variable set to req_gardients = True\n",
    "        loss.backward()\n",
    "\n",
    "        # Take one step of the gradient descent for this epoch ! \n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                  (epoch + 1+25, i + 1, running_loss / 2000))\n",
    "            lossvsiter.append(running_loss / 2000)\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets see the result ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 74\n",
      "The network predicted correct for 7403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(74.03, 7403)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Accuracy of the network on the 50000 trained images: 84 %\n"
     ]
    }
   ],
   "source": [
    "train_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_crazy_architecture_with_batch_norm.state_dict(),\"./models/model_crazy_architecture_with_batch_norm_with_sigmoid-50-epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets run it again for 25 more ... yay !!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_crazy_architecture_with_batch_norm.load_state_dict(torch.load(\"./models/model_crazy_architecture_with_batch_norm-50-epochs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (conv2): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_bn): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv3): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_bn): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv4): Conv2d(20, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv5): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv6): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv7): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv8): Conv2d(64, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv8_bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv9): Conv2d(72, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv9_bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc1): Linear (1280 -> 120)\n",
       "  (fc1_bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc2): Linear (120 -> 84)\n",
       "  (fc2_bn): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc3): Linear (84 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_crazy_architecture_with_batch_norm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 0.150\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 0.150\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 0.140\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 0.138\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 0.139\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 0.149\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 0.098\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 0.099\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 0.109\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 0.111\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 0.109\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 0.117\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 0.084\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 0.097\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 0.092\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 0.093\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 0.102\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 0.107\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 0.070\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 0.081\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 0.086\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 0.091\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 0.102\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 0.086\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 0.068\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 0.075\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 0.081\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 0.079\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 0.090\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 0.088\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 0.064\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 0.081\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 0.076\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 0.075\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 0.079\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 0.080\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 0.061\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 0.063\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 0.071\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 0.066\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 0.079\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 0.067\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 0.062\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 0.065\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 0.061\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 0.073\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 0.067\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 0.081\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 0.051\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 0.060\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 0.072\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 0.064\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 0.067\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 0.079\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 0.049\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 0.058\n",
      "[Epoch :: 10, Mini Batch ::  6000] loss: 0.071\n",
      "[Epoch :: 10, Mini Batch ::  8000] loss: 0.074\n",
      "[Epoch :: 10, Mini Batch :: 10000] loss: 0.064\n",
      "[Epoch :: 10, Mini Batch :: 12000] loss: 0.062\n",
      "[Epoch :: 11, Mini Batch ::  2000] loss: 0.051\n",
      "[Epoch :: 11, Mini Batch ::  4000] loss: 0.061\n",
      "[Epoch :: 11, Mini Batch ::  6000] loss: 0.061\n",
      "[Epoch :: 11, Mini Batch ::  8000] loss: 0.057\n",
      "[Epoch :: 11, Mini Batch :: 10000] loss: 0.063\n",
      "[Epoch :: 11, Mini Batch :: 12000] loss: 0.064\n",
      "[Epoch :: 12, Mini Batch ::  2000] loss: 0.055\n",
      "[Epoch :: 12, Mini Batch ::  4000] loss: 0.049\n",
      "[Epoch :: 12, Mini Batch ::  6000] loss: 0.061\n",
      "[Epoch :: 12, Mini Batch ::  8000] loss: 0.054\n",
      "[Epoch :: 12, Mini Batch :: 10000] loss: 0.067\n",
      "[Epoch :: 12, Mini Batch :: 12000] loss: 0.066\n",
      "[Epoch :: 13, Mini Batch ::  2000] loss: 0.045\n",
      "[Epoch :: 13, Mini Batch ::  4000] loss: 0.057\n",
      "[Epoch :: 13, Mini Batch ::  6000] loss: 0.054\n",
      "[Epoch :: 13, Mini Batch ::  8000] loss: 0.058\n",
      "[Epoch :: 13, Mini Batch :: 10000] loss: 0.064\n",
      "[Epoch :: 13, Mini Batch :: 12000] loss: 0.060\n",
      "[Epoch :: 14, Mini Batch ::  2000] loss: 0.053\n",
      "[Epoch :: 14, Mini Batch ::  4000] loss: 0.054\n",
      "[Epoch :: 14, Mini Batch ::  6000] loss: 0.049\n",
      "[Epoch :: 14, Mini Batch ::  8000] loss: 0.058\n",
      "[Epoch :: 14, Mini Batch :: 10000] loss: 0.058\n",
      "[Epoch :: 14, Mini Batch :: 12000] loss: 0.059\n",
      "[Epoch :: 15, Mini Batch ::  2000] loss: 0.043\n",
      "[Epoch :: 15, Mini Batch ::  4000] loss: 0.055\n",
      "[Epoch :: 15, Mini Batch ::  6000] loss: 0.052\n",
      "[Epoch :: 15, Mini Batch ::  8000] loss: 0.060\n",
      "[Epoch :: 15, Mini Batch :: 10000] loss: 0.063\n",
      "[Epoch :: 15, Mini Batch :: 12000] loss: 0.046\n",
      "[Epoch :: 16, Mini Batch ::  2000] loss: 0.054\n",
      "[Epoch :: 16, Mini Batch ::  4000] loss: 0.042\n",
      "[Epoch :: 16, Mini Batch ::  6000] loss: 0.049\n",
      "[Epoch :: 16, Mini Batch ::  8000] loss: 0.048\n",
      "[Epoch :: 16, Mini Batch :: 10000] loss: 0.059\n",
      "[Epoch :: 16, Mini Batch :: 12000] loss: 0.051\n",
      "[Epoch :: 17, Mini Batch ::  2000] loss: 0.047\n",
      "[Epoch :: 17, Mini Batch ::  4000] loss: 0.056\n",
      "[Epoch :: 17, Mini Batch ::  6000] loss: 0.048\n",
      "[Epoch :: 17, Mini Batch ::  8000] loss: 0.051\n",
      "[Epoch :: 17, Mini Batch :: 10000] loss: 0.054\n",
      "[Epoch :: 17, Mini Batch :: 12000] loss: 0.056\n",
      "[Epoch :: 18, Mini Batch ::  2000] loss: 0.034\n",
      "[Epoch :: 18, Mini Batch ::  4000] loss: 0.056\n",
      "[Epoch :: 18, Mini Batch ::  6000] loss: 0.041\n",
      "[Epoch :: 18, Mini Batch ::  8000] loss: 0.057\n",
      "[Epoch :: 18, Mini Batch :: 10000] loss: 0.060\n",
      "[Epoch :: 18, Mini Batch :: 12000] loss: 0.049\n",
      "[Epoch :: 19, Mini Batch ::  2000] loss: 0.029\n",
      "[Epoch :: 19, Mini Batch ::  4000] loss: 0.047\n",
      "[Epoch :: 19, Mini Batch ::  6000] loss: 0.046\n",
      "[Epoch :: 19, Mini Batch ::  8000] loss: 0.043\n",
      "[Epoch :: 19, Mini Batch :: 10000] loss: 0.055\n",
      "[Epoch :: 19, Mini Batch :: 12000] loss: 0.057\n",
      "[Epoch :: 20, Mini Batch ::  2000] loss: 0.040\n",
      "[Epoch :: 20, Mini Batch ::  4000] loss: 0.042\n",
      "[Epoch :: 20, Mini Batch ::  6000] loss: 0.050\n",
      "[Epoch :: 20, Mini Batch ::  8000] loss: 0.044\n",
      "[Epoch :: 20, Mini Batch :: 10000] loss: 0.054\n",
      "[Epoch :: 20, Mini Batch :: 12000] loss: 0.049\n",
      "[Epoch :: 21, Mini Batch ::  2000] loss: 0.038\n",
      "[Epoch :: 21, Mini Batch ::  4000] loss: 0.043\n",
      "[Epoch :: 21, Mini Batch ::  6000] loss: 0.052\n",
      "[Epoch :: 21, Mini Batch ::  8000] loss: 0.042\n",
      "[Epoch :: 21, Mini Batch :: 10000] loss: 0.047\n",
      "[Epoch :: 21, Mini Batch :: 12000] loss: 0.053\n",
      "[Epoch :: 22, Mini Batch ::  2000] loss: 0.043\n",
      "[Epoch :: 22, Mini Batch ::  4000] loss: 0.044\n",
      "[Epoch :: 22, Mini Batch ::  6000] loss: 0.045\n",
      "[Epoch :: 22, Mini Batch ::  8000] loss: 0.048\n",
      "[Epoch :: 22, Mini Batch :: 10000] loss: 0.042\n",
      "[Epoch :: 22, Mini Batch :: 12000] loss: 0.040\n",
      "[Epoch :: 23, Mini Batch ::  2000] loss: 0.039\n",
      "[Epoch :: 23, Mini Batch ::  4000] loss: 0.037\n",
      "[Epoch :: 23, Mini Batch ::  6000] loss: 0.048\n",
      "[Epoch :: 23, Mini Batch ::  8000] loss: 0.046\n",
      "[Epoch :: 23, Mini Batch :: 10000] loss: 0.063\n",
      "[Epoch :: 23, Mini Batch :: 12000] loss: 0.038\n",
      "[Epoch :: 24, Mini Batch ::  2000] loss: 0.040\n",
      "[Epoch :: 24, Mini Batch ::  4000] loss: 0.042\n",
      "[Epoch :: 24, Mini Batch ::  6000] loss: 0.038\n",
      "[Epoch :: 24, Mini Batch ::  8000] loss: 0.048\n",
      "[Epoch :: 24, Mini Batch :: 10000] loss: 0.043\n",
      "[Epoch :: 24, Mini Batch :: 12000] loss: 0.046\n",
      "[Epoch :: 25, Mini Batch ::  2000] loss: 0.041\n",
      "[Epoch :: 25, Mini Batch ::  4000] loss: 0.038\n",
      "[Epoch :: 25, Mini Batch ::  6000] loss: 0.041\n",
      "[Epoch :: 25, Mini Batch ::  8000] loss: 0.042\n",
      "[Epoch :: 25, Mini Batch :: 10000] loss: 0.037\n",
      "[Epoch :: 25, Mini Batch :: 12000] loss: 0.041\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net=model_crazy_architecture_with_batch_norm\n",
    "lossvsiter=lossvsiter_crazy_architecture_with_batch_norm\n",
    "# To see if the model is on CUDA or not !\n",
    "if (next(net.parameters()).is_cuda) :\n",
    "    print(\"The model is on CUDA\")\n",
    "else :\n",
    "    print(\"The model is on CPU\")\n",
    "\n",
    "# Import the optimizers \n",
    "import torch.optim as optim\n",
    "\n",
    "# Declare a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Declare an optimizer\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.0005,weight_decay=1e-7)\n",
    "\n",
    "#No of iterations !\n",
    "iterations = 25\n",
    "\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "    # Reset the loss for the current epoch !\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable & if possible make them cuda tensors\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients for the current epoch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Calculate gradients of whatever variable set to req_gardients = True\n",
    "        loss.backward()\n",
    "\n",
    "        # Take one step of the gradient descent for this epoch ! \n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            lossvsiter.append(running_loss / 2000)\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 81\n",
      "The network predicted correct for 8104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(81.04, 8104)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_crazy_architecture_with_batch_norm.state_dict(),\"./models/model_crazy_architecture_with_batch_norm-75-epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Accuracy of the network on the 50000 trained images: 99 %\n"
     ]
    }
   ],
   "source": [
    "train_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets get in a century"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 0.047\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 0.037\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 0.044\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 0.040\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 0.046\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 0.039\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 0.041\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 0.040\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 0.042\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 0.041\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 0.037\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 0.044\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 0.034\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 0.036\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 0.037\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 0.047\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 0.035\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 0.038\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 0.035\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 0.043\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 0.037\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 0.041\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 0.038\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 0.042\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 0.027\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 0.039\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 0.036\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 0.043\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 0.040\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 0.044\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 0.030\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 0.037\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 0.034\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 0.041\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 0.034\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 0.044\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 0.036\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 0.030\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 0.026\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 0.049\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 0.039\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 0.042\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 0.032\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 0.028\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 0.039\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 0.038\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 0.037\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 0.034\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 0.025\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 0.042\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 0.044\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 0.035\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 0.045\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 0.038\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 0.029\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 0.030\n",
      "[Epoch :: 10, Mini Batch ::  6000] loss: 0.040\n",
      "[Epoch :: 10, Mini Batch ::  8000] loss: 0.041\n",
      "[Epoch :: 10, Mini Batch :: 10000] loss: 0.036\n",
      "[Epoch :: 10, Mini Batch :: 12000] loss: 0.038\n",
      "[Epoch :: 11, Mini Batch ::  2000] loss: 0.030\n",
      "[Epoch :: 11, Mini Batch ::  4000] loss: 0.030\n",
      "[Epoch :: 11, Mini Batch ::  6000] loss: 0.033\n",
      "[Epoch :: 11, Mini Batch ::  8000] loss: 0.042\n",
      "[Epoch :: 11, Mini Batch :: 10000] loss: 0.032\n",
      "[Epoch :: 11, Mini Batch :: 12000] loss: 0.040\n",
      "[Epoch :: 12, Mini Batch ::  2000] loss: 0.024\n",
      "[Epoch :: 12, Mini Batch ::  4000] loss: 0.037\n",
      "[Epoch :: 12, Mini Batch ::  6000] loss: 0.033\n",
      "[Epoch :: 12, Mini Batch ::  8000] loss: 0.038\n",
      "[Epoch :: 12, Mini Batch :: 10000] loss: 0.037\n",
      "[Epoch :: 12, Mini Batch :: 12000] loss: 0.037\n",
      "[Epoch :: 13, Mini Batch ::  2000] loss: 0.034\n",
      "[Epoch :: 13, Mini Batch ::  4000] loss: 0.034\n",
      "[Epoch :: 13, Mini Batch ::  6000] loss: 0.029\n",
      "[Epoch :: 13, Mini Batch ::  8000] loss: 0.040\n",
      "[Epoch :: 13, Mini Batch :: 10000] loss: 0.034\n",
      "[Epoch :: 13, Mini Batch :: 12000] loss: 0.037\n",
      "[Epoch :: 14, Mini Batch ::  2000] loss: 0.024\n",
      "[Epoch :: 14, Mini Batch ::  4000] loss: 0.043\n",
      "[Epoch :: 14, Mini Batch ::  6000] loss: 0.035\n",
      "[Epoch :: 14, Mini Batch ::  8000] loss: 0.030\n",
      "[Epoch :: 14, Mini Batch :: 10000] loss: 0.030\n",
      "[Epoch :: 14, Mini Batch :: 12000] loss: 0.031\n",
      "[Epoch :: 15, Mini Batch ::  2000] loss: 0.026\n",
      "[Epoch :: 15, Mini Batch ::  4000] loss: 0.032\n",
      "[Epoch :: 15, Mini Batch ::  6000] loss: 0.035\n",
      "[Epoch :: 15, Mini Batch ::  8000] loss: 0.030\n",
      "[Epoch :: 15, Mini Batch :: 10000] loss: 0.036\n",
      "[Epoch :: 15, Mini Batch :: 12000] loss: 0.027\n",
      "[Epoch :: 16, Mini Batch ::  2000] loss: 0.029\n",
      "[Epoch :: 16, Mini Batch ::  4000] loss: 0.025\n",
      "[Epoch :: 16, Mini Batch ::  6000] loss: 0.034\n",
      "[Epoch :: 16, Mini Batch ::  8000] loss: 0.038\n",
      "[Epoch :: 16, Mini Batch :: 10000] loss: 0.035\n",
      "[Epoch :: 16, Mini Batch :: 12000] loss: 0.037\n",
      "[Epoch :: 17, Mini Batch ::  2000] loss: 0.020\n",
      "[Epoch :: 17, Mini Batch ::  4000] loss: 0.040\n",
      "[Epoch :: 17, Mini Batch ::  6000] loss: 0.030\n",
      "[Epoch :: 17, Mini Batch ::  8000] loss: 0.037\n",
      "[Epoch :: 17, Mini Batch :: 10000] loss: 0.037\n",
      "[Epoch :: 17, Mini Batch :: 12000] loss: 0.038\n",
      "[Epoch :: 18, Mini Batch ::  2000] loss: 0.036\n",
      "[Epoch :: 18, Mini Batch ::  4000] loss: 0.034\n",
      "[Epoch :: 18, Mini Batch ::  6000] loss: 0.032\n",
      "[Epoch :: 18, Mini Batch ::  8000] loss: 0.028\n",
      "[Epoch :: 18, Mini Batch :: 10000] loss: 0.037\n",
      "[Epoch :: 18, Mini Batch :: 12000] loss: 0.030\n",
      "[Epoch :: 19, Mini Batch ::  2000] loss: 0.026\n",
      "[Epoch :: 19, Mini Batch ::  4000] loss: 0.028\n",
      "[Epoch :: 19, Mini Batch ::  6000] loss: 0.037\n",
      "[Epoch :: 19, Mini Batch ::  8000] loss: 0.032\n",
      "[Epoch :: 19, Mini Batch :: 10000] loss: 0.032\n",
      "[Epoch :: 19, Mini Batch :: 12000] loss: 0.032\n",
      "[Epoch :: 20, Mini Batch ::  2000] loss: 0.030\n",
      "[Epoch :: 20, Mini Batch ::  4000] loss: 0.030\n",
      "[Epoch :: 20, Mini Batch ::  6000] loss: 0.029\n",
      "[Epoch :: 20, Mini Batch ::  8000] loss: 0.034\n",
      "[Epoch :: 20, Mini Batch :: 10000] loss: 0.037\n",
      "[Epoch :: 20, Mini Batch :: 12000] loss: 0.030\n",
      "[Epoch :: 21, Mini Batch ::  2000] loss: 0.022\n",
      "[Epoch :: 21, Mini Batch ::  4000] loss: 0.040\n",
      "[Epoch :: 21, Mini Batch ::  6000] loss: 0.028\n",
      "[Epoch :: 21, Mini Batch ::  8000] loss: 0.031\n",
      "[Epoch :: 21, Mini Batch :: 10000] loss: 0.030\n",
      "[Epoch :: 21, Mini Batch :: 12000] loss: 0.027\n",
      "[Epoch :: 22, Mini Batch ::  2000] loss: 0.029\n",
      "[Epoch :: 22, Mini Batch ::  4000] loss: 0.027\n",
      "[Epoch :: 22, Mini Batch ::  6000] loss: 0.039\n",
      "[Epoch :: 22, Mini Batch ::  8000] loss: 0.024\n",
      "[Epoch :: 22, Mini Batch :: 10000] loss: 0.042\n",
      "[Epoch :: 22, Mini Batch :: 12000] loss: 0.025\n",
      "[Epoch :: 23, Mini Batch ::  2000] loss: 0.023\n",
      "[Epoch :: 23, Mini Batch ::  4000] loss: 0.032\n",
      "[Epoch :: 23, Mini Batch ::  6000] loss: 0.030\n",
      "[Epoch :: 23, Mini Batch ::  8000] loss: 0.032\n",
      "[Epoch :: 23, Mini Batch :: 10000] loss: 0.017\n",
      "[Epoch :: 23, Mini Batch :: 12000] loss: 0.041\n",
      "[Epoch :: 24, Mini Batch ::  2000] loss: 0.030\n",
      "[Epoch :: 24, Mini Batch ::  4000] loss: 0.026\n",
      "[Epoch :: 24, Mini Batch ::  6000] loss: 0.028\n",
      "[Epoch :: 24, Mini Batch ::  8000] loss: 0.030\n",
      "[Epoch :: 24, Mini Batch :: 10000] loss: 0.025\n",
      "[Epoch :: 24, Mini Batch :: 12000] loss: 0.038\n",
      "[Epoch :: 25, Mini Batch ::  2000] loss: 0.027\n",
      "[Epoch :: 25, Mini Batch ::  4000] loss: 0.027\n",
      "[Epoch :: 25, Mini Batch ::  6000] loss: 0.032\n",
      "[Epoch :: 25, Mini Batch ::  8000] loss: 0.025\n",
      "[Epoch :: 25, Mini Batch :: 10000] loss: 0.033\n",
      "[Epoch :: 25, Mini Batch :: 12000] loss: 0.023\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net=model_crazy_architecture_with_batch_norm\n",
    "lossvsiter=lossvsiter_crazy_architecture_with_batch_norm\n",
    "# To see if the model is on CUDA or not !\n",
    "if (next(net.parameters()).is_cuda) :\n",
    "    print(\"The model is on CUDA\")\n",
    "else :\n",
    "    print(\"The model is on CPU\")\n",
    "\n",
    "# Import the optimizers \n",
    "import torch.optim as optim\n",
    "\n",
    "# Declare a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Declare an optimizer\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.0005,weight_decay=1e-7)\n",
    "\n",
    "#No of iterations !\n",
    "iterations = 25\n",
    "\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "    # Reset the loss for the current epoch !\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable & if possible make them cuda tensors\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients for the current epoch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Calculate gradients of whatever variable set to req_gardients = True\n",
    "        loss.backward()\n",
    "\n",
    "        # Take one step of the gradient descent for this epoch ! \n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            lossvsiter.append(running_loss / 2000)\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 79\n",
      "The network predicted correct for 7982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(79.82, 7982)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Accuracy of the network on the 50000 trained images: 98 %\n"
     ]
    }
   ],
   "source": [
    "train_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_crazy_architecture_with_batch_norm.state_dict(),\"./models/model_crazy_architecture_with_batch_norm-100-epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_crazy_architecture_with_batch_norm.load_state_dict(torch.load(\"./models/model_crazy_architecture_with_batch_norm-75-epochs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (conv2): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_bn): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv3): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_bn): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv4): Conv2d(20, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv5): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv6): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv7): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv8): Conv2d(64, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv8_bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv9): Conv2d(72, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv9_bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc1): Linear (1280 -> 120)\n",
       "  (fc1_bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc2): Linear (120 -> 84)\n",
       "  (fc2_bn): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc3): Linear (84 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_crazy_architecture_with_batch_norm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 81\n",
      "The network predicted correct for 8104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(81.04, 8104)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
