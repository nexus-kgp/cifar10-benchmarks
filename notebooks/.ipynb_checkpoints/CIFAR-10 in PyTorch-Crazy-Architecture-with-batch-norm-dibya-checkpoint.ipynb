{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture is::\n",
    "\n",
    "conv1-->relu-->conv2-->relu-->conv3-->relu-->pool-->\n",
    "\n",
    "conv4-->relu-->conv5-->relu-->conv6-->relu-->pool-->\n",
    "\n",
    "conv7-->relu-->conv8-->relu-->conv9-->relu-->pool-->\n",
    "\n",
    "fully_connected_1-->relu-->fully_connected_2-->relu-->fully_connected_3 **[OUTPUT]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader= torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "classes=('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 11 Layers : 8 conv layers and 3 fully connected layers !\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3,padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 12, 3,padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(12)\n",
    "        self.conv3 = nn.Conv2d(12,20, 3,padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(20)\n",
    "        self.conv4 = nn.Conv2d(20,24, 3,padding=1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(24)\n",
    "        self.conv5 = nn.Conv2d(24,32, 3,padding=1)\n",
    "        self.conv5_bn = nn.BatchNorm2d(32)\n",
    "        self.conv6 = nn.Conv2d(32,48, 3,padding=1)\n",
    "        self.conv6_bn = nn.BatchNorm2d(48)\n",
    "        self.conv7 = nn.Conv2d(48,64, 3,padding=1)\n",
    "        self.conv7_bn = nn.BatchNorm2d(64)\n",
    "        self.conv8 = nn.Conv2d(64,72, 3,padding=1)\n",
    "        self.conv8_bn = nn.BatchNorm2d(72)\n",
    "        self.conv9 = nn.Conv2d(72,80, 3,padding=1)\n",
    "        self.conv9_bn = nn.BatchNorm2d(80)\n",
    "        self.fc1 = nn.Linear(80*4*4, 120)\n",
    "        self.fc1_bn = nn.BatchNorm2d(120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc2_bn = nn.BatchNorm2d(84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2_bn((self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.conv3_bn((self.conv3(x)))))\n",
    "        x = F.relu(self.conv4_bn((self.conv4(x))))\n",
    "        x = F.relu(self.conv5_bn((self.conv5(x))))\n",
    "        x = self.pool(F.relu(self.conv6_bn((self.conv6(x)))))\n",
    "        x = F.relu(self.conv7_bn((self.conv7(x))))\n",
    "        x = F.relu(self.conv8_bn((self.conv8(x))))\n",
    "        x = self.pool(F.relu(self.conv9_bn((self.conv9(x)))))\n",
    "\n",
    "        x = x.view(-1, 80*4*4)\n",
    "        x = F.relu(self.fc1_bn(self.fc1(x)))\n",
    "        x = F.relu(self.fc2_bn(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create an instance of the model on CUDA\n",
    "loss_list = []\n",
    "def new_net(lrate,wd):\n",
    "    global loss_list\n",
    "    net = Net().cuda()\n",
    "\n",
    "    # net=Net()\n",
    "\n",
    "    lossvsiter=[]\n",
    "\n",
    "    # To see if the model is on CUDA or not !\n",
    "    if (next(net.parameters()).is_cuda) :\n",
    "        print(\"The model is on CUDA\")\n",
    "    else :\n",
    "        print(\"The model is on CPU\")\n",
    "\n",
    "    # Import the optimizers \n",
    "    import torch.optim as optim\n",
    "\n",
    "    # Declare a loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Declare an optimizer\n",
    "    optimizer = optim.Adam(net.parameters(),lr=lrate,weight_decay=wd)\n",
    "\n",
    "    #No of iterations !\n",
    "    iterations = 25\n",
    "    \n",
    "\n",
    "\n",
    "    for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "        # Reset the loss for the current epoch !\n",
    "        tloss = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable & if possible make them cuda tensors\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "            # zero the parameter gradients for the current epoch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "\n",
    "            # forward\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate gradients of whatever variable set to req_gardients = True\n",
    "            loss.backward()\n",
    "\n",
    "            # Take one step of the gradient descent for this epoch ! \n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            tloss += loss.data[0]\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                lossvsiter.append(running_loss / 2000)\n",
    "                running_loss = 0.0\n",
    "        loss_list.append(tloss)\n",
    "\n",
    "    print('Finished Training')\n",
    "    return lossvsiter,net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learnin_rate_sample=[0.0005,0.0007,0.001,0.0001,]\n",
    "weight_decay_smaple=[1e-6,1e-7,1e-5,1e-4,1e-3,1e-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 2.024\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 1.861\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 1.735\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 1.628\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 1.568\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 1.512\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 1.438\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 1.411\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 1.369\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 1.340\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 1.298\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 1.257\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 1.232\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 1.226\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 1.204\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 1.167\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 1.143\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 1.140\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 1.096\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 1.103\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 1.071\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 1.090\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 1.071\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 1.065\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 1.010\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 1.007\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 0.997\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 0.992\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 1.002\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 0.994\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 0.954\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 0.947\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 0.961\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 0.905\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 0.932\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 0.929\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 0.882\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 0.868\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 0.914\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 0.904\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 0.896\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 0.882\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 0.844\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 0.850\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 0.851\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 0.832\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 0.849\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 0.827\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 0.803\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 0.830\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 0.799\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 0.806\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 0.787\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 0.811\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 0.771\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 0.773\n",
      "[Epoch :: 10, Mini Batch ::  6000] loss: 0.777\n",
      "[Epoch :: 10, Mini Batch ::  8000] loss: 0.753\n",
      "[Epoch :: 10, Mini Batch :: 10000] loss: 0.795\n",
      "[Epoch :: 10, Mini Batch :: 12000] loss: 0.780\n"
     ]
    }
   ],
   "source": [
    "lossvsiter_crazy_architecture_with_batch_norm,model_crazy_architecture_with_batch_norm=new_net(0.0005,1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8aafd66748>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl0VeW9//H3NzMhIQlJgJCBMWpB\n5oioVNF6Eb1WbK9V21qpE7e1emvbe9va/n61t+29vw7W3tpWWqpWbVVK1VauVRGrVlFBwjypBBAS\nwpCQQAhTSPL9/XE29AiBhEwnyfm81jrrnHz3cJ69zlp82Pt59n7M3REREQkXE+kGiIhI16NwEBGR\nEygcRETkBAoHERE5gcJBREROoHAQEZETKBxEROQECgcRETlBs+FgZvlm9qqZrTOztWb25aD+EzN7\n18xWmdmfzSw9bJu7zazEzN4zs8vC6tOCWomZfTOsPsTMFgf1P5pZQnsfqIiItJw1d4e0meUAOe6+\nzMxSgaXA1UAe8Iq715vZjwDc/RtmNgJ4EpgIDAReBs4Idvc+8E9AGbAE+LS7rzOzucAz7j7HzH4N\nrHT3WadqV1ZWlg8ePLhVBy0iEq2WLl1a6e7Zza0X19wK7r4d2B583mdm64Fcd38pbLVFwDXB5+nA\nHHc/DGw2sxJCQQFQ4u6bAMxsDjA92N8lwGeCdR4FvgucMhwGDx5McXFxc80XEZEwZralJeudVp+D\nmQ0GxgGLj1t0M/BC8DkXKA1bVhbUTlbPBPa4e/1x9aa+f6aZFZtZcUVFxek0XURETkOLw8HMUoCn\ngbvcvSas/m2gHni8/Zv3Ye4+292L3L0oO7vZsyIREWmlZi8rAZhZPKFgeNzdnwmrfx64EviY/6Pz\nYhuQH7Z5XlDjJPXdQLqZxQVnD+Hri4hIBLRktJIBDwHr3f2+sPo04OvAVe5+IGyTecD1ZpZoZkOA\nQuAdQh3QhcHIpATgemBeECqv8o8+ixnAs20/NBERaa2WnDlcAHwOWG1mK4Lat4D7gURgQSg/WOTu\nX3D3tcHoo3WELjd9yd0bAMzsDmA+EAs87O5rg/19A5hjZj8AlhMKIxERiZBmh7J2VUVFRa7RSiIi\np8fMlrp7UXPr6Q5pERE5QdSFw+/f/oD/XVke6WaIiHRpURcOc4vLePKdrZFuhohIlxZ14TAmP41V\nZXtpaOyefS0iIp0h6sJhbH4GtYfr2VhRG+mmiIh0WVEYDqGHx64o3RPhloiIdF1RFw5Ds3qTmhSn\ncBAROYWoC4eYGGNMXjortiocREROJurCAUKXlt7buY+DdQ2RboqISJcUleEwJj+dhkZnTfneSDdF\nRKRLispwONYprUtLIiJNispwyE5NJDe9FyvKFA4iIk2JynCA0NmDzhxERJoW1eGwbc9BKvYdjnRT\nRES6nOgNh4JQv8NK3e8gInKCqA2HswemERtjuhlORKQJURsOvRJiObN/qsJBRKQJLZlDOt/MXjWz\ndWa21sy+HNT7mtkCM9sQvGcEdTOz+82sxMxWmdn4sH3NCNbfYGYzwuoTzGx1sM39wbzVHW5sQTor\ny/bQqCe0ioh8SEvOHOqBr7n7CGAS8CUzGwF8E/ibuxcCfwv+BrgcKAxeM4FZEAoT4B7gXGAicM/R\nQAnWuS1su2ltP7Tmjc1LZ9+hejZV7u+MrxMR6TaaDQd33+7uy4LP+4D1QC4wHXg0WO1R4Org83Tg\nMQ9ZBKSbWQ5wGbDA3avcvRpYAEwLlvVx90UemtD6sbB9daijndK6tCQi8mGn1edgZoOBccBioL+7\nbw8W7QD6B59zgdKwzcqC2qnqZU3Um/r+mWZWbGbFFRUVp9P0Jg3LTiElMU4jlkREjtPicDCzFOBp\n4C53rwlfFvyPv8Mv3Lv7bHcvcvei7OzsNu8vNsYYlZumMwcRkeO0KBzMLJ5QMDzu7s8E5Z3BJSGC\n911BfRuQH7Z5XlA7VT2viXqnGFuQzvrtNRw6oie0iogc1ZLRSgY8BKx39/vCFs0Djo44mgE8G1a/\nMRi1NAnYG1x+mg9MNbOMoCN6KjA/WFZjZpOC77oxbF8dbmx+OvWNztrymuZXFhGJEnEtWOcC4HPA\najNbEdS+BfwQmGtmtwBbgGuDZc8DVwAlwAHgJgB3rzKz7wNLgvW+5+5VwefbgUeAXsALwatThE8b\nOmFQRjNri4hEh2bDwd0XAie77+BjTazvwJdOsq+HgYebqBcDZzfXlo7Qv08SOWlJ6ncQEQkTtXdI\nhxubn64RSyIiYRQOhGaG21p1gN21ekKriAgoHIB/9DusKtO0oSIioHAAYFRuGjEGy3VpSUQEUDgA\n0DsxjjP0hFYRkWMUDoGjndKhwVYiItFN4RAYm5/O3oNH+GD3gUg3RUQk4hQOgTHHboarjnBLREQi\nT+EQOKN/KskJsaws1YglERGFQ+DoE1o1YklEROHwIWPz01lfXsPhej2hVUSim8IhzNj8dOoaGlm/\nfV+kmyIiElEKhzDHOqW3qlNaRKKbwiFMTloS/VITdTOciEQ9hUMYMwvdDKdnLIlIlFM4HGdMfjqb\nK/ez50BdpJsiIhIxCofjjAubGU5EJFq1ZA7ph81sl5mtCauNNbNFZrbCzIrNbGJQNzO738xKzGyV\nmY0P22aGmW0IXjPC6hPMbHWwzf3BPNIRMyovDTN0M5yIRLWWnDk8Akw7rvZj4D/dfSzwneBvgMuB\nwuA1E5gFYGZ9gXuAc4GJwD1mdnTC5lnAbWHbHf9dnSo1KZ7h2Sl6jIaIRLVmw8HdXweqji8DfYLP\naUB58Hk68JiHLALSzSwHuAxY4O5V7l4NLACmBcv6uPuiYO7px4Cr23xUbTQ2P50VekKriESx1vY5\n3AX8xMxKgXuBu4N6LlAatl5ZUDtVvayJepPMbGZwGau4oqKilU1v3tiCdKoPHKG06mCHfYeISFfW\n2nD4IvAVd88HvgI81H5NOjl3n+3uRe5elJ2d3WHfMyYv1Cm9XJeWRCRKtTYcZgDPBJ//RKgfAWAb\nkB+2Xl5QO1U9r4l6RJ01IJWk+BiNWBKRqNXacCgHLgo+XwJsCD7PA24MRi1NAva6+3ZgPjDVzDKC\njuipwPxgWY2ZTQpGKd0IPNvag2kvcbExjMpNY6XCQUSiVFxzK5jZk8AUIMvMygiNOroN+LmZxQGH\nCI1MAngeuAIoAQ4ANwG4e5WZfR9YEqz3PXc/2sl9O6ERUb2AF4JXxI3JS+exRVuoq28kIU63g4hI\ndGk2HNz90ydZNKGJdR340kn28zDwcBP1YuDs5trR2cYWpPPgws28u6OG0UEfhIhItNB/iU9ibHCn\ntC4tiUg0UjicRG56L7JSEjQznIhEJYXDSRx9QqtGLIlINFI4nMLY/HQ2Vexn78EjkW6KiEinUjic\nwtGZ4VaV6exBRKKLwuEUjo5SWrFV4SAi0UXhcAppveIZlt2blTpzEJEoo3Boxhg9oVVEopDCoRnj\n8tOprK2jrFpPaBWR6KFwaMa4gtCcRG9trIxwS0REOo/CoRkjcvpwZv9UHnxjM42NurQkItFB4dCM\nmBjji1OGsWFXLS+v3xnp5oiIdAqFQwtcOTqH/L69eOC1jeqYFpGooHBogbjYGGZeOIwVpXt4e9Pu\nSDdHRKTDKRxa6FMT8shKSWTWaxsj3RQRkQ6ncGihpPhYbpk8hDc2VLK6bG+kmyMi0qEUDqfhhkkF\npCbF8cBrJZFuiohIh2o2HMzsYTPbZWZrjqvfaWbvmtlaM/txWP1uMysxs/fM7LKw+rSgVmJm3wyr\nDzGzxUH9j2aW0F4H195Sk+K58bxBvLh2ByW7aiPdHBGRDtOSM4dHgGnhBTO7GJgOjHH3kcC9QX0E\ncD0wMtjmATOLNbNY4FfA5cAI4NPBugA/An7m7sOBauCWth5UR7rpgiEkxMbwm7+r70FEeq5mw8Hd\nXweqjit/Efihux8O1tkV1KcDc9z9sLtvBkqAicGrxN03uXsdMAeYbmYGXAI8FWz/KHB1G4+pQ2Wl\nJHL9Ofn8efk2yvfokRoi0jO1ts/hDOCjweWgv5vZOUE9FygNW68sqJ2sngnscff64+pd2m0XDgXg\nt29sinBLREQ6RmvDIQ7oC0wC/gOYG5wFdCgzm2lmxWZWXFFR0dFfd1J5GclcNXYgc94ppWp/XcTa\nISLSUVobDmXAMx7yDtAIZAHbgPyw9fKC2snqu4F0M4s7rt4kd5/t7kXuXpSdnd3KprePL140jINH\nGnjkzc0RbYeISEdobTj8BbgYwMzOABKASmAecL2ZJZrZEKAQeAdYAhQGI5MSCHVaz/PQsyheBa4J\n9jsDeLa1B9OZCvunMnVEfx556wNqD9c3v4GISDfSkqGsTwJvA2eaWZmZ3QI8DAwNhrfOAWYEZxFr\ngbnAOuBF4Evu3hD0KdwBzAfWA3ODdQG+AXzVzEoI9UE81L6H2HFuv3g4NYfqeWLxlkg3RUSkXVl3\nfZBcUVGRFxcXR7oZfOa3iyjZVcsb37iYxLjYSDdHROSUzGypuxc1t57ukG6j26cMZ9e+wzy99KRd\nJSIi3Y7CoY0uGJ7J6Lw0fvP6RuobGiPdHBGRdqFwaCMz4/Ypw9iy+wDPr9kR6eaIiLQLhUM7mDpi\nAMOyezNLkwGJSA+hcGgHMTHGFy4axvrtNbz2XuRuzhMRaS8Kh3YyfWwuA9OS9DhvEekRFA7tJCEu\nhtsuHMqSD6pZ8sHxzykUEeleFA7t6PpzCujbO4EHXtXZg4h0bwqHdtQrIZabzh/Mq+9VsK68JtLN\nERFpNYVDO7vxvMGkJMYxS5MBiUg3pnBoZ2nJ8dwwaRDPrSrnzZLKSDdHRKRVFA4d4M5LhjMsO4Uv\nz1nOzppDkW6OiMhpUzh0gN6Jccz67Hj2H27gzieW67EaItLtKBw6SGH/VP7fJ0fxzgdV/OSl9yLd\nHBGR06Jw6EBXj8vlM+cW8Ju/b2LBup2Rbo6ISIspHDrYd64cwdm5ffja3BWUVh2IdHNERFpE4dDB\nkuJjeeAzE3Dg9seXcehIQ6SbJCLSrJZME/qwme0KpgQ9ftnXzMzNLCv428zsfjMrMbNVZjY+bN0Z\nZrYheM0Iq08ws9XBNvebmbXXwXUVBZnJ/PRTY1i9bS8/+Ou6SDdHRKRZLTlzeASYdnzRzPKBqcDW\nsPLlQGHwmgnMCtbtC9wDnAtMBO4xs4xgm1nAbWHbnfBdPcHUkQOYeeFQ/rBoK8+u0KxxItK1NRsO\n7v460NST5H4GfB0In8BgOvCYhywC0s0sB7gMWODuVe5eDSwApgXL+rj7Ig9NhPAYcHXbDqnr+o/L\nzuScwRnc/cxqSnbti3RzREROqlV9DmY2Hdjm7iuPW5QLlIb9XRbUTlUva6LeI8XHxvCLT4+nV3ws\nX/zDMg7U1Ue6SSIiTTrtcDCzZOBbwHfavznNfvdMMys2s+KKiu45qc6AtCR+fv04Sipq+faf12jm\nOBHpklpz5jAMGAKsNLMPgDxgmZkNALYB+WHr5gW1U9Xzmqg3yd1nu3uRuxdlZ2e3ouldw+TCLL5y\n6Rn8efk2nnyntPkNREQ62WmHg7uvdvd+7j7Y3QcTuhQ03t13APOAG4NRS5OAve6+HZgPTDWzjKAj\neiowP1hWY2aTglFKNwLPttOxdWl3XDycC8/I5rvz1rJm295IN0dE5ENaMpT1SeBt4EwzKzOzW06x\n+vPAJqAE+C1wO4C7VwHfB5YEr+8FNYJ1Hgy22Qi80LpD6V5iYoz/uW4smSkJfPHxpew9eCTSTRIR\nOca66zXvoqIiLy4ujnQz2mzplmqu+83bXHxWP2Z/bgI98DYPEelCzGypuxc1t57ukI6wCYMyuPuK\nj7Bg3U5NECQiXUZcpBsgcPMFg1m+tZofv/gefZMTuH5iQaSbJCJRTuHQBZgZ9107ltrD9dz959X0\nSohl+tgee7uHiHQDuqzURSTExfDrGyYwcXBfvjp3JS+t3RHpJolIFFM4dCFJ8bE89PlzGJWbxh1P\nLOeNDd3zRj8R6f4UDl1MSmIcj940kWH9UrjtsWLe2dzUY61ERDqWwqELSkuO5/e3TGRgei9ufmQJ\nq8r2RLpJIhJlFA5dVFZKIo/fei7pyfHc+PA7vLujJtJNEpEoonDownLSevHErZNIjIvhhgffYVNF\nbaSbJCJRQuHQxRVkJvP4rZNwd254cDFl1ZqHWkQ6nsKhGxjeL4XHbplI7eF6PvvgYnbVHIp0k0Sk\nh1M4dBMjB6bxyM0Tqdh3mM8+uJiq/XWRbpKI9GAKh25kfEEGD804h61VB/jcQ4v1JFcR6TAKh27m\nvGGZ/PqGCby/cx83P7KE2sOaalRE2p/CoRu6+Kx+3H/9OJZvreaTD7zJ5sr9kW6SiPQwCodu6vJR\nOTx287lU7DvMVb9YyMvrdka6SSLSgygcurHJhVnMu2Myg7KSufWxYu5b8D6Njd1z8iYR6VpaMk3o\nw2a2y8zWhNV+YmbvmtkqM/uzmaWHLbvbzErM7D0zuyysPi2olZjZN8PqQ8xscVD/o5kltOcB9nT5\nfZN56gvn8y/j87j/bxu45dEl7D2gjmoRaZuWnDk8Akw7rrYAONvdRwPvA3cDmNkI4HpgZLDNA2YW\na2axwK+Ay4ERwKeDdQF+BPzM3YcD1cCp5qiWJiTFx3Lvp0bz/avPZmFJJVf9aiHrt+txGyLSes2G\ng7u/DlQdV3vJ3Y8Ok1kE5AWfpwNz3P2wu28GSoCJwavE3Te5ex0wB5huoQmTLwGeCrZ/FLi6jccU\nlcyMz00axJyZkzhY18AnH3iLZ1dsi3SzRKSbao8+h5uBF4LPuUBp2LKyoHayeiawJyxojtallSYM\n6stz/zaZs3P78OU5K/j+c+s40tAY6WaJSDfTpnAws28D9cDj7dOcZr9vppkVm1lxRYUmwjmZfqlJ\nPHHbJD5//mAeWriZGx5cTMW+w5Fuloh0I60OBzP7PHAl8Fl3PzpEZhuQH7ZaXlA7WX03kG5mccfV\nm+Tus929yN2LsrOzW9v0qBAfG8N3rxrJz64bw8qyPXz8FwtZvrU60s0SkW6iVeFgZtOArwNXuXv4\nY0LnAdebWaKZDQEKgXeAJUBhMDIpgVCn9bwgVF4Frgm2nwE827pDkaZ8YlweT3/xfOJijet+s4g/\nLNrCP7JcRKRpLRnK+iTwNnCmmZWZ2S3AL4FUYIGZrTCzXwO4+1pgLrAOeBH4krs3BH0KdwDzgfXA\n3GBdgG8AXzWzEkJ9EA+16xEKIwem8dydk5k0LJP/85c1fOrXb2t2ORE5Jeuu/4ssKiry4uLiSDej\nW2lsdJ5aWsaP579LZW0d10zI4+uXnUm/PkmRbpqIdBIzW+ruRc2tpzuko0hMjHHtOfm8+u9T+NeL\nhjJvRTlT7n2NX71awqEjDZFunoh0IQqHKJSaFM/dl3+EBV+9kMnDs/jJ/Pe49L6/8/zq7eqPEBFA\n4RDVBmX2ZvaNRTxx67mkJMZx++PLuH72ItaW741000QkwhQOwvnDs3juzsn84OqzeX/nPq78xUK+\n+fQq3RshEsUUDgJAXGwMN0waxGv/fjE3nT+Ep5aWcfG9rzH79Y3U1esOa5Foo3CQD0lLjuc7Hx/B\ni3ddyDmDM/jv59/lsv95nYUbKiPdNBHpRAoHadLwfin87qaJ/O6mc2h054aHFnPHE8vYWXMo0k0T\nkU6gcJBTuvjMfsy/60LuurSQl9bt5GM//TsPL9xMvR7mJ9KjKRykWUnxsdx16Rm8dNeFjB+Uwfee\nW8dVv3yTZXpWk0iPpXCQFhuc1ZtHbzqHBz47nqr9dfzLrLe4+5nV7DlQF+mmiUg7UzjIaTEzrhiV\nw8tfu4hbJw9hbnEpl/z078wtLtX81SI9iMJBWiUlMY5v//MInrtzMkOyevP1p1Zx3ey3eXeHpicV\n6QkUDtImH8npw5/+9Tx+fM1oSnbV8s/3L+S/n19PzaEjkW6aiLSBwkHaLCbGuLYon1e+NoVri/KY\n/fomJv/wFe5b8L76I0S6KT2yW9rdmm17+cUrG5i/dicpiXF87rxB3Dp5CJkpiZFumkjUa+kjuxUO\n0mHe3VHDL18p4a+rt5MUF8tnzy1g5oVDNX+ESAQpHKTLKNlVywOvlvDsynJiY4zrz8nnCxcNY2B6\nr0g3TSTqtNtkP2b2sJntMrM1YbW+ZrbAzDYE7xlB3czsfjMrMbNVZjY+bJsZwfobzGxGWH2Cma0O\ntrnfzOz0D1e6suH9UrjvurG88rWL+MTYXJ5YvJWLfvIqdz+zitKqA83vQEQ6XUs6pB8Bph1X+ybw\nN3cvBP4W/A1wOVAYvGYCsyAUJsA9wLnAROCeo4ESrHNb2HbHf5f0EIMye/Oja0bz2n9M4bpz8nl6\n6Tam3PsaX5u7kk0VtZFunoiEaTYc3P11oOq48nTg0eDzo8DVYfXHPGQRkG5mOcBlwAJ3r3L3amAB\nMC1Y1sfdF3no+tZjYfuSHiovI5kfXD2K179+MTeeN4jnVpVzyU//zmcfXMS8leWaslSkC4hr5Xb9\n3X178HkH0D/4nAuUhq1XFtROVS9roi5RYEBaEvd8fCS3TxnOE4u3Mre4lH97cjlpveL5xLhcri3K\nZ8TAPpFupkhUam04HOPubmad0qttZjMJXa6ioKCgM75SOkF2aiJfvrSQOy8ZzlsbdzNnyVaeWLyV\nR976gFG5aVx7Tj5XjRlIWq/4SDdVJGq0Nhx2mlmOu28PLg3tCurbgPyw9fKC2jZgynH114J6XhPr\nN8ndZwOzITRaqZVtly4qJsaYXJjF5MIsqvfX8ZcV2/jjklL+71/W8IPn1nHFqByuLcpn0tC+aNyC\nSMdq7R3S84CjI45mAM+G1W8MRi1NAvYGl5/mA1PNLCPoiJ4KzA+W1ZjZpGCU0o1h+5IoltE7gZsu\nGMILX/4o8+64gGsm5PHyup18+reLmHLva/zq1RJNPCTSgZq9z8HMniT0v/4sYCehUUd/AeYCBcAW\n4Fp3rwr+gf8loRFHB4Cb3L042M/NwLeC3f6Xu/8uqBcRGhHVC3gBuNNbcPOF7nOIPgfrGnh+9Xb+\nWFzKO5uriDGYcmY/ri3K45Kz+pMQp6fBiDRHN8FJj7a5cj9/Ki7l6WVl7Kw5TGbvhFAn9jn5nNE/\nNdLNE+myFA4SFeobGnljQyVzi0t5ef1OjjQ4Y/LTubYoj4+PGUifJHVii4RTOEjU2V17mL+sKGfu\nklLe27mPpPgYLj87h08V5TFpSCYxMerEFlE4SNRyd1Zv28sfl5Qyb0U5+w7XU9A3mU9NyOPqcbnk\n902OdBNFIkbhIEKoE3v+2h3MLS7lrY27ASgalMFVYwdyxagcsvQYcYkyCgeR45RWHWDeynLmrSjn\nvZ37iI0xJg/PYvrYgUwdOYCUxDbfEyrS5SkcRE7h3R01zFtRzrMrytm25yCJcTFcOqI/V40ZyJQz\ns0mMi410E0U6hMJBpAXcnWVbq3l2RTnPrdpO1f46+iTFcfnZOUwfO5Bzh2YSq45s6UEUDiKn6UhD\nI2+WVDJvRTnz1+5gf10D/VITuWJUDh8fk8O4/AyNeJJuT+Eg0gYH6xp45d1dPLtiG6+9X0FdfSMD\n05K4csxArhydw6jcND3fSbolhYNIO9l36AgL1u3kuVXbeWNDBUcanIK+yVw5OocrRw/kIzmpCgrp\nNhQOIh1g74EjzF+7g/9dVc5bG3fT0OgMze7NlaMH8vHRORTq0R3SxSkcRDrY7trDvLBmB8+tKmfx\n5irc4awBqUw7ewAXnpHN6Nw04mL1MEDpWhQOIp1oV80hnl+9nedWbWfp1mrcoU9SHOcPy+KjZ2Rx\nYWG27syWLkHhIBIh1fvreHNjJQs3VPL6+xWU7w3NOzE4M5nJhVl8tDCb84Zl6qGAEhEKB5EuwN3Z\nVLmfN96vYGFJJW9v3M3+ugZiY4xx+enHwmJMni5BSedQOIh0QXX1jSzfWs0bGyp5o6SSVWV7cIfU\npDjOG5rJRwuzuGB4FkOyemsElHQIhYNIN1C9v463Nu5mYUkFb2yopKz6IAC56b2YPDyLCwqzuGBY\nJpl6QKC0k04JBzP7CnAr4MBq4CYgB5gDZAJLgc+5e52ZJQKPAROA3cB17v5BsJ+7gVuABuDf3H1+\nc9+tcJCext3ZWnWANzaE+ive2lhJzaF6AEYO7MPkwiwmD8/inMF9SYrXs5+kdTo8HMwsF1gIjHD3\ng2Y2F3geuAJ4xt3nmNmvgZXuPsvMbgdGu/sXzOx64BPufp2ZjQCeBCYCA4GXgTPcveFU369wkJ6u\noTE0L8XCDaGzimVbqznS4CTExXDO4AwmDOrL+IJ0xuVnkJaszm1pmZaGQ1ufURwH9DKzI0AysB24\nBPhMsPxR4LvALGB68BngKeCXFrqoOh2Y4+6Hgc1mVkIoKN5uY9tEurXYGGNsfjpj89O545JCDtTV\ns3hzVXBWsZtfvrKBxuD/dsOyezO+IINxBRmMH5ROYb9UPTBQ2qTV4eDu28zsXmArcBB4idBlpD3u\nXh+sVgbkBp9zgdJg23oz20vo0lMusChs1+HbiEggOSGOi8/sx8Vn9gOg9nA9q8r2sHzrHpZtqeZv\n7+7iT0vLAEhJjGNMfloQGKGzi4zeCZFsvnQzrQ4HM8sg9L/+IcAe4E/AtHZq18m+cyYwE6CgoKAj\nv0qky0tJDN1kd/6wLCDUZ7Fl9wGWba0OBcbWah54bSMNwenF4MxkRg5MY8TAPowc2IeRA9PITlVH\ntzStLZeVLgU2u3sFgJk9A1wApJtZXHD2kAdsC9bfBuQDZWYWB6QR6pg+Wj8qfJsPcffZwGwI9Tm0\noe0iPY6ZMTirN4OzevPJ8XkAHKirZ1XZXpZtrWZV6V5WbdvDX1dvP7ZNv9TEY0Fx9D2/by8No5U2\nhcNWYJKZJRO6rPQxoBh4FbiG0IilGcCzwfrzgr/fDpa/4u5uZvOAJ8zsPkId0oXAO21ol4gEkhPi\nmDQ0k0lDM4/V9h48wrryGtaW7w3ea3h9Q+WxM4zUpDhG5ISC4vxhmUwuzNLoqCjU1qGs/wlcB9QD\nywkNa80lFAx9g9oN7n7YzJKA3wPjgCrgenffFOzn28DNwX7ucvcXmvtujVYSaT+HjjTw3o59rA1C\nY215De/uqOHQkUZ6J8Qy5axrSAzJAAAJCElEQVR+XDZyABefmU2qHvvRrekmOBFpk7r6RhZt2s2L\na3fw0tqdVNYeJiE2hsmFWUwbOYBLR/Snrzq5ux2Fg4i0m4ZGZ/nWal5cs4MX1+6grPogMQYTh/Rl\n2sgBTB05gIHpvSLdTGkBhYOIdAh3Z215DfPX7mD+2h28v7MWgDH56Uwd0Z+RA/swvF8KA9N6ac7t\nLkjhICKdYmNFbSgo1uxgZdneY/XkhFiGZvdmeHYKw/v94zUoszfxegJtxCgcRKTTVe2vY8POfZRU\n1FKyK/TauKv22JwWAHExxqDM5GNhUdgvlRED+zA0q7ceW94JOuvxGSIix/TtncC5QzM5N2zoLITu\n5t4UFhglu2rZsKuWl9fvOjaENiEuhjP7pzIipw8fyUllxMA0zspJ1aRIEaJwEJEOl5IYx+i8dEbn\npX+oXlffyKbKWtZvr2FdeQ3rt+9jwfqd/LG49Ng6+X17BYHRhxE5fRgxsA+56bpRr6MpHEQkYhLi\nYjhrQB/OGtCHT4wL1dydnTWHQ4ERvNaX1/DSup0cvQqenhzP2QPTODs3jdF5aYzKTSMvQ4HRnhQO\nItKlmBkD0pIYkJbExWf1O1Y/UFfPuzv2Hbu7e1XZXh5auIkjDaHECA+MUUFoKDBaT+EgIt1CckIc\n4wsyGF+Qcax2uD50Z/fqbXtZs+3EwEjrFc+o3DRG5vZhWFYKBZnJDM7sTb/URA2zbYbCQUS6rcS4\n2BP6Mo4PjNXb9vLwws3HAiO0XQwFfZMZlNmbQZnJwas3g/omk5vRS0NtUTiISA/TVGAcaWikfM9B\ntuw+wJaqA2yp3M+WqgNs3X2AhSUVHDrSeGzd2BgjN70Xw/ulcN7QTM4fnslHBvSJujMNhYOI9Hjx\nsTHBWULvE5a5O7v2HQ4Fx+79xwJkbfleXnl3FxAaonvesEwuGBaax7sgM7mzD6HTKRxEJKqZGf37\nJNG/TxITh/T90LLtew/yVslu3iyp5M2Nlfx1VWgujLyMXkwensX5w7M4f1gmWSk9b9Ik3SEtItIC\n7s7Giv2hoCip5O1Nu9l3KDQj8lkDUrlgeBZn9E8hJTGe3omxpCbFkZIYT0pSHCmJoVdXmNdbd0iL\niLQjMzv2yI8Z5w+mvqGRNeU1vFlSyVsbK/n9oi3U1Teech+94mNJSYojNTEu9J4Ux0cG9KFocAbj\nB2XQLzWpk46meTpzEBFpB4eONFC1v47aw/XsO1RP7eF69h+up/ZQPfuC9/11/1hWe+gIVQeOsH57\nzbFQKeibzIRBGcdeZ/RPbfezDZ05iIh0oqT42FbNaXG4voG15TUs/aCapVuqeWNDJX9evg2A1MQ4\nxhakUzSoL0WDMxiTn05KYuf8s93WaULTgQeBswEnNNXne8AfgcHAB8C17l5todsUfw5cARwAPu/u\ny4L9zAD+T7DbH7j7o819t84cRKQncne2Vh1g6ZZqirdUs2xLNe/t3Ic7xBh8JKcPf7jlXDJaOQtf\nZ505/Bx40d2vMbMEIBn4FvA3d/+hmX0T+CbwDeByoDB4nQvMAs41s77APUARoYBZambz3L26jW0T\nEel2zOzYsNtPjs8DYO/BI6wo3cPSD6p4d8c+0pM7/km1rQ4HM0sDLgQ+D+DudUCdmU0HpgSrPQq8\nRigcpgOPeehUZZGZpZtZTrDuAnevCva7AJgGPNnatomI9CRpveK56IxsLjoju9O+sy33iA8BKoDf\nmdlyM3vQzHoD/d19e7DODqB/8DkXKA3bviyonawuIiIR0pZwiAPGA7PcfRywn9AlpGOCs4R2Gw5l\nZjPNrNjMiisqKtprtyIicpy2hEMZUObui4O/nyIUFjuDy0UE77uC5duA/LDt84LayeoncPfZ7l7k\n7kXZ2Z13eiUiEm1aHQ7uvgMoNbMzg9LHgHXAPGBGUJsBPBt8ngfcaCGTgL3B5af5wFQzyzCzDGBq\nUBMRkQhp62ilO4HHg5FKm4CbCAXOXDO7BdgCXBus+zyhYawlhIay3gTg7lVm9n1gSbDe9452TouI\nSGToDmkRkSjS0vscNKOFiIicQOEgIiIn6LaXlcysglCfRmtkAZXt2JzuJJqPHaL7+KP52CG6jz/8\n2Ae5e7PDPbttOLSFmRW35JpbTxTNxw7RffzRfOwQ3cffmmPXZSURETmBwkFERE4QreEwO9INiKBo\nPnaI7uOP5mOH6D7+0z72qOxzEBGRU4vWMwcRETmFqAoHM5tmZu+ZWUkwEVFUMbMPzGy1ma0wsx5/\ne7mZPWxmu8xsTVitr5ktMLMNwXtGJNvYUU5y7N81s23B77/CzK6IZBs7ipnlm9mrZrbOzNaa2ZeD\neo//7U9x7Kf920fNZSUziwXeB/6J0BNllwCfdvd1EW1YJzKzD4Aid4+Ksd5mdiFQS2iSqbOD2o+B\nqrCZCjPc/RuRbGdHOMmxfxeodfd7I9m2jhY8DTrH3ZeZWSqwFLia0MRkPfq3P8WxX8tp/vbRdOYw\nEShx903BrHVzCM1OJz2Uu78OHP8Qx+mEZigkeL+6UxvVSU5y7FHB3bcfnZ/e3fcB6wlNINbjf/tT\nHPtpi6Zw0IxzoYmXXjKzpWY2M9KNiZCTzVQYLe4ws1XBZaced1nleGY2GBgHLCbKfvvjjh1O87eP\npnAQmOzu44HLgS8Flx6iVnvPVNgNzAKGAWOB7cBPI9ucjmVmKcDTwF3uXhO+rKf/9k0c+2n/9tEU\nDi2eca6ncvdtwfsu4M+ELrVFm5PNVNjjuftOd29w90bgt/Tg39/M4gn94/i4uz8TlKPit2/q2Fvz\n20dTOCwBCs1sSDA50fWEZqeLCmbWO+igwsx6E5pxb82pt+qRTjZTYY939B/GwCfoob+/mRnwELDe\n3e8LW9Tjf/uTHXtrfvuoGa0EEAzf+h8gFnjY3f8rwk3qNGY2lNDZAoRmAHyipx+/mT0JTCH0RMqd\nwD3AX4C5QAHBTIU9cebBkxz7FEKXFRz4APjXsGvwPYaZTQbeAFYDjUH5W4Suvffo3/4Ux/5pTvO3\nj6pwEBGRlommy0oiItJCCgcRETmBwkFERE6gcBARkRMoHERE5AQKBxEROYHCQURETqBwEBGRE/x/\nr+6S+nJ16N4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8ab4176be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_net2(lrate,wd,prenet):\n",
    "    global loss_list\n",
    "    net = prenet\n",
    "\n",
    "    # net=Net()\n",
    "\n",
    "    lossvsiter=[]\n",
    "\n",
    "    # To see if the model is on CUDA or not !\n",
    "    if (next(net.parameters()).is_cuda) :\n",
    "        print(\"The model is on CUDA\")\n",
    "    else :\n",
    "        print(\"The model is on CPU\")\n",
    "\n",
    "    # Import the optimizers \n",
    "    import torch.optim as optim\n",
    "\n",
    "    # Declare a loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Declare an optimizer\n",
    "    optimizer = optim.Adam(net.parameters(),lr=lrate,weight_decay=wd)\n",
    "\n",
    "    #No of iterations !\n",
    "    iterations = 25\n",
    "    \n",
    "\n",
    "\n",
    "    for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "        # Reset the loss for the current epoch !\n",
    "        tloss = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable & if possible make them cuda tensors\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "            # zero the parameter gradients for the current epoch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "\n",
    "            # forward\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate gradients of whatever variable set to req_gardients = True\n",
    "            loss.backward()\n",
    "\n",
    "            # Take one step of the gradient descent for this epoch ! \n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            tloss += loss.data[0]\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                lossvsiter.append(running_loss / 2000)\n",
    "                running_loss = 0.0\n",
    "        loss_list.append(tloss)\n",
    "\n",
    "    print('Finished Training')\n",
    "    return lossvsiter,net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4ab0f6acbaa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_percentage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "def test_accuracy(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images=images.cuda()\n",
    "        labels=labels.cuda()\n",
    "        try:\n",
    "            outputs = model(Variable(images))\n",
    "        except RuntimeError as re:\n",
    "            print(outputs.is_cuda)\n",
    "            print(str(re))\n",
    "            sys.exit()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        accuracy_percentage= 100 * correct / total\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d' % (accuracy_percentage))\n",
    "    print(\"The network predicted correct for %s\"%(correct))\n",
    "    return accuracy_percentage,correct\n",
    "\n",
    "test_accuracy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_accuracy(model):\n",
    "    net = model\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        images, labels = data\n",
    "        images=images.cuda()\n",
    "        labels=labels.cuda()\n",
    "        try:\n",
    "            outputs = net(Variable(images))\n",
    "        except RuntimeError as re:\n",
    "            print(outputs.is_cuda)\n",
    "            print(str(re))\n",
    "            sys.exit()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        if (i%1000) == 0:\n",
    "            print(i)\n",
    "\n",
    "    print('Accuracy of the network on the 50000 trained images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 76\n",
      "The network predicted correct for 7606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(76.06, 7606)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Accuracy of the network on the 50000 trained images: 84 %\n"
     ]
    }
   ],
   "source": [
    "train_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It seems the loss will further decrease with more iterations !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
