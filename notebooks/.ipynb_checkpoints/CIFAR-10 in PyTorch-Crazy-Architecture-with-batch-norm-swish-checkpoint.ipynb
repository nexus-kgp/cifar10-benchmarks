{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv x 8 + F.C. x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader= torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "classes=('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 11 Layers : 8 conv layers and 3 fully connected layers !\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3,padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 12, 3,padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(12)\n",
    "        self.conv3 = nn.Conv2d(12,20, 3,padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(20)\n",
    "        self.conv4 = nn.Conv2d(20,24, 3,padding=1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(24)\n",
    "        self.conv5 = nn.Conv2d(24,32, 3,padding=1)\n",
    "        self.conv5_bn = nn.BatchNorm2d(32)\n",
    "        self.conv6 = nn.Conv2d(32,48, 3,padding=1)\n",
    "        self.conv6_bn = nn.BatchNorm2d(48)\n",
    "        self.conv7 = nn.Conv2d(48,64, 3,padding=1)\n",
    "        self.conv7_bn = nn.BatchNorm2d(64)\n",
    "        self.conv8 = nn.Conv2d(64,72, 3,padding=1)\n",
    "        self.conv8_bn = nn.BatchNorm2d(72)\n",
    "        self.conv9 = nn.Conv2d(72,80, 3,padding=1)\n",
    "        self.conv9_bn = nn.BatchNorm2d(80)\n",
    "        self.fc1 = nn.Linear(80*4*4, 120)\n",
    "        self.fc1_bn = nn.BatchNorm2d(120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc2_bn = nn.BatchNorm2d(84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2_bn((self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.conv3_bn((self.conv3(x)))))\n",
    "        x = F.relu(self.conv4_bn((self.conv4(x))))\n",
    "        x = F.relu(self.conv5_bn((self.conv5(x))))\n",
    "        x = self.pool(F.relu(self.conv6_bn((self.conv6(x)))))\n",
    "        x = F.relu(self.conv7_bn((self.conv7(x))))\n",
    "        x = F.relu(self.conv8_bn((self.conv8(x))))\n",
    "        x = self.pool(F.relu(self.conv9_bn((self.conv9(x)))))\n",
    "\n",
    "        x = x.view(-1, 80*4*4)\n",
    "        x = F.relu(self.fc1_bn(self.fc1(x)))\n",
    "        x = F.relu(self.fc2_bn(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create an instance of the model on CUDA\n",
    "def new_net(lrate,wd):\n",
    "\n",
    "    net = Net().cuda()\n",
    "\n",
    "    # net=Net()\n",
    "\n",
    "    lossvsiter=[]\n",
    "\n",
    "    # To see if the model is on CUDA or not !\n",
    "    if (next(net.parameters()).is_cuda) :\n",
    "        print(\"The model is on CUDA\")\n",
    "    else :\n",
    "        print(\"The model is on CPU\")\n",
    "\n",
    "    # Import the optimizers \n",
    "    import torch.optim as optim\n",
    "\n",
    "    # Declare a loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Declare an optimizer\n",
    "    optimizer = optim.Adam(net.parameters(),lr=lrate,weight_decay=wd)\n",
    "\n",
    "    #No of iterations !\n",
    "    iterations = 25\n",
    "\n",
    "\n",
    "    for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "        # Reset the loss for the current epoch !\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable & if possible make them cuda tensors\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "            # zero the parameter gradients for the current epoch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "\n",
    "            # forward\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate gradients of whatever variable set to req_gardients = True\n",
    "            loss.backward()\n",
    "\n",
    "            # Take one step of the gradient descent for this epoch ! \n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                lossvsiter.append(running_loss / 2000)\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "    return lossvsiter,net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learnin_rate_sample=[0.0005,0.0007,0.001,0.0001,]\n",
    "weight_decay_smaple=[1e-6,1e-7,1e-5,1e-4,1e-3,1e-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 2.093\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 1.880\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 1.776\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 1.671\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 1.593\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 1.533\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 1.448\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 1.417\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 1.369\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 1.339\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 1.332\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 1.267\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 1.235\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 1.205\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 1.197\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 1.195\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 1.168\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 1.148\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 1.090\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 1.089\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 1.114\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 1.074\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 1.082\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 1.071\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 1.008\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 1.026\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 1.004\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 1.022\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 1.009\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 0.990\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 0.972\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 0.954\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 0.940\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 0.936\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 0.969\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 0.929\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 0.914\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 0.895\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 0.899\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 0.890\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 0.872\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 0.908\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 0.854\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 0.857\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 0.860\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 0.864\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 0.879\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 0.848\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 0.807\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 0.821\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 0.804\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 0.832\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 0.798\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 0.817\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 0.782\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 0.783\n",
      "[Epoch :: 10, Mini Batch ::  6000] loss: 0.777\n",
      "[Epoch :: 10, Mini Batch ::  8000] loss: 0.806\n",
      "[Epoch :: 10, Mini Batch :: 10000] loss: 0.777\n",
      "[Epoch :: 10, Mini Batch :: 12000] loss: 0.791\n",
      "[Epoch :: 11, Mini Batch ::  2000] loss: 0.747\n",
      "[Epoch :: 11, Mini Batch ::  4000] loss: 0.750\n",
      "[Epoch :: 11, Mini Batch ::  6000] loss: 0.739\n",
      "[Epoch :: 11, Mini Batch ::  8000] loss: 0.763\n",
      "[Epoch :: 11, Mini Batch :: 10000] loss: 0.763\n",
      "[Epoch :: 11, Mini Batch :: 12000] loss: 0.752\n",
      "[Epoch :: 12, Mini Batch ::  2000] loss: 0.721\n",
      "[Epoch :: 12, Mini Batch ::  4000] loss: 0.738\n",
      "[Epoch :: 12, Mini Batch ::  6000] loss: 0.740\n",
      "[Epoch :: 12, Mini Batch ::  8000] loss: 0.743\n",
      "[Epoch :: 12, Mini Batch :: 10000] loss: 0.718\n",
      "[Epoch :: 12, Mini Batch :: 12000] loss: 0.712\n",
      "[Epoch :: 13, Mini Batch ::  2000] loss: 0.690\n",
      "[Epoch :: 13, Mini Batch ::  4000] loss: 0.708\n",
      "[Epoch :: 13, Mini Batch ::  6000] loss: 0.689\n",
      "[Epoch :: 13, Mini Batch ::  8000] loss: 0.709\n",
      "[Epoch :: 13, Mini Batch :: 10000] loss: 0.695\n",
      "[Epoch :: 13, Mini Batch :: 12000] loss: 0.715\n",
      "[Epoch :: 14, Mini Batch ::  2000] loss: 0.679\n",
      "[Epoch :: 14, Mini Batch ::  4000] loss: 0.669\n",
      "[Epoch :: 14, Mini Batch ::  6000] loss: 0.667\n",
      "[Epoch :: 14, Mini Batch ::  8000] loss: 0.674\n",
      "[Epoch :: 14, Mini Batch :: 10000] loss: 0.690\n",
      "[Epoch :: 14, Mini Batch :: 12000] loss: 0.696\n",
      "[Epoch :: 15, Mini Batch ::  2000] loss: 0.659\n",
      "[Epoch :: 15, Mini Batch ::  4000] loss: 0.644\n",
      "[Epoch :: 15, Mini Batch ::  6000] loss: 0.645\n",
      "[Epoch :: 15, Mini Batch ::  8000] loss: 0.686\n",
      "[Epoch :: 15, Mini Batch :: 10000] loss: 0.640\n",
      "[Epoch :: 15, Mini Batch :: 12000] loss: 0.680\n",
      "[Epoch :: 16, Mini Batch ::  2000] loss: 0.637\n",
      "[Epoch :: 16, Mini Batch ::  4000] loss: 0.604\n",
      "[Epoch :: 16, Mini Batch ::  6000] loss: 0.649\n",
      "[Epoch :: 16, Mini Batch ::  8000] loss: 0.622\n",
      "[Epoch :: 16, Mini Batch :: 10000] loss: 0.647\n",
      "[Epoch :: 16, Mini Batch :: 12000] loss: 0.672\n",
      "[Epoch :: 17, Mini Batch ::  2000] loss: 0.605\n",
      "[Epoch :: 17, Mini Batch ::  4000] loss: 0.620\n",
      "[Epoch :: 17, Mini Batch ::  6000] loss: 0.629\n",
      "[Epoch :: 17, Mini Batch ::  8000] loss: 0.622\n",
      "[Epoch :: 17, Mini Batch :: 10000] loss: 0.633\n",
      "[Epoch :: 17, Mini Batch :: 12000] loss: 0.611\n",
      "[Epoch :: 18, Mini Batch ::  2000] loss: 0.598\n",
      "[Epoch :: 18, Mini Batch ::  4000] loss: 0.599\n",
      "[Epoch :: 18, Mini Batch ::  6000] loss: 0.624\n",
      "[Epoch :: 18, Mini Batch ::  8000] loss: 0.594\n",
      "[Epoch :: 18, Mini Batch :: 10000] loss: 0.617\n",
      "[Epoch :: 18, Mini Batch :: 12000] loss: 0.607\n",
      "[Epoch :: 19, Mini Batch ::  2000] loss: 0.567\n",
      "[Epoch :: 19, Mini Batch ::  4000] loss: 0.585\n",
      "[Epoch :: 19, Mini Batch ::  6000] loss: 0.606\n",
      "[Epoch :: 19, Mini Batch ::  8000] loss: 0.605\n",
      "[Epoch :: 19, Mini Batch :: 10000] loss: 0.596\n",
      "[Epoch :: 19, Mini Batch :: 12000] loss: 0.599\n",
      "[Epoch :: 20, Mini Batch ::  2000] loss: 0.545\n",
      "[Epoch :: 20, Mini Batch ::  4000] loss: 0.569\n",
      "[Epoch :: 20, Mini Batch ::  6000] loss: 0.579\n",
      "[Epoch :: 20, Mini Batch ::  8000] loss: 0.570\n",
      "[Epoch :: 20, Mini Batch :: 10000] loss: 0.603\n",
      "[Epoch :: 20, Mini Batch :: 12000] loss: 0.588\n",
      "[Epoch :: 21, Mini Batch ::  2000] loss: 0.539\n",
      "[Epoch :: 21, Mini Batch ::  4000] loss: 0.557\n",
      "[Epoch :: 21, Mini Batch ::  6000] loss: 0.540\n",
      "[Epoch :: 21, Mini Batch ::  8000] loss: 0.577\n",
      "[Epoch :: 21, Mini Batch :: 10000] loss: 0.566\n",
      "[Epoch :: 21, Mini Batch :: 12000] loss: 0.563\n",
      "[Epoch :: 22, Mini Batch ::  2000] loss: 0.526\n",
      "[Epoch :: 22, Mini Batch ::  4000] loss: 0.528\n",
      "[Epoch :: 22, Mini Batch ::  6000] loss: 0.558\n",
      "[Epoch :: 22, Mini Batch ::  8000] loss: 0.556\n",
      "[Epoch :: 22, Mini Batch :: 10000] loss: 0.569\n",
      "[Epoch :: 22, Mini Batch :: 12000] loss: 0.562\n",
      "[Epoch :: 23, Mini Batch ::  2000] loss: 0.488\n",
      "[Epoch :: 23, Mini Batch ::  4000] loss: 0.520\n",
      "[Epoch :: 23, Mini Batch ::  6000] loss: 0.541\n",
      "[Epoch :: 23, Mini Batch ::  8000] loss: 0.554\n",
      "[Epoch :: 23, Mini Batch :: 10000] loss: 0.545\n",
      "[Epoch :: 23, Mini Batch :: 12000] loss: 0.558\n",
      "[Epoch :: 24, Mini Batch ::  2000] loss: 0.498\n",
      "[Epoch :: 24, Mini Batch ::  4000] loss: 0.519\n",
      "[Epoch :: 24, Mini Batch ::  6000] loss: 0.525\n",
      "[Epoch :: 24, Mini Batch ::  8000] loss: 0.525\n",
      "[Epoch :: 24, Mini Batch :: 10000] loss: 0.524\n",
      "[Epoch :: 24, Mini Batch :: 12000] loss: 0.531\n",
      "[Epoch :: 25, Mini Batch ::  2000] loss: 0.514\n",
      "[Epoch :: 25, Mini Batch ::  4000] loss: 0.505\n",
      "[Epoch :: 25, Mini Batch ::  6000] loss: 0.505\n",
      "[Epoch :: 25, Mini Batch ::  8000] loss: 0.498\n",
      "[Epoch :: 25, Mini Batch :: 10000] loss: 0.523\n",
      "[Epoch :: 25, Mini Batch :: 12000] loss: 0.532\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "lossvsiter_crazy_architecture_with_batch_norm,model_crazy_architecture_with_batch_norm=new_net(0.0005,1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./results/lossvsiter_crazy_architecture_with_batch_norm.pkl\",\"wb\") as f:\n",
    "    pickle.dump(lossvsiter_crazy_architecture_with_batch_norm,f)\n",
    "    \n",
    "with open(\"./results/model_crazy_architecture_with_batch_norm.pkl\",\"wb\") as f:\n",
    "    pickle.dump(model_crazy_architecture_with_batch_norm,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_accuracy(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images=images.cuda()\n",
    "        labels=labels.cuda()\n",
    "        try:\n",
    "            outputs = model(Variable(images))\n",
    "        except RuntimeError as re:\n",
    "            print(outputs.is_cuda)\n",
    "            print(str(re))\n",
    "            sys.exit()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        accuracy_percentage= 100 * correct / total\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d' % (accuracy_percentage))\n",
    "    print(\"The network predicted correct for %s\"%(correct))\n",
    "    return accuracy_percentage,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_accuracy(model):\n",
    "    net = model\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        images, labels = data\n",
    "        images=images.cuda()\n",
    "        labels=labels.cuda()\n",
    "        try:\n",
    "            outputs = net(Variable(images))\n",
    "        except RuntimeError as re:\n",
    "            print(outputs.is_cuda)\n",
    "            print(str(re))\n",
    "            sys.exit()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        if (i%1000) == 0:\n",
    "            print(i)\n",
    "\n",
    "    print('Accuracy of the network on the 50000 trained images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 75\n",
      "The network predicted correct for 7538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(75.38, 7538)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Accuracy of the network on the 50000 trained images: 84 %\n"
     ]
    }
   ],
   "source": [
    "train_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 79 %\n",
      "Accuracy of   car : 84 %\n",
      "Accuracy of  bird : 59 %\n",
      "Accuracy of   cat : 57 %\n",
      "Accuracy of  deer : 73 %\n",
      "Accuracy of   dog : 68 %\n",
      "Accuracy of  frog : 83 %\n",
      "Accuracy of horse : 79 %\n",
      "Accuracy of  ship : 83 %\n",
      "Accuracy of truck : 86 %\n"
     ]
    }
   ],
   "source": [
    "net=model_crazy_architecture_with_batch_norm\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    images=images.cuda()\n",
    "    labels=labels.cuda()\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGGN8LBSMMcb4WCgYY4zxsVAwxhjj8/9Mgfg/pcsM3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8e749a3898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(lossvsiter_crazy_architecture_with_batch_norm)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('every 2000th mini-batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets save the model !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_crazy_architecture_with_batch_norm.state_dict(),\"./models/model_crazy_architecture_with_batch_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It seems the loss will further decrease with more iterations !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 0.469\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 0.493\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 0.515\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 0.510\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 0.498\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 0.500\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 0.462\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 0.462\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 0.483\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 0.500\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 0.493\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 0.495\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 0.456\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 0.466\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 0.477\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 0.477\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 0.486\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 0.490\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 0.438\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 0.448\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 0.475\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 0.462\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 0.476\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 0.454\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 0.433\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 0.449\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 0.452\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 0.453\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 0.451\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 0.469\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 0.429\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 0.417\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 0.442\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 0.463\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 0.446\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 0.451\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 0.405\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 0.428\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 0.415\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 0.438\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 0.443\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 0.452\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 0.408\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 0.425\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 0.422\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 0.449\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 0.437\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 0.429\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 0.410\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 0.398\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 0.406\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 0.419\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 0.436\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 0.432\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 0.386\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 0.404\n",
      "[Epoch :: 10, Mini Batch ::  6000] loss: 0.418\n",
      "[Epoch :: 10, Mini Batch ::  8000] loss: 0.439\n",
      "[Epoch :: 10, Mini Batch :: 10000] loss: 0.423\n",
      "[Epoch :: 10, Mini Batch :: 12000] loss: 0.409\n",
      "[Epoch :: 11, Mini Batch ::  2000] loss: 0.403\n",
      "[Epoch :: 11, Mini Batch ::  4000] loss: 0.397\n",
      "[Epoch :: 11, Mini Batch ::  6000] loss: 0.380\n",
      "[Epoch :: 11, Mini Batch ::  8000] loss: 0.417\n",
      "[Epoch :: 11, Mini Batch :: 10000] loss: 0.390\n",
      "[Epoch :: 11, Mini Batch :: 12000] loss: 0.415\n",
      "[Epoch :: 12, Mini Batch ::  2000] loss: 0.376\n",
      "[Epoch :: 12, Mini Batch ::  4000] loss: 0.393\n",
      "[Epoch :: 12, Mini Batch ::  6000] loss: 0.394\n",
      "[Epoch :: 12, Mini Batch ::  8000] loss: 0.397\n",
      "[Epoch :: 12, Mini Batch :: 10000] loss: 0.396\n",
      "[Epoch :: 12, Mini Batch :: 12000] loss: 0.414\n",
      "[Epoch :: 13, Mini Batch ::  2000] loss: 0.393\n",
      "[Epoch :: 13, Mini Batch ::  4000] loss: 0.357\n",
      "[Epoch :: 13, Mini Batch ::  6000] loss: 0.403\n",
      "[Epoch :: 13, Mini Batch ::  8000] loss: 0.371\n",
      "[Epoch :: 13, Mini Batch :: 10000] loss: 0.371\n",
      "[Epoch :: 13, Mini Batch :: 12000] loss: 0.413\n",
      "[Epoch :: 14, Mini Batch ::  2000] loss: 0.348\n",
      "[Epoch :: 14, Mini Batch ::  4000] loss: 0.374\n",
      "[Epoch :: 14, Mini Batch ::  6000] loss: 0.366\n",
      "[Epoch :: 14, Mini Batch ::  8000] loss: 0.376\n",
      "[Epoch :: 14, Mini Batch :: 10000] loss: 0.384\n",
      "[Epoch :: 14, Mini Batch :: 12000] loss: 0.399\n",
      "[Epoch :: 15, Mini Batch ::  2000] loss: 0.361\n",
      "[Epoch :: 15, Mini Batch ::  4000] loss: 0.331\n",
      "[Epoch :: 15, Mini Batch ::  6000] loss: 0.365\n",
      "[Epoch :: 15, Mini Batch ::  8000] loss: 0.378\n",
      "[Epoch :: 15, Mini Batch :: 10000] loss: 0.366\n",
      "[Epoch :: 15, Mini Batch :: 12000] loss: 0.370\n",
      "[Epoch :: 16, Mini Batch ::  2000] loss: 0.353\n",
      "[Epoch :: 16, Mini Batch ::  4000] loss: 0.344\n",
      "[Epoch :: 16, Mini Batch ::  6000] loss: 0.361\n",
      "[Epoch :: 16, Mini Batch ::  8000] loss: 0.373\n",
      "[Epoch :: 16, Mini Batch :: 10000] loss: 0.379\n",
      "[Epoch :: 16, Mini Batch :: 12000] loss: 0.362\n",
      "[Epoch :: 17, Mini Batch ::  2000] loss: 0.336\n",
      "[Epoch :: 17, Mini Batch ::  4000] loss: 0.346\n",
      "[Epoch :: 17, Mini Batch ::  6000] loss: 0.364\n",
      "[Epoch :: 17, Mini Batch ::  8000] loss: 0.371\n",
      "[Epoch :: 17, Mini Batch :: 10000] loss: 0.373\n",
      "[Epoch :: 17, Mini Batch :: 12000] loss: 0.369\n",
      "[Epoch :: 18, Mini Batch ::  2000] loss: 0.336\n",
      "[Epoch :: 18, Mini Batch ::  4000] loss: 0.333\n",
      "[Epoch :: 18, Mini Batch ::  6000] loss: 0.357\n",
      "[Epoch :: 18, Mini Batch ::  8000] loss: 0.346\n",
      "[Epoch :: 18, Mini Batch :: 10000] loss: 0.371\n",
      "[Epoch :: 18, Mini Batch :: 12000] loss: 0.359\n",
      "[Epoch :: 19, Mini Batch ::  2000] loss: 0.332\n",
      "[Epoch :: 19, Mini Batch ::  4000] loss: 0.344\n",
      "[Epoch :: 19, Mini Batch ::  6000] loss: 0.347\n",
      "[Epoch :: 19, Mini Batch ::  8000] loss: 0.339\n",
      "[Epoch :: 19, Mini Batch :: 10000] loss: 0.338\n",
      "[Epoch :: 19, Mini Batch :: 12000] loss: 0.345\n",
      "[Epoch :: 20, Mini Batch ::  2000] loss: 0.323\n",
      "[Epoch :: 20, Mini Batch ::  4000] loss: 0.334\n",
      "[Epoch :: 20, Mini Batch ::  6000] loss: 0.330\n",
      "[Epoch :: 20, Mini Batch ::  8000] loss: 0.341\n",
      "[Epoch :: 20, Mini Batch :: 10000] loss: 0.347\n",
      "[Epoch :: 20, Mini Batch :: 12000] loss: 0.364\n",
      "[Epoch :: 21, Mini Batch ::  2000] loss: 0.291\n",
      "[Epoch :: 21, Mini Batch ::  4000] loss: 0.314\n",
      "[Epoch :: 21, Mini Batch ::  6000] loss: 0.315\n",
      "[Epoch :: 21, Mini Batch ::  8000] loss: 0.354\n",
      "[Epoch :: 21, Mini Batch :: 10000] loss: 0.330\n",
      "[Epoch :: 21, Mini Batch :: 12000] loss: 0.329\n",
      "[Epoch :: 22, Mini Batch ::  2000] loss: 0.322\n",
      "[Epoch :: 22, Mini Batch ::  4000] loss: 0.307\n",
      "[Epoch :: 22, Mini Batch ::  6000] loss: 0.336\n",
      "[Epoch :: 22, Mini Batch ::  8000] loss: 0.339\n",
      "[Epoch :: 22, Mini Batch :: 10000] loss: 0.335\n",
      "[Epoch :: 22, Mini Batch :: 12000] loss: 0.358\n",
      "[Epoch :: 23, Mini Batch ::  2000] loss: 0.296\n",
      "[Epoch :: 23, Mini Batch ::  4000] loss: 0.303\n",
      "[Epoch :: 23, Mini Batch ::  6000] loss: 0.313\n",
      "[Epoch :: 23, Mini Batch ::  8000] loss: 0.319\n",
      "[Epoch :: 23, Mini Batch :: 10000] loss: 0.331\n",
      "[Epoch :: 23, Mini Batch :: 12000] loss: 0.310\n",
      "[Epoch :: 24, Mini Batch ::  2000] loss: 0.284\n",
      "[Epoch :: 24, Mini Batch ::  4000] loss: 0.298\n",
      "[Epoch :: 24, Mini Batch ::  6000] loss: 0.316\n",
      "[Epoch :: 24, Mini Batch ::  8000] loss: 0.330\n",
      "[Epoch :: 24, Mini Batch :: 10000] loss: 0.318\n",
      "[Epoch :: 24, Mini Batch :: 12000] loss: 0.301\n",
      "[Epoch :: 25, Mini Batch ::  2000] loss: 0.303\n",
      "[Epoch :: 25, Mini Batch ::  4000] loss: 0.321\n",
      "[Epoch :: 25, Mini Batch ::  6000] loss: 0.315\n",
      "[Epoch :: 25, Mini Batch ::  8000] loss: 0.317\n",
      "[Epoch :: 25, Mini Batch :: 10000] loss: 0.330\n",
      "[Epoch :: 25, Mini Batch :: 12000] loss: 0.306\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net=model_crazy_architecture_with_batch_norm\n",
    "lossvsiter=lossvsiter_crazy_architecture_with_batch_norm\n",
    "# To see if the model is on CUDA or not !\n",
    "if (next(net.parameters()).is_cuda) :\n",
    "    print(\"The model is on CUDA\")\n",
    "else :\n",
    "    print(\"The model is on CPU\")\n",
    "\n",
    "# Import the optimizers \n",
    "import torch.optim as optim\n",
    "\n",
    "# Declare a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Declare an optimizer\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.0005,weight_decay=1e-7)\n",
    "\n",
    "#No of iterations !\n",
    "iterations = 25\n",
    "\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "    # Reset the loss for the current epoch !\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable & if possible make them cuda tensors\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients for the current epoch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Calculate gradients of whatever variable set to req_gardients = True\n",
    "        loss.backward()\n",
    "\n",
    "        # Take one step of the gradient descent for this epoch ! \n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            lossvsiter.append(running_loss / 2000)\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets see the result ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 77\n",
      "The network predicted correct for 7703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77.03, 7703)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Accuracy of the network on the 50000 trained images: 91 %\n"
     ]
    }
   ],
   "source": [
    "train_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_crazy_architecture_with_batch_norm.state_dict(),\"./models/model_crazy_architecture_with_batch_norm-50-epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets run it again for 25 more ... yay !!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_crazy_architecture_with_batch_norm.load_state_dict(torch.load(\"./models/model_crazy_architecture_with_batch_norm-50-epochs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (conv2): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_bn): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv3): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_bn): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv4): Conv2d(20, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv5): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv6): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv7): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv8): Conv2d(64, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv8_bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv9): Conv2d(72, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv9_bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc1): Linear (1280 -> 120)\n",
       "  (fc1_bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc2): Linear (120 -> 84)\n",
       "  (fc2_bn): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc3): Linear (84 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_crazy_architecture_with_batch_norm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 0.150\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 0.150\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 0.140\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 0.138\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 0.139\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 0.149\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 0.098\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 0.099\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 0.109\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 0.111\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 0.109\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 0.117\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 0.084\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 0.097\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 0.092\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 0.093\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 0.102\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 0.107\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 0.070\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 0.081\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 0.086\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 0.091\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 0.102\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 0.086\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 0.068\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 0.075\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 0.081\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 0.079\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 0.090\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 0.088\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 0.064\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 0.081\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 0.076\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 0.075\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 0.079\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 0.080\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 0.061\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 0.063\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 0.071\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 0.066\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 0.079\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 0.067\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 0.062\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 0.065\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 0.061\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 0.073\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 0.067\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 0.081\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 0.051\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 0.060\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 0.072\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 0.064\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 0.067\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 0.079\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 0.049\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 0.058\n",
      "[Epoch :: 10, Mini Batch ::  6000] loss: 0.071\n",
      "[Epoch :: 10, Mini Batch ::  8000] loss: 0.074\n",
      "[Epoch :: 10, Mini Batch :: 10000] loss: 0.064\n",
      "[Epoch :: 10, Mini Batch :: 12000] loss: 0.062\n",
      "[Epoch :: 11, Mini Batch ::  2000] loss: 0.051\n",
      "[Epoch :: 11, Mini Batch ::  4000] loss: 0.061\n",
      "[Epoch :: 11, Mini Batch ::  6000] loss: 0.061\n",
      "[Epoch :: 11, Mini Batch ::  8000] loss: 0.057\n",
      "[Epoch :: 11, Mini Batch :: 10000] loss: 0.063\n",
      "[Epoch :: 11, Mini Batch :: 12000] loss: 0.064\n",
      "[Epoch :: 12, Mini Batch ::  2000] loss: 0.055\n",
      "[Epoch :: 12, Mini Batch ::  4000] loss: 0.049\n",
      "[Epoch :: 12, Mini Batch ::  6000] loss: 0.061\n",
      "[Epoch :: 12, Mini Batch ::  8000] loss: 0.054\n",
      "[Epoch :: 12, Mini Batch :: 10000] loss: 0.067\n",
      "[Epoch :: 12, Mini Batch :: 12000] loss: 0.066\n",
      "[Epoch :: 13, Mini Batch ::  2000] loss: 0.045\n",
      "[Epoch :: 13, Mini Batch ::  4000] loss: 0.057\n",
      "[Epoch :: 13, Mini Batch ::  6000] loss: 0.054\n",
      "[Epoch :: 13, Mini Batch ::  8000] loss: 0.058\n",
      "[Epoch :: 13, Mini Batch :: 10000] loss: 0.064\n",
      "[Epoch :: 13, Mini Batch :: 12000] loss: 0.060\n",
      "[Epoch :: 14, Mini Batch ::  2000] loss: 0.053\n",
      "[Epoch :: 14, Mini Batch ::  4000] loss: 0.054\n",
      "[Epoch :: 14, Mini Batch ::  6000] loss: 0.049\n",
      "[Epoch :: 14, Mini Batch ::  8000] loss: 0.058\n",
      "[Epoch :: 14, Mini Batch :: 10000] loss: 0.058\n",
      "[Epoch :: 14, Mini Batch :: 12000] loss: 0.059\n",
      "[Epoch :: 15, Mini Batch ::  2000] loss: 0.043\n",
      "[Epoch :: 15, Mini Batch ::  4000] loss: 0.055\n",
      "[Epoch :: 15, Mini Batch ::  6000] loss: 0.052\n",
      "[Epoch :: 15, Mini Batch ::  8000] loss: 0.060\n",
      "[Epoch :: 15, Mini Batch :: 10000] loss: 0.063\n",
      "[Epoch :: 15, Mini Batch :: 12000] loss: 0.046\n",
      "[Epoch :: 16, Mini Batch ::  2000] loss: 0.054\n",
      "[Epoch :: 16, Mini Batch ::  4000] loss: 0.042\n",
      "[Epoch :: 16, Mini Batch ::  6000] loss: 0.049\n",
      "[Epoch :: 16, Mini Batch ::  8000] loss: 0.048\n",
      "[Epoch :: 16, Mini Batch :: 10000] loss: 0.059\n",
      "[Epoch :: 16, Mini Batch :: 12000] loss: 0.051\n",
      "[Epoch :: 17, Mini Batch ::  2000] loss: 0.047\n",
      "[Epoch :: 17, Mini Batch ::  4000] loss: 0.056\n",
      "[Epoch :: 17, Mini Batch ::  6000] loss: 0.048\n",
      "[Epoch :: 17, Mini Batch ::  8000] loss: 0.051\n",
      "[Epoch :: 17, Mini Batch :: 10000] loss: 0.054\n",
      "[Epoch :: 17, Mini Batch :: 12000] loss: 0.056\n",
      "[Epoch :: 18, Mini Batch ::  2000] loss: 0.034\n",
      "[Epoch :: 18, Mini Batch ::  4000] loss: 0.056\n",
      "[Epoch :: 18, Mini Batch ::  6000] loss: 0.041\n",
      "[Epoch :: 18, Mini Batch ::  8000] loss: 0.057\n",
      "[Epoch :: 18, Mini Batch :: 10000] loss: 0.060\n",
      "[Epoch :: 18, Mini Batch :: 12000] loss: 0.049\n",
      "[Epoch :: 19, Mini Batch ::  2000] loss: 0.029\n",
      "[Epoch :: 19, Mini Batch ::  4000] loss: 0.047\n",
      "[Epoch :: 19, Mini Batch ::  6000] loss: 0.046\n",
      "[Epoch :: 19, Mini Batch ::  8000] loss: 0.043\n",
      "[Epoch :: 19, Mini Batch :: 10000] loss: 0.055\n",
      "[Epoch :: 19, Mini Batch :: 12000] loss: 0.057\n",
      "[Epoch :: 20, Mini Batch ::  2000] loss: 0.040\n",
      "[Epoch :: 20, Mini Batch ::  4000] loss: 0.042\n",
      "[Epoch :: 20, Mini Batch ::  6000] loss: 0.050\n",
      "[Epoch :: 20, Mini Batch ::  8000] loss: 0.044\n",
      "[Epoch :: 20, Mini Batch :: 10000] loss: 0.054\n",
      "[Epoch :: 20, Mini Batch :: 12000] loss: 0.049\n",
      "[Epoch :: 21, Mini Batch ::  2000] loss: 0.038\n",
      "[Epoch :: 21, Mini Batch ::  4000] loss: 0.043\n",
      "[Epoch :: 21, Mini Batch ::  6000] loss: 0.052\n",
      "[Epoch :: 21, Mini Batch ::  8000] loss: 0.042\n",
      "[Epoch :: 21, Mini Batch :: 10000] loss: 0.047\n",
      "[Epoch :: 21, Mini Batch :: 12000] loss: 0.053\n",
      "[Epoch :: 22, Mini Batch ::  2000] loss: 0.043\n",
      "[Epoch :: 22, Mini Batch ::  4000] loss: 0.044\n",
      "[Epoch :: 22, Mini Batch ::  6000] loss: 0.045\n",
      "[Epoch :: 22, Mini Batch ::  8000] loss: 0.048\n",
      "[Epoch :: 22, Mini Batch :: 10000] loss: 0.042\n",
      "[Epoch :: 22, Mini Batch :: 12000] loss: 0.040\n",
      "[Epoch :: 23, Mini Batch ::  2000] loss: 0.039\n",
      "[Epoch :: 23, Mini Batch ::  4000] loss: 0.037\n",
      "[Epoch :: 23, Mini Batch ::  6000] loss: 0.048\n",
      "[Epoch :: 23, Mini Batch ::  8000] loss: 0.046\n",
      "[Epoch :: 23, Mini Batch :: 10000] loss: 0.063\n",
      "[Epoch :: 23, Mini Batch :: 12000] loss: 0.038\n",
      "[Epoch :: 24, Mini Batch ::  2000] loss: 0.040\n",
      "[Epoch :: 24, Mini Batch ::  4000] loss: 0.042\n",
      "[Epoch :: 24, Mini Batch ::  6000] loss: 0.038\n",
      "[Epoch :: 24, Mini Batch ::  8000] loss: 0.048\n",
      "[Epoch :: 24, Mini Batch :: 10000] loss: 0.043\n",
      "[Epoch :: 24, Mini Batch :: 12000] loss: 0.046\n",
      "[Epoch :: 25, Mini Batch ::  2000] loss: 0.041\n",
      "[Epoch :: 25, Mini Batch ::  4000] loss: 0.038\n",
      "[Epoch :: 25, Mini Batch ::  6000] loss: 0.041\n",
      "[Epoch :: 25, Mini Batch ::  8000] loss: 0.042\n",
      "[Epoch :: 25, Mini Batch :: 10000] loss: 0.037\n",
      "[Epoch :: 25, Mini Batch :: 12000] loss: 0.041\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net=model_crazy_architecture_with_batch_norm\n",
    "lossvsiter=lossvsiter_crazy_architecture_with_batch_norm\n",
    "# To see if the model is on CUDA or not !\n",
    "if (next(net.parameters()).is_cuda) :\n",
    "    print(\"The model is on CUDA\")\n",
    "else :\n",
    "    print(\"The model is on CPU\")\n",
    "\n",
    "# Import the optimizers \n",
    "import torch.optim as optim\n",
    "\n",
    "# Declare a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Declare an optimizer\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.0005,weight_decay=1e-7)\n",
    "\n",
    "#No of iterations !\n",
    "iterations = 25\n",
    "\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "    # Reset the loss for the current epoch !\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable & if possible make them cuda tensors\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients for the current epoch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Calculate gradients of whatever variable set to req_gardients = True\n",
    "        loss.backward()\n",
    "\n",
    "        # Take one step of the gradient descent for this epoch ! \n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            lossvsiter.append(running_loss / 2000)\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 81\n",
      "The network predicted correct for 8104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(81.04, 8104)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_crazy_architecture_with_batch_norm.state_dict(),\"./models/model_crazy_architecture_with_batch_norm-75-epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Accuracy of the network on the 50000 trained images: 99 %\n"
     ]
    }
   ],
   "source": [
    "train_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets get in a century"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 0.047\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 0.037\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 0.044\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 0.040\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 0.046\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 0.039\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 0.041\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 0.040\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 0.042\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 0.041\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 0.037\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 0.044\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 0.034\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 0.036\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 0.037\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 0.047\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 0.035\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 0.038\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 0.035\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 0.043\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 0.037\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 0.041\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 0.038\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 0.042\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 0.027\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 0.039\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 0.036\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 0.043\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 0.040\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 0.044\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 0.030\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 0.037\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 0.034\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 0.041\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 0.034\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 0.044\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 0.036\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 0.030\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 0.026\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 0.049\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 0.039\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 0.042\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 0.032\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 0.028\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 0.039\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 0.038\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 0.037\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 0.034\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 0.025\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 0.042\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 0.044\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 0.035\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 0.045\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 0.038\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 0.029\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 0.030\n",
      "[Epoch :: 10, Mini Batch ::  6000] loss: 0.040\n",
      "[Epoch :: 10, Mini Batch ::  8000] loss: 0.041\n",
      "[Epoch :: 10, Mini Batch :: 10000] loss: 0.036\n",
      "[Epoch :: 10, Mini Batch :: 12000] loss: 0.038\n",
      "[Epoch :: 11, Mini Batch ::  2000] loss: 0.030\n",
      "[Epoch :: 11, Mini Batch ::  4000] loss: 0.030\n",
      "[Epoch :: 11, Mini Batch ::  6000] loss: 0.033\n",
      "[Epoch :: 11, Mini Batch ::  8000] loss: 0.042\n",
      "[Epoch :: 11, Mini Batch :: 10000] loss: 0.032\n",
      "[Epoch :: 11, Mini Batch :: 12000] loss: 0.040\n",
      "[Epoch :: 12, Mini Batch ::  2000] loss: 0.024\n",
      "[Epoch :: 12, Mini Batch ::  4000] loss: 0.037\n",
      "[Epoch :: 12, Mini Batch ::  6000] loss: 0.033\n",
      "[Epoch :: 12, Mini Batch ::  8000] loss: 0.038\n",
      "[Epoch :: 12, Mini Batch :: 10000] loss: 0.037\n",
      "[Epoch :: 12, Mini Batch :: 12000] loss: 0.037\n",
      "[Epoch :: 13, Mini Batch ::  2000] loss: 0.034\n",
      "[Epoch :: 13, Mini Batch ::  4000] loss: 0.034\n",
      "[Epoch :: 13, Mini Batch ::  6000] loss: 0.029\n",
      "[Epoch :: 13, Mini Batch ::  8000] loss: 0.040\n",
      "[Epoch :: 13, Mini Batch :: 10000] loss: 0.034\n",
      "[Epoch :: 13, Mini Batch :: 12000] loss: 0.037\n",
      "[Epoch :: 14, Mini Batch ::  2000] loss: 0.024\n",
      "[Epoch :: 14, Mini Batch ::  4000] loss: 0.043\n",
      "[Epoch :: 14, Mini Batch ::  6000] loss: 0.035\n",
      "[Epoch :: 14, Mini Batch ::  8000] loss: 0.030\n",
      "[Epoch :: 14, Mini Batch :: 10000] loss: 0.030\n",
      "[Epoch :: 14, Mini Batch :: 12000] loss: 0.031\n",
      "[Epoch :: 15, Mini Batch ::  2000] loss: 0.026\n",
      "[Epoch :: 15, Mini Batch ::  4000] loss: 0.032\n",
      "[Epoch :: 15, Mini Batch ::  6000] loss: 0.035\n",
      "[Epoch :: 15, Mini Batch ::  8000] loss: 0.030\n",
      "[Epoch :: 15, Mini Batch :: 10000] loss: 0.036\n",
      "[Epoch :: 15, Mini Batch :: 12000] loss: 0.027\n",
      "[Epoch :: 16, Mini Batch ::  2000] loss: 0.029\n",
      "[Epoch :: 16, Mini Batch ::  4000] loss: 0.025\n",
      "[Epoch :: 16, Mini Batch ::  6000] loss: 0.034\n",
      "[Epoch :: 16, Mini Batch ::  8000] loss: 0.038\n",
      "[Epoch :: 16, Mini Batch :: 10000] loss: 0.035\n",
      "[Epoch :: 16, Mini Batch :: 12000] loss: 0.037\n",
      "[Epoch :: 17, Mini Batch ::  2000] loss: 0.020\n",
      "[Epoch :: 17, Mini Batch ::  4000] loss: 0.040\n",
      "[Epoch :: 17, Mini Batch ::  6000] loss: 0.030\n",
      "[Epoch :: 17, Mini Batch ::  8000] loss: 0.037\n",
      "[Epoch :: 17, Mini Batch :: 10000] loss: 0.037\n",
      "[Epoch :: 17, Mini Batch :: 12000] loss: 0.038\n",
      "[Epoch :: 18, Mini Batch ::  2000] loss: 0.036\n",
      "[Epoch :: 18, Mini Batch ::  4000] loss: 0.034\n",
      "[Epoch :: 18, Mini Batch ::  6000] loss: 0.032\n",
      "[Epoch :: 18, Mini Batch ::  8000] loss: 0.028\n",
      "[Epoch :: 18, Mini Batch :: 10000] loss: 0.037\n",
      "[Epoch :: 18, Mini Batch :: 12000] loss: 0.030\n",
      "[Epoch :: 19, Mini Batch ::  2000] loss: 0.026\n",
      "[Epoch :: 19, Mini Batch ::  4000] loss: 0.028\n",
      "[Epoch :: 19, Mini Batch ::  6000] loss: 0.037\n",
      "[Epoch :: 19, Mini Batch ::  8000] loss: 0.032\n",
      "[Epoch :: 19, Mini Batch :: 10000] loss: 0.032\n",
      "[Epoch :: 19, Mini Batch :: 12000] loss: 0.032\n",
      "[Epoch :: 20, Mini Batch ::  2000] loss: 0.030\n",
      "[Epoch :: 20, Mini Batch ::  4000] loss: 0.030\n",
      "[Epoch :: 20, Mini Batch ::  6000] loss: 0.029\n",
      "[Epoch :: 20, Mini Batch ::  8000] loss: 0.034\n",
      "[Epoch :: 20, Mini Batch :: 10000] loss: 0.037\n",
      "[Epoch :: 20, Mini Batch :: 12000] loss: 0.030\n",
      "[Epoch :: 21, Mini Batch ::  2000] loss: 0.022\n",
      "[Epoch :: 21, Mini Batch ::  4000] loss: 0.040\n",
      "[Epoch :: 21, Mini Batch ::  6000] loss: 0.028\n",
      "[Epoch :: 21, Mini Batch ::  8000] loss: 0.031\n",
      "[Epoch :: 21, Mini Batch :: 10000] loss: 0.030\n",
      "[Epoch :: 21, Mini Batch :: 12000] loss: 0.027\n",
      "[Epoch :: 22, Mini Batch ::  2000] loss: 0.029\n",
      "[Epoch :: 22, Mini Batch ::  4000] loss: 0.027\n",
      "[Epoch :: 22, Mini Batch ::  6000] loss: 0.039\n",
      "[Epoch :: 22, Mini Batch ::  8000] loss: 0.024\n",
      "[Epoch :: 22, Mini Batch :: 10000] loss: 0.042\n",
      "[Epoch :: 22, Mini Batch :: 12000] loss: 0.025\n",
      "[Epoch :: 23, Mini Batch ::  2000] loss: 0.023\n",
      "[Epoch :: 23, Mini Batch ::  4000] loss: 0.032\n",
      "[Epoch :: 23, Mini Batch ::  6000] loss: 0.030\n",
      "[Epoch :: 23, Mini Batch ::  8000] loss: 0.032\n",
      "[Epoch :: 23, Mini Batch :: 10000] loss: 0.017\n",
      "[Epoch :: 23, Mini Batch :: 12000] loss: 0.041\n",
      "[Epoch :: 24, Mini Batch ::  2000] loss: 0.030\n",
      "[Epoch :: 24, Mini Batch ::  4000] loss: 0.026\n",
      "[Epoch :: 24, Mini Batch ::  6000] loss: 0.028\n",
      "[Epoch :: 24, Mini Batch ::  8000] loss: 0.030\n",
      "[Epoch :: 24, Mini Batch :: 10000] loss: 0.025\n",
      "[Epoch :: 24, Mini Batch :: 12000] loss: 0.038\n",
      "[Epoch :: 25, Mini Batch ::  2000] loss: 0.027\n",
      "[Epoch :: 25, Mini Batch ::  4000] loss: 0.027\n",
      "[Epoch :: 25, Mini Batch ::  6000] loss: 0.032\n",
      "[Epoch :: 25, Mini Batch ::  8000] loss: 0.025\n",
      "[Epoch :: 25, Mini Batch :: 10000] loss: 0.033\n",
      "[Epoch :: 25, Mini Batch :: 12000] loss: 0.023\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net=model_crazy_architecture_with_batch_norm\n",
    "lossvsiter=lossvsiter_crazy_architecture_with_batch_norm\n",
    "# To see if the model is on CUDA or not !\n",
    "if (next(net.parameters()).is_cuda) :\n",
    "    print(\"The model is on CUDA\")\n",
    "else :\n",
    "    print(\"The model is on CPU\")\n",
    "\n",
    "# Import the optimizers \n",
    "import torch.optim as optim\n",
    "\n",
    "# Declare a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Declare an optimizer\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.0005,weight_decay=1e-7)\n",
    "\n",
    "#No of iterations !\n",
    "iterations = 25\n",
    "\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "    # Reset the loss for the current epoch !\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable & if possible make them cuda tensors\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients for the current epoch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Calculate gradients of whatever variable set to req_gardients = True\n",
    "        loss.backward()\n",
    "\n",
    "        # Take one step of the gradient descent for this epoch ! \n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            lossvsiter.append(running_loss / 2000)\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 79\n",
      "The network predicted correct for 7982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(79.82, 7982)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Accuracy of the network on the 50000 trained images: 98 %\n"
     ]
    }
   ],
   "source": [
    "train_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_crazy_architecture_with_batch_norm.state_dict(),\"./models/model_crazy_architecture_with_batch_norm-100-epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_crazy_architecture_with_batch_norm.load_state_dict(torch.load(\"./models/model_crazy_architecture_with_batch_norm-75-epochs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (conv2): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_bn): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv3): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_bn): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv4): Conv2d(20, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv5): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv6): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv7): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv8): Conv2d(64, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv8_bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv9): Conv2d(72, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv9_bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc1): Linear (1280 -> 120)\n",
       "  (fc1_bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc2): Linear (120 -> 84)\n",
       "  (fc2_bn): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc3): Linear (84 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_crazy_architecture_with_batch_norm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 81\n",
      "The network predicted correct for 8104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(81.04, 8104)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
