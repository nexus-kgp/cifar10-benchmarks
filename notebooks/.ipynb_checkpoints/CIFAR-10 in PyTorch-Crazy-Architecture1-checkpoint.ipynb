{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv x 8 + F.C. x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader= torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "classes=('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 11 Layers : 8 conv layers and 3 fully connected layers !\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3,padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 12, 3,padding=1)\n",
    "        self.conv3 = nn.Conv2d(12,20, 3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(20,24, 3,padding=1)\n",
    "        self.conv5 = nn.Conv2d(24,32, 3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(32,48, 3,padding=1)\n",
    "        self.conv7 = nn.Conv2d(48,64, 3,padding=1)\n",
    "        self.conv8 = nn.Conv2d(64,72, 3,padding=1)\n",
    "        self.conv9 = nn.Conv2d(72,80, 3,padding=1)\n",
    "        self.fc1 = nn.Linear(80*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool(F.relu(self.conv6(x)))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = self.pool(F.relu(self.conv9(x)))\n",
    "\n",
    "        x = x.view(-1, 80*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create an instance of the model on CUDA ,\n",
    "# Do not use this function if you are loading a network to further train it !\n",
    "# In that case just load it and call train_existing_net(model_name) !\n",
    "# Infact train_existing net is almost same as new_net function with the net= Net().cuda() commented out !\n",
    "\n",
    "def new_net(lrate,wd):\n",
    "    \n",
    "    net = Net().cuda()\n",
    "\n",
    "    # net=Net()\n",
    "\n",
    "    lossvsiter=[]\n",
    "\n",
    "    # To see if the model is on CUDA or not !\n",
    "    if (next(net.parameters()).is_cuda) :\n",
    "        print(\"The model is on CUDA\")\n",
    "    else :\n",
    "        print(\"The model is on CPU\")\n",
    "\n",
    "    # Import the optimizers \n",
    "    import torch.optim as optim\n",
    "\n",
    "    # Declare a loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Declare an optimizer\n",
    "    optimizer = optim.Adam(net.parameters(),lr=lrate,weight_decay=wd)\n",
    "\n",
    "    #No of iterations !\n",
    "    iterations = 10\n",
    "\n",
    "\n",
    "    for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "        # Reset the loss for the current epoch !\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable & if possible make them cuda tensors\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "            # zero the parameter gradients for the current epoch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "\n",
    "            # forward\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate gradients of whatever variable set to req_gardients = True\n",
    "            loss.backward()\n",
    "\n",
    "            # Take one step of the gradient descent for this epoch ! \n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                lossvsiter.append(running_loss / 2000)\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "    return lossvsiter,net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(lossvsiter_curr_model):\n",
    "    import pickle\n",
    "    with open(\"./results/\"+str(lossvsiter_curr_model)+\".pkl\",\"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_existing_net(net,lossvsiter_curr_model,lrate=0.0005,wd=1e-6,iterations=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train an existing model, continuing its progression\n",
    "\n",
    "    Arguments :\n",
    "        net : The name of the nn.Module instance being trained !\n",
    "        learning rate (0.0005 by default) \n",
    "        Weight Decay (1e-6) by default\n",
    "        lossvsiter_curr_model : The list containing loss of all minibatches and epoches\n",
    "        iterations : The no of epochs you want to train this model for !\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Just xontinue training from last epoch\n",
    "\n",
    "    lossvsiter=lossvsiter_curr_model\n",
    "    \n",
    "    start_epoch=int(2000/len(lossvsiter))\n",
    "    \n",
    "    print(\"starting from epoch no %d \"%(start_epoch))\n",
    "\n",
    "    # To see if the model is on CUDA or not !\n",
    "    if (next(net.parameters()).is_cuda) :\n",
    "        print(\"The model is on CUDA\")\n",
    "    else :\n",
    "        print(\"The model is on CPU\")\n",
    "\n",
    "    # Import the optimizers \n",
    "    import torch.optim as optim\n",
    "\n",
    "    # Declare a loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Declare an optimizer\n",
    "    optimizer = optim.Adam(net.parameters(),lr=lrate,weight_decay=wd)\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch,start_epoch+iterations):  # loop over the dataset multiple times\n",
    "\n",
    "        # Reset the loss for the current epoch !\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable & if possible make them cuda tensors\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "            # zero the parameter gradients for the current epoch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "\n",
    "            # forward\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate gradients of whatever variable set to req_gardients = True\n",
    "            loss.backward()\n",
    "\n",
    "            # Take one step of the gradient descent for this epoch ! \n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                lossvsiter.append(running_loss / 2000)\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "    return lossvsiter,net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 2.125\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 1.849\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 1.766\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 1.687\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 1.648\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 1.586\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 1.495\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 1.484\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 1.454\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 1.422\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 1.400\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 1.346\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 1.292\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 1.248\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 1.249\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 1.243\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 1.178\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 1.196\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 1.134\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 1.102\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 1.087\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 1.052\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 1.059\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 1.045\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 0.975\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 0.989\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 0.982\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 0.968\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 0.975\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 0.973\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 0.905\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 0.890\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 0.911\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 0.887\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 0.901\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 0.881\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 0.820\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 0.838\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 0.865\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 0.849\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 0.831\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 0.840\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 0.757\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 0.773\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 0.793\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 0.806\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 0.808\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 0.801\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 0.714\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 0.742\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 0.745\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 0.759\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 0.791\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 0.758\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 0.700\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 0.702\n",
      "[Epoch :: 10, Mini Batch ::  6000] loss: 0.707\n",
      "[Epoch :: 10, Mini Batch ::  8000] loss: 0.719\n",
      "[Epoch :: 10, Mini Batch :: 10000] loss: 0.718\n",
      "[Epoch :: 10, Mini Batch :: 12000] loss: 0.751\n",
      "[Epoch :: 11, Mini Batch ::  2000] loss: 0.664\n",
      "[Epoch :: 11, Mini Batch ::  4000] loss: 0.659\n",
      "[Epoch :: 11, Mini Batch ::  6000] loss: 0.670\n",
      "[Epoch :: 11, Mini Batch ::  8000] loss: 0.715\n",
      "[Epoch :: 11, Mini Batch :: 10000] loss: 0.700\n",
      "[Epoch :: 11, Mini Batch :: 12000] loss: 0.725\n",
      "[Epoch :: 12, Mini Batch ::  2000] loss: 0.649\n",
      "[Epoch :: 12, Mini Batch ::  4000] loss: 0.636\n",
      "[Epoch :: 12, Mini Batch ::  6000] loss: 0.647\n",
      "[Epoch :: 12, Mini Batch ::  8000] loss: 0.665\n",
      "[Epoch :: 12, Mini Batch :: 10000] loss: 0.723\n",
      "[Epoch :: 12, Mini Batch :: 12000] loss: 0.690\n",
      "[Epoch :: 13, Mini Batch ::  2000] loss: 0.611\n",
      "[Epoch :: 13, Mini Batch ::  4000] loss: 0.651\n",
      "[Epoch :: 13, Mini Batch ::  6000] loss: 0.652\n",
      "[Epoch :: 13, Mini Batch ::  8000] loss: 0.650\n",
      "[Epoch :: 13, Mini Batch :: 10000] loss: 0.662\n",
      "[Epoch :: 13, Mini Batch :: 12000] loss: 0.690\n",
      "[Epoch :: 14, Mini Batch ::  2000] loss: 0.599\n",
      "[Epoch :: 14, Mini Batch ::  4000] loss: 0.614\n",
      "[Epoch :: 14, Mini Batch ::  6000] loss: 0.634\n",
      "[Epoch :: 14, Mini Batch ::  8000] loss: 0.654\n",
      "[Epoch :: 14, Mini Batch :: 10000] loss: 0.665\n",
      "[Epoch :: 14, Mini Batch :: 12000] loss: 0.638\n",
      "[Epoch :: 15, Mini Batch ::  2000] loss: 0.583\n",
      "[Epoch :: 15, Mini Batch ::  4000] loss: 0.601\n",
      "[Epoch :: 15, Mini Batch ::  6000] loss: 0.611\n",
      "[Epoch :: 15, Mini Batch ::  8000] loss: 0.632\n",
      "[Epoch :: 15, Mini Batch :: 10000] loss: 0.640\n",
      "[Epoch :: 15, Mini Batch :: 12000] loss: 0.643\n"
     ]
    }
   ],
   "source": [
    "# Train a new modelfor || Do not run this if you want to continue from the last epoch !\n",
    "\n",
    "\n",
    "loss_distribution,model=new_net(0.0005,1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (conv2): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(20, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv8): Conv2d(64, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv9): Conv2d(72, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear (1280 -> 120)\n",
       "  (fc2): Linear (120 -> 84)\n",
       "  (fc3): Linear (84 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To visualize the model !\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions to evaluate the performance of our network on test images !\n",
    "\"\"\"\n",
    "Arguments::\n",
    "    network :: the custom nn.Module\n",
    "\n",
    "Returns::\n",
    "    accuracy_percentage\n",
    "    No of correctly classified image\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def test_accuracy(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images=images.cuda()\n",
    "        labels=labels.cuda()\n",
    "        try:\n",
    "            outputs = model(Variable(images))\n",
    "        except RuntimeError as re:\n",
    "            print(outputs.is_cuda)\n",
    "            print(str(re))\n",
    "            sys.exit()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        accuracy_percentage= 100 * correct / total\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d  percent' % (accuracy_percentage))\n",
    "    print(\"The network correctly predicted %d images\"%(correct))\n",
    "    return accuracy_percentage,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions to evaluate the performance of our network on test images !\n",
    "\"\"\"\n",
    "Arguments::\n",
    "    network :: the custom nn.Module\n",
    "\n",
    "Just prints the training accuracy\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def train_accuracy(net):\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        images, labels = data\n",
    "        images=images.cuda()\n",
    "        labels=labels.cuda()\n",
    "        try:\n",
    "            outputs = net(Variable(images))\n",
    "        except RuntimeError as re:\n",
    "            print(outputs.is_cuda)\n",
    "            print(str(re))\n",
    "            sys.exit()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        if (i%1000) == 0:\n",
    "            print(\"Estimattion %f percent complete \"%(i/125 ))\n",
    "\n",
    "    print('Accuracy of the network on the 50000 trained images: %f percent' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to pickle \"stuff\"\n",
    "\n",
    "def pickleit(stuff,name):\n",
    "    import pickle\n",
    "    with open(\"./results/\"+str(name)+\".pkl\",\"wb\") as f:\n",
    "            pickle.dump(stuff,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 71  percent\n",
      "The network correctly predicted 7128 images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(71.28, 7128)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimattion 0.000000 percent complete \n",
      "Estimattion 8.000000 percent complete \n",
      "Estimattion 16.000000 percent complete \n",
      "Estimattion 24.000000 percent complete \n",
      "Estimattion 32.000000 percent complete \n",
      "Estimattion 40.000000 percent complete \n",
      "Estimattion 48.000000 percent complete \n",
      "Estimattion 56.000000 percent complete \n",
      "Estimattion 64.000000 percent complete \n",
      "Estimattion 72.000000 percent complete \n",
      "Estimattion 80.000000 percent complete \n",
      "Estimattion 88.000000 percent complete \n",
      "Estimattion 96.000000 percent complete \n",
      "Accuracy of the network on the 50000 trained images: 91.194000 percent\n"
     ]
    }
   ],
   "source": [
    "train_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickleit(model,\"crazy-archi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickleit(loss_distribution,\"15_epoch_loss_distribution_for_crazy_archi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 80 %\n",
      "Accuracy of   car : 86 %\n",
      "Accuracy of  bird : 58 %\n",
      "Accuracy of   cat : 52 %\n",
      "Accuracy of  deer : 65 %\n",
      "Accuracy of   dog : 65 %\n",
      "Accuracy of  frog : 73 %\n",
      "Accuracy of horse : 72 %\n",
      "Accuracy of  ship : 79 %\n",
      "Accuracy of truck : 78 %\n"
     ]
    }
   ],
   "source": [
    "net=model\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    images=images.cuda()\n",
    "    labels=labels.cuda()\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w820gc4zAW34gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8633af1c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_distribution)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('every 2000th mini-batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets save the model !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"./models/CrazyAchitecture\" + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Now if you want to take forward the training further , just load the model and call the training function !\n",
    "#### Also load the list of loss from pickled form !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 0.313\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 0.292\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 0.311\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 0.320\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 0.347\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 0.327\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# To see if the model is on CUDA or not !\n",
    "lrate=0.0005\n",
    "wd=1e-7\n",
    "lossvsiter=loss_distribution\n",
    "if(next(net.parameters()).is_cuda) :\n",
    "    print(\"The model is on CUDA\")\n",
    "else :\n",
    "    print(\"The model is on CPU\")\n",
    "\n",
    "# Import the optimizers \n",
    "import torch.optim as optim\n",
    "\n",
    "# Declare a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Declare an optimizer\n",
    "optimizer = optim.Adam(net.parameters(),lr=lrate,weight_decay=wd)\n",
    "\n",
    "#No of iterations !\n",
    "iterations = 1\n",
    "\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "    # Reset the loss for the current epoch !\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable & if possible make them cuda tensors\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients for the current epoch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Calculate gradients of whatever variable set to req_gardients = True\n",
    "        loss.backward()\n",
    "\n",
    "        # Take one step of the gradient descent for this epoch ! \n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            lossvsiter.append(running_loss / 2000)\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 71  percent\n",
      "The network correctly predicted 7118 images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(71.18, 7118)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimattion 0.000000 percent complete \n",
      "Estimattion 8.000000 percent complete \n",
      "Estimattion 16.000000 percent complete \n",
      "Estimattion 24.000000 percent complete \n",
      "Estimattion 32.000000 percent complete \n",
      "Estimattion 40.000000 percent complete \n",
      "Estimattion 48.000000 percent complete \n",
      "Estimattion 56.000000 percent complete \n",
      "Estimattion 64.000000 percent complete \n",
      "Estimattion 72.000000 percent complete \n",
      "Estimattion 80.000000 percent complete \n",
      "Estimattion 88.000000 percent complete \n",
      "Estimattion 96.000000 percent complete \n",
      "Accuracy of the network on the 50000 trained images: 90.288000 percent\n"
     ]
    }
   ],
   "source": [
    "train_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
