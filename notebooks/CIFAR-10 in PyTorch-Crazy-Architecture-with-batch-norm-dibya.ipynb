{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture is::\n",
    "\n",
    "conv1-->relu-->conv2-->relu-->conv3-->relu-->pool-->\n",
    "\n",
    "conv4-->relu-->conv5-->relu-->conv6-->relu-->pool-->\n",
    "\n",
    "conv7-->relu-->conv8-->relu-->conv9-->relu-->pool-->\n",
    "\n",
    "fully_connected_1-->relu-->fully_connected_2-->relu-->fully_connected_3 **[OUTPUT]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader= torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "classes=('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 11 Layers : 8 conv layers and 3 fully connected layers !\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3,padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 12, 3,padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(12)\n",
    "        self.conv3 = nn.Conv2d(12,20, 3,padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(20)\n",
    "        self.conv4 = nn.Conv2d(20,24, 3,padding=1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(24)\n",
    "        self.conv5 = nn.Conv2d(24,32, 3,padding=1)\n",
    "        self.conv5_bn = nn.BatchNorm2d(32)\n",
    "        self.conv6 = nn.Conv2d(32,48, 3,padding=1)\n",
    "        self.conv6_bn = nn.BatchNorm2d(48)\n",
    "        self.conv7 = nn.Conv2d(48,64, 3,padding=1)\n",
    "        self.conv7_bn = nn.BatchNorm2d(64)\n",
    "        self.conv8 = nn.Conv2d(64,72, 3,padding=1)\n",
    "        self.conv8_bn = nn.BatchNorm2d(72)\n",
    "        self.conv9 = nn.Conv2d(72,80, 3,padding=1)\n",
    "        self.conv9_bn = nn.BatchNorm2d(80)\n",
    "        self.fc1 = nn.Linear(80*4*4, 120)\n",
    "        self.fc1_bn = nn.BatchNorm2d(120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc2_bn = nn.BatchNorm2d(84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2_bn((self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.conv3_bn((self.conv3(x)))))\n",
    "        x = F.relu(self.conv4_bn((self.conv4(x))))\n",
    "        x = F.relu(self.conv5_bn((self.conv5(x))))\n",
    "        x = self.pool(F.relu(self.conv6_bn((self.conv6(x)))))\n",
    "        x = F.relu(self.conv7_bn((self.conv7(x))))\n",
    "        x = F.relu(self.conv8_bn((self.conv8(x))))\n",
    "        x = self.pool(F.relu(self.conv9_bn((self.conv9(x)))))\n",
    "\n",
    "        x = x.view(-1, 80*4*4)\n",
    "        x = F.relu(self.fc1_bn(self.fc1(x)))\n",
    "        x = F.relu(self.fc2_bn(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create an instance of the model on CUDA\n",
    "loss_list = []\n",
    "def new_net(lrate,wd):\n",
    "    global loss_list\n",
    "    net = Net().cuda()\n",
    "\n",
    "    # net=Net()\n",
    "\n",
    "    lossvsiter=[]\n",
    "\n",
    "    # To see if the model is on CUDA or not !\n",
    "    if (next(net.parameters()).is_cuda) :\n",
    "        print(\"The model is on CUDA\")\n",
    "    else :\n",
    "        print(\"The model is on CPU\")\n",
    "\n",
    "    # Import the optimizers \n",
    "    import torch.optim as optim\n",
    "\n",
    "    # Declare a loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Declare an optimizer\n",
    "    optimizer = optim.Adam(net.parameters(),lr=lrate,weight_decay=wd)\n",
    "\n",
    "    #No of iterations !\n",
    "    iterations = 25\n",
    "    \n",
    "\n",
    "\n",
    "    for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "        # Reset the loss for the current epoch !\n",
    "        tloss = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable & if possible make them cuda tensors\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "            # zero the parameter gradients for the current epoch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "\n",
    "            # forward\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate gradients of whatever variable set to req_gardients = True\n",
    "            loss.backward()\n",
    "\n",
    "            # Take one step of the gradient descent for this epoch ! \n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            tloss += loss.data[0]\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                lossvsiter.append(running_loss / 2000)\n",
    "                running_loss = 0.0\n",
    "        loss_list.append(tloss)\n",
    "\n",
    "    print('Finished Training')\n",
    "    return lossvsiter,net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learnin_rate_sample=[0.0005,0.0007,0.001,0.0001,]\n",
    "weight_decay_smaple=[1e-6,1e-7,1e-5,1e-4,1e-3,1e-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 2.024\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 1.861\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 1.735\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 1.628\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 1.568\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 1.512\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 1.438\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 1.411\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 1.369\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 1.340\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 1.298\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 1.257\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 1.232\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 1.226\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 1.204\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 1.167\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 1.143\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 1.140\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 1.096\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 1.103\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 1.071\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 1.090\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 1.071\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 1.065\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 1.010\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 1.007\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 0.997\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 0.992\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 1.002\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 0.994\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 0.954\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 0.947\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 0.961\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 0.905\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 0.932\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 0.929\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 0.882\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 0.868\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 0.914\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 0.904\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 0.896\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 0.882\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 0.844\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 0.850\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 0.851\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 0.832\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 0.849\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 0.827\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 0.803\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 0.830\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 0.799\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 0.806\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 0.787\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 0.811\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 0.771\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 0.773\n",
      "[Epoch :: 10, Mini Batch ::  6000] loss: 0.777\n",
      "[Epoch :: 10, Mini Batch ::  8000] loss: 0.753\n",
      "[Epoch :: 10, Mini Batch :: 10000] loss: 0.795\n",
      "[Epoch :: 10, Mini Batch :: 12000] loss: 0.780\n"
     ]
    }
   ],
   "source": [
    "lossvsiter_crazy_architecture_with_batch_norm,model_crazy_architecture_with_batch_norm=new_net(0.0005,1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsxkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8aaca7c080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_list)\n",
    "print(len(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_net2(lrate,wd,prenet):\n",
    "    global loss_list\n",
    "    net = prenet\n",
    "\n",
    "    # net=Net()\n",
    "\n",
    "    lossvsiter=[]\n",
    "\n",
    "    # To see if the model is on CUDA or not !\n",
    "    if (next(net.parameters()).is_cuda) :\n",
    "        print(\"The model is on CUDA\")\n",
    "    else :\n",
    "        print(\"The model is on CPU\")\n",
    "\n",
    "    # Import the optimizers \n",
    "    import torch.optim as optim\n",
    "\n",
    "    # Declare a loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Declare an optimizer\n",
    "    optimizer = optim.Adam(net.parameters(),lr=lrate,weight_decay=wd)\n",
    "\n",
    "    #No of iterations !\n",
    "    iterations = 25\n",
    "    \n",
    "\n",
    "\n",
    "    for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "        # Reset the loss for the current epoch !\n",
    "        tloss = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable & if possible make them cuda tensors\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "            # zero the parameter gradients for the current epoch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "\n",
    "            # forward\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate gradients of whatever variable set to req_gardients = True\n",
    "            loss.backward()\n",
    "\n",
    "            # Take one step of the gradient descent for this epoch ! \n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            tloss += loss.data[0]\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                lossvsiter.append(running_loss / 2000)\n",
    "                running_loss = 0.0\n",
    "        loss_list.append(tloss)\n",
    "\n",
    "    print('Finished Training')\n",
    "    return lossvsiter,net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 0.285\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 0.298\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 0.300\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 0.307\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 0.322\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 0.317\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 0.284\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 0.310\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 0.307\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 0.296\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 0.299\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 0.324\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 0.283\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 0.299\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 0.279\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 0.288\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 0.279\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 0.293\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 0.273\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 0.304\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 0.302\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 0.303\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 0.289\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 0.304\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 0.278\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 0.279\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 0.270\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 0.287\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 0.286\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 0.285\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 0.267\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 0.256\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 0.278\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 0.263\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 0.289\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 0.291\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 0.260\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 0.262\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 0.242\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 0.283\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 0.286\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 0.281\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 0.265\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 0.260\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 0.265\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 0.282\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 0.288\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 0.282\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 0.254\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 0.245\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 0.276\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 0.278\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 0.278\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 0.276\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 0.243\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 0.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-124:\n",
      "Process Process-123:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/siplab/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3854d584fd13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlossvsiter_crazy_architecture_with_batch_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_crazy_architecture_with_batch_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_net2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_crazy_architecture_with_batch_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-d3fd9f73d6c8>\u001b[0m in \u001b[0;36mnew_net2\u001b[0;34m(lrate, wd, prenet)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# Take one step of the gradient descent for this epoch !\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/siplab/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/local/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "lossvsiter_crazy_architecture_with_batch_norm,model_crazy_architecture_with_batch_norm=new_net2(0.0005,1e-7,model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 77\n",
      "The network predicted correct for 7748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77.48, 7748)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_accuracy(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images=images.cuda()\n",
    "        labels=labels.cuda()\n",
    "        try:\n",
    "            outputs = model(Variable(images))\n",
    "        except RuntimeError as re:\n",
    "            print(outputs.is_cuda)\n",
    "            print(str(re))\n",
    "            sys.exit()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        accuracy_percentage= 100 * correct / total\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d' % (accuracy_percentage))\n",
    "    print(\"The network predicted correct for %s\"%(correct))\n",
    "    return accuracy_percentage,correct\n",
    "\n",
    "test_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_accuracy(model):\n",
    "    net = model\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        images, labels = data\n",
    "        images=images.cuda()\n",
    "        labels=labels.cuda()\n",
    "        try:\n",
    "            outputs = net(Variable(images))\n",
    "        except RuntimeError as re:\n",
    "            print(outputs.is_cuda)\n",
    "            print(str(re))\n",
    "            sys.exit()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        if (i%1000) == 0:\n",
    "            print(i)\n",
    "\n",
    "    print('Accuracy of the network on the 50000 trained images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 76\n",
      "The network predicted correct for 7606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(76.06, 7606)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Accuracy of the network on the 50000 trained images: 84 %\n"
     ]
    }
   ],
   "source": [
    "train_accuracy(model_crazy_architecture_with_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It seems the loss will further decrease with more iterations !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
