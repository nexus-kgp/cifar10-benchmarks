{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv x 8 + F.C. x 3 with drop out added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader= torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "classes=('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 11 Layers : 8 conv layers and 3 fully connected layers !\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3,padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 12, 3,padding=1)\n",
    "        self.conv2_drop = nn.Dropout2d(p=0.2)\n",
    "        self.conv3 = nn.Conv2d(12,20, 3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(20,24, 3,padding=1)\n",
    "        self.conv4_drop = nn.Dropout2d(p=0.2)\n",
    "        self.conv5 = nn.Conv2d(24,32, 3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(32,48, 3,padding=1)\n",
    "        self.conv6_drop = nn.Dropout2d(p=0.2)\n",
    "        self.conv7 = nn.Conv2d(48,64, 3,padding=1)\n",
    "        self.conv8 = nn.Conv2d(64,72, 3,padding=1)\n",
    "        self.conv8_drop = nn.Dropout2d(p=0.2)\n",
    "        self.conv9 = nn.Conv2d(72,80, 3,padding=1)\n",
    "        self.fc1 = nn.Linear(80*4*4, 120)\n",
    "        self.fc1_drop = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc2_drop = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2_drop(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.conv2_drop(F.relu(self.conv4(x)))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.conv2_drop(self.pool(F.relu(self.conv6(x))))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = self.conv2_drop(F.relu(self.conv8(x)))\n",
    "        x = self.pool(F.relu(self.conv9(x)))\n",
    "\n",
    "        x = x.view(-1, 80*4*4)\n",
    "        x = self.fc1_drop(F.relu(self.fc1(x)))\n",
    "        x = self.fc2_drop(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create an instance of the model on CUDA\n",
    "def new_net(lrate,wd):\n",
    "\n",
    "    net = Net().cuda()\n",
    "\n",
    "    # net=Net()\n",
    "\n",
    "    lossvsiter=[]\n",
    "\n",
    "    # To see if the model is on CUDA or not !\n",
    "    if (next(net.parameters()).is_cuda) :\n",
    "        print(\"The model is on CUDA\")\n",
    "    else :\n",
    "        print(\"The model is on CPU\")\n",
    "\n",
    "    # Import the optimizers \n",
    "    import torch.optim as optim\n",
    "\n",
    "    # Declare a loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Declare an optimizer\n",
    "    optimizer = optim.Adam(net.parameters(),lr=lrate,weight_decay=wd)\n",
    "\n",
    "    #No of iterations !\n",
    "    iterations = 25\n",
    "\n",
    "\n",
    "    for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "        # Reset the loss for the current epoch !\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable & if possible make them cuda tensors\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "            # zero the parameter gradients for the current epoch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "\n",
    "            # forward\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate gradients of whatever variable set to req_gardients = True\n",
    "            loss.backward()\n",
    "\n",
    "            # Take one step of the gradient descent for this epoch ! \n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                lossvsiter.append(running_loss / 2000)\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "    return lossvsiter,net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learnin_rate_sample=[0.0005,0.0007,0.001,0.0001,]\n",
    "weight_decay_smaple=[1e-6,1e-7,1e-5,1e-4,1e-3,1e-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 2.263\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 2.156\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 2.073\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 2.027\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 1.983\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 1.938\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 1.903\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 1.865\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 1.846\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 1.806\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 1.791\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 1.787\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 1.732\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 1.739\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 1.711\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 1.696\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 1.697\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 1.656\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 1.654\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 1.646\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 1.624\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 1.606\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 1.604\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 1.597\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 1.552\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 1.545\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 1.564\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 1.552\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 1.527\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 1.524\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 1.495\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 1.489\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 1.498\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 1.487\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 1.460\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 1.466\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 1.430\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 1.419\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 1.443\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 1.431\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 1.439\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 1.412\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 1.397\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 1.397\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 1.382\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 1.369\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 1.373\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 1.360\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 1.345\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 1.353\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 1.334\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 1.312\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 1.338\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 1.308\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 1.296\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 1.284\n",
      "[Epoch :: 10, Mini Batch ::  6000] loss: 1.286\n",
      "[Epoch :: 10, Mini Batch ::  8000] loss: 1.292\n",
      "[Epoch :: 10, Mini Batch :: 10000] loss: 1.280\n",
      "[Epoch :: 10, Mini Batch :: 12000] loss: 1.255\n",
      "[Epoch :: 11, Mini Batch ::  2000] loss: 1.256\n",
      "[Epoch :: 11, Mini Batch ::  4000] loss: 1.240\n",
      "[Epoch :: 11, Mini Batch ::  6000] loss: 1.242\n",
      "[Epoch :: 11, Mini Batch ::  8000] loss: 1.249\n",
      "[Epoch :: 11, Mini Batch :: 10000] loss: 1.245\n",
      "[Epoch :: 11, Mini Batch :: 12000] loss: 1.218\n",
      "[Epoch :: 12, Mini Batch ::  2000] loss: 1.197\n",
      "[Epoch :: 12, Mini Batch ::  4000] loss: 1.218\n",
      "[Epoch :: 12, Mini Batch ::  6000] loss: 1.188\n",
      "[Epoch :: 12, Mini Batch ::  8000] loss: 1.185\n",
      "[Epoch :: 12, Mini Batch :: 10000] loss: 1.204\n",
      "[Epoch :: 12, Mini Batch :: 12000] loss: 1.181\n",
      "[Epoch :: 13, Mini Batch ::  2000] loss: 1.169\n",
      "[Epoch :: 13, Mini Batch ::  4000] loss: 1.166\n",
      "[Epoch :: 13, Mini Batch ::  6000] loss: 1.165\n",
      "[Epoch :: 13, Mini Batch ::  8000] loss: 1.147\n",
      "[Epoch :: 13, Mini Batch :: 10000] loss: 1.156\n",
      "[Epoch :: 13, Mini Batch :: 12000] loss: 1.174\n",
      "[Epoch :: 14, Mini Batch ::  2000] loss: 1.122\n",
      "[Epoch :: 14, Mini Batch ::  4000] loss: 1.140\n",
      "[Epoch :: 14, Mini Batch ::  6000] loss: 1.135\n",
      "[Epoch :: 14, Mini Batch ::  8000] loss: 1.119\n",
      "[Epoch :: 14, Mini Batch :: 10000] loss: 1.115\n",
      "[Epoch :: 14, Mini Batch :: 12000] loss: 1.128\n",
      "[Epoch :: 15, Mini Batch ::  2000] loss: 1.093\n",
      "[Epoch :: 15, Mini Batch ::  4000] loss: 1.112\n",
      "[Epoch :: 15, Mini Batch ::  6000] loss: 1.102\n",
      "[Epoch :: 15, Mini Batch ::  8000] loss: 1.089\n",
      "[Epoch :: 15, Mini Batch :: 10000] loss: 1.097\n",
      "[Epoch :: 15, Mini Batch :: 12000] loss: 1.095\n",
      "[Epoch :: 16, Mini Batch ::  2000] loss: 1.068\n",
      "[Epoch :: 16, Mini Batch ::  4000] loss: 1.064\n",
      "[Epoch :: 16, Mini Batch ::  6000] loss: 1.065\n",
      "[Epoch :: 16, Mini Batch ::  8000] loss: 1.085\n",
      "[Epoch :: 16, Mini Batch :: 10000] loss: 1.062\n",
      "[Epoch :: 16, Mini Batch :: 12000] loss: 1.077\n",
      "[Epoch :: 17, Mini Batch ::  2000] loss: 1.036\n",
      "[Epoch :: 17, Mini Batch ::  4000] loss: 1.052\n",
      "[Epoch :: 17, Mini Batch ::  6000] loss: 1.041\n",
      "[Epoch :: 17, Mini Batch ::  8000] loss: 1.047\n",
      "[Epoch :: 17, Mini Batch :: 10000] loss: 1.024\n",
      "[Epoch :: 17, Mini Batch :: 12000] loss: 1.040\n",
      "[Epoch :: 18, Mini Batch ::  2000] loss: 1.021\n",
      "[Epoch :: 18, Mini Batch ::  4000] loss: 1.001\n",
      "[Epoch :: 18, Mini Batch ::  6000] loss: 1.012\n",
      "[Epoch :: 18, Mini Batch ::  8000] loss: 1.016\n",
      "[Epoch :: 18, Mini Batch :: 10000] loss: 1.014\n",
      "[Epoch :: 18, Mini Batch :: 12000] loss: 1.028\n",
      "[Epoch :: 19, Mini Batch ::  2000] loss: 1.001\n",
      "[Epoch :: 19, Mini Batch ::  4000] loss: 0.993\n",
      "[Epoch :: 19, Mini Batch ::  6000] loss: 1.008\n",
      "[Epoch :: 19, Mini Batch ::  8000] loss: 0.988\n",
      "[Epoch :: 19, Mini Batch :: 10000] loss: 0.973\n",
      "[Epoch :: 19, Mini Batch :: 12000] loss: 1.011\n",
      "[Epoch :: 20, Mini Batch ::  2000] loss: 0.975\n",
      "[Epoch :: 20, Mini Batch ::  4000] loss: 0.972\n",
      "[Epoch :: 20, Mini Batch ::  6000] loss: 0.953\n",
      "[Epoch :: 20, Mini Batch ::  8000] loss: 1.003\n",
      "[Epoch :: 20, Mini Batch :: 10000] loss: 0.987\n",
      "[Epoch :: 20, Mini Batch :: 12000] loss: 0.972\n",
      "[Epoch :: 21, Mini Batch ::  2000] loss: 0.962\n",
      "[Epoch :: 21, Mini Batch ::  4000] loss: 0.950\n",
      "[Epoch :: 21, Mini Batch ::  6000] loss: 0.944\n",
      "[Epoch :: 21, Mini Batch ::  8000] loss: 0.935\n",
      "[Epoch :: 21, Mini Batch :: 10000] loss: 0.970\n",
      "[Epoch :: 21, Mini Batch :: 12000] loss: 0.959\n",
      "[Epoch :: 22, Mini Batch ::  2000] loss: 0.954\n",
      "[Epoch :: 22, Mini Batch ::  4000] loss: 0.930\n",
      "[Epoch :: 22, Mini Batch ::  6000] loss: 0.950\n",
      "[Epoch :: 22, Mini Batch ::  8000] loss: 0.924\n",
      "[Epoch :: 22, Mini Batch :: 10000] loss: 0.939\n",
      "[Epoch :: 22, Mini Batch :: 12000] loss: 0.942\n",
      "[Epoch :: 23, Mini Batch ::  2000] loss: 0.910\n",
      "[Epoch :: 23, Mini Batch ::  4000] loss: 0.928\n",
      "[Epoch :: 23, Mini Batch ::  6000] loss: 0.929\n",
      "[Epoch :: 23, Mini Batch ::  8000] loss: 0.905\n",
      "[Epoch :: 23, Mini Batch :: 10000] loss: 0.913\n",
      "[Epoch :: 23, Mini Batch :: 12000] loss: 0.921\n",
      "[Epoch :: 24, Mini Batch ::  2000] loss: 0.892\n",
      "[Epoch :: 24, Mini Batch ::  4000] loss: 0.905\n",
      "[Epoch :: 24, Mini Batch ::  6000] loss: 0.905\n",
      "[Epoch :: 24, Mini Batch ::  8000] loss: 0.902\n",
      "[Epoch :: 24, Mini Batch :: 10000] loss: 0.921\n",
      "[Epoch :: 24, Mini Batch :: 12000] loss: 0.895\n",
      "[Epoch :: 25, Mini Batch ::  2000] loss: 0.868\n",
      "[Epoch :: 25, Mini Batch ::  4000] loss: 0.908\n",
      "[Epoch :: 25, Mini Batch ::  6000] loss: 0.886\n",
      "[Epoch :: 25, Mini Batch ::  8000] loss: 0.905\n",
      "[Epoch :: 25, Mini Batch :: 10000] loss: 0.879\n",
      "[Epoch :: 25, Mini Batch :: 12000] loss: 0.914\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "lossvsiter_crazy_architecture_with_dropout,model_crazy_architecture_with_dropout=new_net(0.0001,1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./results/model_crazy_architecture_with_dropout.pkl\",\"wb\") as f:\n",
    "    pickle.dump(model_crazy_architecture_with_dropout,f)\n",
    "    \n",
    "with open(\"./results/lossvsiter_crazy_architecture_with_dropout.pkl\",\"wb\") as f:\n",
    "    pickle.dump(lossvsiter_crazy_architecture_with_dropout,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_accuracy(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images=images.cuda()\n",
    "        labels=labels.cuda()\n",
    "        try:\n",
    "            outputs = model(Variable(images))\n",
    "        except RuntimeError as re:\n",
    "            print(outputs.is_cuda)\n",
    "            print(str(re))\n",
    "            sys.exit()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        accuracy_percentage= 100 * correct / total\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d' % (accuracy_percentage))\n",
    "    print(\"The network predicted correct for %s\"%(correct))\n",
    "    return accuracy_percentage,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_accuracy(model):\n",
    "    net=model\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        images, labels = data\n",
    "        images=images.cuda()\n",
    "        labels=labels.cuda()\n",
    "        try:\n",
    "            outputs = net(Variable(images))\n",
    "        except RuntimeError as re:\n",
    "            print(outputs.is_cuda)\n",
    "            print(str(re))\n",
    "            sys.exit()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        if (i%1000) == 0:\n",
    "            print(i)\n",
    "\n",
    "    print('Accuracy of the network on the 50000 trained images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 66\n",
      "The network predicted correct for 6607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(66.07, 6607)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model_crazy_architecture_with_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Accuracy of the network on the 50000 trained images: 71 %\n"
     ]
    }
   ],
   "source": [
    "train_accuracy(model_crazy_architecture_with_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxphylhSMMcaUs6RgjDGmnCUFY4wx5f4fGny8uOjZjq0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb294027b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossvsiter_crazy_architecture_with_dropout)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('every 2000th mini-batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets save the model !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_crazy_architecture_with_dropout.state_dict(),\"./models/model_crazy_architecture_with_dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clearly the loss is decreasing so lets train it more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 1.189\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 1.182\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 1.195\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 1.195\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 1.183\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 1.202\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 1.197\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 1.172\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 1.173\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 1.198\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 1.207\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 1.190\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 1.176\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2fd4012a9e95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1999\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# print every 2000 mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-69:\n",
      "Process Process-70:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sangeet/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/sangeet/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "net=model_crazy_architecture_with_dropout\n",
    "lossvsiter=lossvsiter_crazy_architecture_with_dropout\n",
    "# To see if the model is on CUDA or not !\n",
    "if (next(net.parameters()).is_cuda) :\n",
    "    print(\"The model is on CUDA\")\n",
    "else :\n",
    "    print(\"The model is on CPU\")\n",
    "\n",
    "# Import the optimizers \n",
    "import torch.optim as optim\n",
    "\n",
    "# Declare a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Declare an optimizer\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.0005,weight_decay=1e-7)\n",
    "\n",
    "#No of iterations !\n",
    "iterations = 25\n",
    "\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "    # Reset the loss for the current epoch !\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable & if possible make them cuda tensors\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients for the current epoch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Calculate gradients of whatever variable set to req_gardients = True\n",
    "        loss.backward()\n",
    "\n",
    "        # Take one step of the gradient descent for this epoch ! \n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            lossvsiter.append(running_loss / 2000)\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_crazy_architecture_with_dropout.load_state_dict(torch.load(\"./models/model_crazy_architecture_with_dropout\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (conv2): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_drop): Dropout2d (p=0.2)\n",
       "  (conv3): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(20, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_drop): Dropout2d (p=0.2)\n",
       "  (conv5): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_drop): Dropout2d (p=0.2)\n",
       "  (conv7): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv8): Conv2d(64, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv8_drop): Dropout2d (p=0.2)\n",
       "  (conv9): Conv2d(72, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear (1280 -> 120)\n",
       "  (fc1_drop): Dropout (p = 0.5)\n",
       "  (fc2): Linear (120 -> 84)\n",
       "  (fc2_drop): Dropout (p = 0.5)\n",
       "  (fc3): Linear (84 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_crazy_architecture_with_dropout.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stopped the above kenel as it was a very slow movement !\n",
    "# Lets try a different optimzation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 0.626\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 0.604\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 0.613\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 0.580\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 0.590\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 0.598\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 0.589\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 0.592\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 0.596\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 0.575\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 0.570\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 0.570\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 0.580\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 0.570\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 0.576\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 0.589\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 0.575\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 0.560\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 0.569\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 0.577\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 0.554\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 0.562\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 0.584\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 0.561\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 0.578\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 0.551\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 0.562\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 0.569\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 0.569\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 0.561\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 0.566\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 0.551\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 0.558\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 0.557\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 0.576\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 0.563\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 0.554\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 0.558\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 0.556\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 0.549\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 0.573\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 0.562\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 0.561\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 0.553\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 0.544\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 0.563\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 0.547\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 0.563\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 0.541\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 0.564\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 0.541\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 0.549\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 0.571\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 0.550\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 0.549\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 0.569\n",
      "[Epoch :: 10, Mini Batch ::  6000] loss: 0.567\n",
      "[Epoch :: 10, Mini Batch ::  8000] loss: 0.560\n",
      "[Epoch :: 10, Mini Batch :: 10000] loss: 0.521\n",
      "[Epoch :: 10, Mini Batch :: 12000] loss: 0.546\n",
      "[Epoch :: 11, Mini Batch ::  2000] loss: 0.563\n",
      "[Epoch :: 11, Mini Batch ::  4000] loss: 0.552\n",
      "[Epoch :: 11, Mini Batch ::  6000] loss: 0.525\n",
      "[Epoch :: 11, Mini Batch ::  8000] loss: 0.554\n",
      "[Epoch :: 11, Mini Batch :: 10000] loss: 0.542\n",
      "[Epoch :: 11, Mini Batch :: 12000] loss: 0.559\n",
      "[Epoch :: 12, Mini Batch ::  2000] loss: 0.569\n",
      "[Epoch :: 12, Mini Batch ::  4000] loss: 0.548\n",
      "[Epoch :: 12, Mini Batch ::  6000] loss: 0.545\n",
      "[Epoch :: 12, Mini Batch ::  8000] loss: 0.540\n",
      "[Epoch :: 12, Mini Batch :: 10000] loss: 0.547\n",
      "[Epoch :: 12, Mini Batch :: 12000] loss: 0.536\n",
      "[Epoch :: 13, Mini Batch ::  2000] loss: 0.545\n",
      "[Epoch :: 13, Mini Batch ::  4000] loss: 0.556\n",
      "[Epoch :: 13, Mini Batch ::  6000] loss: 0.542\n",
      "[Epoch :: 13, Mini Batch ::  8000] loss: 0.540\n",
      "[Epoch :: 13, Mini Batch :: 10000] loss: 0.542\n",
      "[Epoch :: 13, Mini Batch :: 12000] loss: 0.543\n",
      "[Epoch :: 14, Mini Batch ::  2000] loss: 0.549\n",
      "[Epoch :: 14, Mini Batch ::  4000] loss: 0.529\n",
      "[Epoch :: 14, Mini Batch ::  6000] loss: 0.544\n",
      "[Epoch :: 14, Mini Batch ::  8000] loss: 0.535\n",
      "[Epoch :: 14, Mini Batch :: 10000] loss: 0.543\n",
      "[Epoch :: 14, Mini Batch :: 12000] loss: 0.552\n",
      "[Epoch :: 15, Mini Batch ::  2000] loss: 0.528\n",
      "[Epoch :: 15, Mini Batch ::  4000] loss: 0.542\n",
      "[Epoch :: 15, Mini Batch ::  6000] loss: 0.545\n",
      "[Epoch :: 15, Mini Batch ::  8000] loss: 0.557\n",
      "[Epoch :: 15, Mini Batch :: 10000] loss: 0.533\n",
      "[Epoch :: 15, Mini Batch :: 12000] loss: 0.536\n",
      "[Epoch :: 16, Mini Batch ::  2000] loss: 0.542\n",
      "[Epoch :: 16, Mini Batch ::  4000] loss: 0.544\n",
      "[Epoch :: 16, Mini Batch ::  6000] loss: 0.543\n",
      "[Epoch :: 16, Mini Batch ::  8000] loss: 0.528\n",
      "[Epoch :: 16, Mini Batch :: 10000] loss: 0.549\n",
      "[Epoch :: 16, Mini Batch :: 12000] loss: 0.529\n",
      "[Epoch :: 17, Mini Batch ::  2000] loss: 0.540\n",
      "[Epoch :: 17, Mini Batch ::  4000] loss: 0.527\n",
      "[Epoch :: 17, Mini Batch ::  6000] loss: 0.544\n",
      "[Epoch :: 17, Mini Batch ::  8000] loss: 0.540\n",
      "[Epoch :: 17, Mini Batch :: 10000] loss: 0.539\n",
      "[Epoch :: 17, Mini Batch :: 12000] loss: 0.536\n",
      "[Epoch :: 18, Mini Batch ::  2000] loss: 0.519\n",
      "[Epoch :: 18, Mini Batch ::  4000] loss: 0.550\n",
      "[Epoch :: 18, Mini Batch ::  6000] loss: 0.524\n",
      "[Epoch :: 18, Mini Batch ::  8000] loss: 0.550\n",
      "[Epoch :: 18, Mini Batch :: 10000] loss: 0.538\n",
      "[Epoch :: 18, Mini Batch :: 12000] loss: 0.531\n",
      "[Epoch :: 19, Mini Batch ::  2000] loss: 0.519\n",
      "[Epoch :: 19, Mini Batch ::  4000] loss: 0.541\n",
      "[Epoch :: 19, Mini Batch ::  6000] loss: 0.544\n",
      "[Epoch :: 19, Mini Batch ::  8000] loss: 0.532\n",
      "[Epoch :: 19, Mini Batch :: 10000] loss: 0.523\n",
      "[Epoch :: 19, Mini Batch :: 12000] loss: 0.536\n",
      "[Epoch :: 20, Mini Batch ::  2000] loss: 0.525\n",
      "[Epoch :: 20, Mini Batch ::  4000] loss: 0.534\n",
      "[Epoch :: 20, Mini Batch ::  6000] loss: 0.531\n",
      "[Epoch :: 20, Mini Batch ::  8000] loss: 0.526\n",
      "[Epoch :: 20, Mini Batch :: 10000] loss: 0.556\n",
      "[Epoch :: 20, Mini Batch :: 12000] loss: 0.528\n",
      "[Epoch :: 21, Mini Batch ::  2000] loss: 0.538\n",
      "[Epoch :: 21, Mini Batch ::  4000] loss: 0.528\n",
      "[Epoch :: 21, Mini Batch ::  6000] loss: 0.525\n",
      "[Epoch :: 21, Mini Batch ::  8000] loss: 0.547\n",
      "[Epoch :: 21, Mini Batch :: 10000] loss: 0.530\n",
      "[Epoch :: 21, Mini Batch :: 12000] loss: 0.527\n",
      "[Epoch :: 22, Mini Batch ::  2000] loss: 0.522\n",
      "[Epoch :: 22, Mini Batch ::  4000] loss: 0.533\n",
      "[Epoch :: 22, Mini Batch ::  6000] loss: 0.539\n",
      "[Epoch :: 22, Mini Batch ::  8000] loss: 0.541\n",
      "[Epoch :: 22, Mini Batch :: 10000] loss: 0.532\n",
      "[Epoch :: 22, Mini Batch :: 12000] loss: 0.507\n",
      "[Epoch :: 23, Mini Batch ::  2000] loss: 0.535\n",
      "[Epoch :: 23, Mini Batch ::  4000] loss: 0.531\n",
      "[Epoch :: 23, Mini Batch ::  6000] loss: 0.543\n",
      "[Epoch :: 23, Mini Batch ::  8000] loss: 0.516\n",
      "[Epoch :: 23, Mini Batch :: 10000] loss: 0.525\n",
      "[Epoch :: 23, Mini Batch :: 12000] loss: 0.520\n",
      "[Epoch :: 24, Mini Batch ::  2000] loss: 0.529\n",
      "[Epoch :: 24, Mini Batch ::  4000] loss: 0.532\n",
      "[Epoch :: 24, Mini Batch ::  6000] loss: 0.527\n",
      "[Epoch :: 24, Mini Batch ::  8000] loss: 0.518\n",
      "[Epoch :: 24, Mini Batch :: 10000] loss: 0.520\n",
      "[Epoch :: 24, Mini Batch :: 12000] loss: 0.529\n",
      "[Epoch :: 25, Mini Batch ::  2000] loss: 0.530\n",
      "[Epoch :: 25, Mini Batch ::  4000] loss: 0.519\n",
      "[Epoch :: 25, Mini Batch ::  6000] loss: 0.509\n",
      "[Epoch :: 25, Mini Batch ::  8000] loss: 0.530\n",
      "[Epoch :: 25, Mini Batch :: 10000] loss: 0.537\n",
      "[Epoch :: 25, Mini Batch :: 12000] loss: 0.529\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net=model_crazy_architecture_with_dropout\n",
    "lossvsiter=lossvsiter_crazy_architecture_with_dropout\n",
    "# To see if the model is on CUDA or not !\n",
    "if (next(net.parameters()).is_cuda) :\n",
    "    print(\"The model is on CUDA\")\n",
    "else :\n",
    "    print(\"The model is on CPU\")\n",
    "\n",
    "# Import the optimizers \n",
    "import torch.optim as optim\n",
    "\n",
    "# Declare a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Declare an optimizer\n",
    "#optimizer = optim.Adam(net.parameters(),lr=0.0005,weight_decay=1e-7)\n",
    "\n",
    "optimizer = torch.optim.Adadelta(net.parameters(),lr=0.0005)\n",
    "\n",
    "# Lets use an lr scheduler\n",
    "\n",
    "#scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=3,verbose=True)\n",
    "\n",
    "#No of iterations !\n",
    "iterations = 25\n",
    "\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "    # Reset the loss for the current epoch !\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable & if possible make them cuda tensors\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients for the current epoch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Calculate gradients of whatever variable set to req_gardients = True\n",
    "        loss.backward()\n",
    "\n",
    "        # Take one step of the gradient descent for this epoch ! \n",
    "        optimizer.step()\n",
    "#         scheduler.step(loss)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            lossvsiter.append(running_loss / 2000)\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 74\n",
      "The network predicted correct for 7450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(74.5, 7450)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model_crazy_architecture_with_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_crazy_architecture_with_dropout.state_dict(),\"./models/model_crazy_architecture_with_dropout-75-epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Accuracy of the network on the 50000 trained images: 81 %\n"
     ]
    }
   ],
   "source": [
    "train_accuracy(model_crazy_architecture_with_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on CUDA\n",
      "[Epoch :: 1, Mini Batch ::  2000] loss: 0.805\n",
      "[Epoch :: 1, Mini Batch ::  4000] loss: 0.833\n",
      "[Epoch :: 1, Mini Batch ::  6000] loss: 0.839\n",
      "[Epoch :: 1, Mini Batch ::  8000] loss: 0.820\n",
      "[Epoch :: 1, Mini Batch :: 10000] loss: 0.832\n",
      "[Epoch :: 1, Mini Batch :: 12000] loss: 0.829\n",
      "[Epoch :: 2, Mini Batch ::  2000] loss: 0.739\n",
      "[Epoch :: 2, Mini Batch ::  4000] loss: 0.748\n",
      "[Epoch :: 2, Mini Batch ::  6000] loss: 0.740\n",
      "[Epoch :: 2, Mini Batch ::  8000] loss: 0.760\n",
      "[Epoch :: 2, Mini Batch :: 10000] loss: 0.764\n",
      "[Epoch :: 2, Mini Batch :: 12000] loss: 0.779\n",
      "[Epoch :: 3, Mini Batch ::  2000] loss: 0.665\n",
      "[Epoch :: 3, Mini Batch ::  4000] loss: 0.711\n",
      "[Epoch :: 3, Mini Batch ::  6000] loss: 0.684\n",
      "[Epoch :: 3, Mini Batch ::  8000] loss: 0.690\n",
      "[Epoch :: 3, Mini Batch :: 10000] loss: 0.692\n",
      "[Epoch :: 3, Mini Batch :: 12000] loss: 0.691\n",
      "[Epoch :: 4, Mini Batch ::  2000] loss: 0.612\n",
      "[Epoch :: 4, Mini Batch ::  4000] loss: 0.628\n",
      "[Epoch :: 4, Mini Batch ::  6000] loss: 0.620\n",
      "[Epoch :: 4, Mini Batch ::  8000] loss: 0.654\n",
      "[Epoch :: 4, Mini Batch :: 10000] loss: 0.658\n",
      "[Epoch :: 4, Mini Batch :: 12000] loss: 0.648\n",
      "[Epoch :: 5, Mini Batch ::  2000] loss: 0.570\n",
      "[Epoch :: 5, Mini Batch ::  4000] loss: 0.590\n",
      "[Epoch :: 5, Mini Batch ::  6000] loss: 0.582\n",
      "[Epoch :: 5, Mini Batch ::  8000] loss: 0.608\n",
      "[Epoch :: 5, Mini Batch :: 10000] loss: 0.630\n",
      "[Epoch :: 5, Mini Batch :: 12000] loss: 0.622\n",
      "[Epoch :: 6, Mini Batch ::  2000] loss: 0.525\n",
      "[Epoch :: 6, Mini Batch ::  4000] loss: 0.551\n",
      "[Epoch :: 6, Mini Batch ::  6000] loss: 0.557\n",
      "[Epoch :: 6, Mini Batch ::  8000] loss: 0.594\n",
      "[Epoch :: 6, Mini Batch :: 10000] loss: 0.596\n",
      "[Epoch :: 6, Mini Batch :: 12000] loss: 0.603\n",
      "[Epoch :: 7, Mini Batch ::  2000] loss: 0.503\n",
      "[Epoch :: 7, Mini Batch ::  4000] loss: 0.534\n",
      "[Epoch :: 7, Mini Batch ::  6000] loss: 0.536\n",
      "[Epoch :: 7, Mini Batch ::  8000] loss: 0.558\n",
      "[Epoch :: 7, Mini Batch :: 10000] loss: 0.548\n",
      "[Epoch :: 7, Mini Batch :: 12000] loss: 0.578\n",
      "[Epoch :: 8, Mini Batch ::  2000] loss: 0.472\n",
      "[Epoch :: 8, Mini Batch ::  4000] loss: 0.497\n",
      "[Epoch :: 8, Mini Batch ::  6000] loss: 0.518\n",
      "[Epoch :: 8, Mini Batch ::  8000] loss: 0.527\n",
      "[Epoch :: 8, Mini Batch :: 10000] loss: 0.530\n",
      "[Epoch :: 8, Mini Batch :: 12000] loss: 0.563\n",
      "[Epoch :: 9, Mini Batch ::  2000] loss: 0.452\n",
      "[Epoch :: 9, Mini Batch ::  4000] loss: 0.481\n",
      "[Epoch :: 9, Mini Batch ::  6000] loss: 0.499\n",
      "[Epoch :: 9, Mini Batch ::  8000] loss: 0.501\n",
      "[Epoch :: 9, Mini Batch :: 10000] loss: 0.537\n",
      "[Epoch :: 9, Mini Batch :: 12000] loss: 0.510\n",
      "[Epoch :: 10, Mini Batch ::  2000] loss: 0.472\n",
      "[Epoch :: 10, Mini Batch ::  4000] loss: 0.467\n",
      "[Epoch :: 10, Mini Batch ::  6000] loss: 0.489\n",
      "[Epoch :: 10, Mini Batch ::  8000] loss: 0.487\n",
      "[Epoch :: 10, Mini Batch :: 10000] loss: 0.517\n",
      "[Epoch :: 10, Mini Batch :: 12000] loss: 0.537\n",
      "[Epoch :: 11, Mini Batch ::  2000] loss: 0.423\n",
      "[Epoch :: 11, Mini Batch ::  4000] loss: 0.456\n",
      "[Epoch :: 11, Mini Batch ::  6000] loss: 0.477\n",
      "[Epoch :: 11, Mini Batch ::  8000] loss: 0.468\n",
      "[Epoch :: 11, Mini Batch :: 10000] loss: 0.505\n",
      "[Epoch :: 11, Mini Batch :: 12000] loss: 0.477\n",
      "[Epoch :: 12, Mini Batch ::  2000] loss: 0.374\n",
      "[Epoch :: 12, Mini Batch ::  4000] loss: 0.454\n",
      "[Epoch :: 12, Mini Batch ::  6000] loss: 0.450\n",
      "[Epoch :: 12, Mini Batch ::  8000] loss: 0.474\n",
      "[Epoch :: 12, Mini Batch :: 10000] loss: 0.483\n",
      "[Epoch :: 12, Mini Batch :: 12000] loss: 0.497\n",
      "[Epoch :: 13, Mini Batch ::  2000] loss: 0.391\n",
      "[Epoch :: 13, Mini Batch ::  4000] loss: 0.426\n",
      "[Epoch :: 13, Mini Batch ::  6000] loss: 0.452\n",
      "[Epoch :: 13, Mini Batch ::  8000] loss: 0.439\n",
      "[Epoch :: 13, Mini Batch :: 10000] loss: 0.456\n",
      "[Epoch :: 13, Mini Batch :: 12000] loss: 0.449\n",
      "[Epoch :: 14, Mini Batch ::  2000] loss: 0.401\n",
      "[Epoch :: 14, Mini Batch ::  4000] loss: 0.418\n",
      "[Epoch :: 14, Mini Batch ::  6000] loss: 0.432\n",
      "[Epoch :: 14, Mini Batch ::  8000] loss: 0.447\n",
      "[Epoch :: 14, Mini Batch :: 10000] loss: 0.448\n",
      "[Epoch :: 14, Mini Batch :: 12000] loss: 0.460\n",
      "[Epoch :: 15, Mini Batch ::  2000] loss: 0.387\n",
      "[Epoch :: 15, Mini Batch ::  4000] loss: 0.408\n",
      "[Epoch :: 15, Mini Batch ::  6000] loss: 0.433\n",
      "[Epoch :: 15, Mini Batch ::  8000] loss: 0.448\n",
      "[Epoch :: 15, Mini Batch :: 10000] loss: 0.441\n",
      "[Epoch :: 15, Mini Batch :: 12000] loss: 0.442\n",
      "[Epoch :: 16, Mini Batch ::  2000] loss: 0.368\n",
      "[Epoch :: 16, Mini Batch ::  4000] loss: 0.397\n",
      "[Epoch :: 16, Mini Batch ::  6000] loss: 0.379\n",
      "[Epoch :: 16, Mini Batch ::  8000] loss: 0.424\n",
      "[Epoch :: 16, Mini Batch :: 10000] loss: 0.436\n",
      "[Epoch :: 16, Mini Batch :: 12000] loss: 0.453\n",
      "[Epoch :: 17, Mini Batch ::  2000] loss: 0.359\n",
      "[Epoch :: 17, Mini Batch ::  4000] loss: 0.384\n",
      "[Epoch :: 17, Mini Batch ::  6000] loss: 0.418\n",
      "[Epoch :: 17, Mini Batch ::  8000] loss: 0.423\n",
      "[Epoch :: 17, Mini Batch :: 10000] loss: 0.423\n",
      "[Epoch :: 17, Mini Batch :: 12000] loss: 0.429\n",
      "[Epoch :: 18, Mini Batch ::  2000] loss: 0.351\n",
      "[Epoch :: 18, Mini Batch ::  4000] loss: 0.386\n",
      "[Epoch :: 18, Mini Batch ::  6000] loss: 0.397\n",
      "[Epoch :: 18, Mini Batch ::  8000] loss: 0.391\n",
      "[Epoch :: 18, Mini Batch :: 10000] loss: 0.394\n",
      "[Epoch :: 18, Mini Batch :: 12000] loss: 0.422\n",
      "[Epoch :: 19, Mini Batch ::  2000] loss: 0.364\n",
      "[Epoch :: 19, Mini Batch ::  4000] loss: 0.407\n",
      "[Epoch :: 19, Mini Batch ::  6000] loss: 0.387\n",
      "[Epoch :: 19, Mini Batch ::  8000] loss: 0.399\n",
      "[Epoch :: 19, Mini Batch :: 10000] loss: 0.405\n",
      "[Epoch :: 19, Mini Batch :: 12000] loss: 0.426\n",
      "[Epoch :: 20, Mini Batch ::  2000] loss: 0.326\n",
      "[Epoch :: 20, Mini Batch ::  4000] loss: 0.378\n",
      "[Epoch :: 20, Mini Batch ::  6000] loss: 0.381\n",
      "[Epoch :: 20, Mini Batch ::  8000] loss: 0.365\n",
      "[Epoch :: 20, Mini Batch :: 10000] loss: 0.407\n",
      "[Epoch :: 20, Mini Batch :: 12000] loss: 0.409\n",
      "[Epoch :: 21, Mini Batch ::  2000] loss: 0.347\n",
      "[Epoch :: 21, Mini Batch ::  4000] loss: 0.359\n",
      "[Epoch :: 21, Mini Batch ::  6000] loss: 0.366\n",
      "[Epoch :: 21, Mini Batch ::  8000] loss: 0.365\n",
      "[Epoch :: 21, Mini Batch :: 10000] loss: 0.468\n",
      "[Epoch :: 21, Mini Batch :: 12000] loss: 0.439\n",
      "[Epoch :: 22, Mini Batch ::  2000] loss: 0.311\n",
      "[Epoch :: 22, Mini Batch ::  4000] loss: 0.374\n",
      "[Epoch :: 22, Mini Batch ::  6000] loss: 0.366\n",
      "[Epoch :: 22, Mini Batch ::  8000] loss: 0.375\n",
      "[Epoch :: 22, Mini Batch :: 10000] loss: 0.408\n",
      "[Epoch :: 22, Mini Batch :: 12000] loss: 0.410\n",
      "[Epoch :: 23, Mini Batch ::  2000] loss: 0.332\n",
      "[Epoch :: 23, Mini Batch ::  4000] loss: 0.356\n",
      "[Epoch :: 23, Mini Batch ::  6000] loss: 0.394\n",
      "[Epoch :: 23, Mini Batch ::  8000] loss: 0.363\n",
      "[Epoch :: 23, Mini Batch :: 10000] loss: 0.424\n",
      "[Epoch :: 23, Mini Batch :: 12000] loss: 0.431\n",
      "[Epoch :: 24, Mini Batch ::  2000] loss: 0.356\n",
      "[Epoch :: 24, Mini Batch ::  4000] loss: 0.347\n",
      "[Epoch :: 24, Mini Batch ::  6000] loss: 0.354\n",
      "[Epoch :: 24, Mini Batch ::  8000] loss: 0.376\n",
      "[Epoch :: 24, Mini Batch :: 10000] loss: 0.399\n",
      "[Epoch :: 24, Mini Batch :: 12000] loss: 0.380\n",
      "[Epoch :: 25, Mini Batch ::  2000] loss: 0.325\n",
      "[Epoch :: 25, Mini Batch ::  4000] loss: 0.352\n",
      "[Epoch :: 25, Mini Batch ::  6000] loss: 0.379\n",
      "[Epoch :: 25, Mini Batch ::  8000] loss: 0.351\n",
      "[Epoch :: 25, Mini Batch :: 10000] loss: 0.410\n",
      "[Epoch :: 25, Mini Batch :: 12000] loss: 0.427\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net=model_crazy_architecture_with_dropout\n",
    "lossvsiter=lossvsiter_crazy_architecture_with_dropout\n",
    "# To see if the model is on CUDA or not !\n",
    "if (next(net.parameters()).is_cuda) :\n",
    "    print(\"The model is on CUDA\")\n",
    "else :\n",
    "    print(\"The model is on CPU\")\n",
    "\n",
    "# Import the optimizers \n",
    "import torch.optim as optim\n",
    "\n",
    "# Declare a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Declare an optimizer\n",
    "#optimizer = optim.Adam(net.parameters(),lr=0.0005,weight_decay=1e-7)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.0005)\n",
    "\n",
    "# Lets use an lr scheduler\n",
    "\n",
    "#scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=3,verbose=True)\n",
    "\n",
    "#No of iterations !\n",
    "iterations = 25\n",
    "\n",
    "\n",
    "for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "    # Reset the loss for the current epoch !\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Loop over all the mini-batches therea are 12500 mini batches of size 4 each !\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable & if possible make them cuda tensors\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients for the current epoch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Calculate gradients of whatever variable set to req_gardients = True\n",
    "        loss.backward()\n",
    "\n",
    "        # Take one step of the gradient descent for this epoch ! \n",
    "        optimizer.step()\n",
    "#         scheduler.step(loss)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[Epoch :: %d, Mini Batch :: %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            lossvsiter.append(running_loss / 2000)\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 74\n",
      "The network predicted correct for 7442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(74.42, 7442)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(model_crazy_architecture_with_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Accuracy of the network on the 50000 trained images: 90 %\n"
     ]
    }
   ],
   "source": [
    "train_accuracy(model_crazy_architecture_with_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
